[
["index.html", "Aprendizaje Automático 1 1 Introducción", " Aprendizaje Automático 1 Juan R González 2020-10-06 1 Introducción Este bookdown sirven como notas para el curso Aprendizaje Automático 1 impartido en el Grado de Estadística de la UAB El objetivo de este material es familiarizar al alumno con diferentes métodos de aprendizaje automático (machine learning en inglés) desde un punto de vista aplicado haciendo énfasis a situaciones donde se dispone de grandes cantidades de datos. Los contenidos incluyen los siguientes temas: Breve introducción a Tidyverse Introducción al aprendizaje automático Regresión lineal y logística Pasos previos a la creación de un modelo predictivo y medidas de validación Métodos de aprendizaje automático Arboles de clasificación K-vecinos más cercanos Random Forest Boosting Métodos de aprendizaje para datos n&lt;&lt;p Métodos de penalización (shrinkage) Métodos de regularización La librería ‘caret’ Métodos de aprendizaje para datos grandes (big data) XGBoost Lasso La librería ‘H20’ En el aula virtual de la asignatura (Moodle) se dispondrán de numerosas preguntas de autoevaluación para que el alumno pueda ver si adquiere los conocimientos descritos en cada sesión teórica, así como varias prácticas que permitirán formar al alumno en el análisis de datos mediante las principales técnicas de aprendizaje automático utilizando datos reales usando el software R. Algunas partes de este material está inspirados en vignettes que se referencian en cada capítulo. Los ejemplos de regresión lineal se basan en el trabajo de Jeff Webb. Este material está licenciado bajo una Creative Commons Attribution 4.0 International License. "],
["introducción-a-tidyverse.html", "2 Introducción a Tidyverse 2.1 Introducción 2.2 Instalación 2.3 Librerías básicas 2.4 Manejo de datos 2.5 Uso del pipe %&gt;% 2.6 Ejercicios (manejo de datos) 2.7 Visualización de datos 2.8 Ejercicios (Visualización de datos)", " 2 Introducción a Tidyverse 2.1 Introducción Tidyverse tidyverse es una forma elegante de usar R y hacer de este software una herramienta mejorada. Ha sido diseñado por Hadley / Rstudio e incluye distintas librería que siguen las reglas principales del manifiesto de The tidy tools. Los autores describen esta nueva implementación como: tidyverse es un conjunto de librerías que funcionan en armonía porque comparten representaciones de datos comunes y diseño de API. El paquete tidyverse está diseñado para facilitar la instalación y carga de los paquetes principales desde el tidyverse en un solo comando. Existen numeros ayudas, cursos y material en general para aprender todo sobre las librerías de tidyverse, pero el autor ha usado el libro R for Data Science como base para ilustrar cómo usar este conjunto de herramientas para el análisis de datos en ciencias. Este libro (del que os he puesto el link) es una excelente fuente para aprender todo sobre tidyverse. En este capítulo os referenciaré otro material que puede ser de ayuda. El libro de R for Data Science se organiza siguiendo este esquema: Figure 2.1: Esquema R for Data Science De forma que las librerías incluidas en tidyverse cubren todos estos aspectos. Está pensado para facilitar tareas de gestión de datos, y en su caso, el manejo de grandes volúmenes de información de forma eficiente. Se describen técnicas que ayudan a la visualización de datos que es el primer paso que se debe llevar a cabo en cualquier análisis estadístico que se precie. Esta visualización y posterior análisis no sólo deber llevarse a cabo en toda la base de datos, puede requerirse analizar subconjuntos de datos obtenidos mediante algún filtro o inlcuso puede necesitarse recodificar o restructurar la información disponible . Quizás estos sean los procedimientos para los que tidiverse mejore de forma sustancial el uso de R tradicional (junto con la forma compacta y clara de escribir código), ya que la posterior modelización puede llevarse a cabo con decenas de librerías diseñadas para tal efecto. 2.2 Instalación Para instalar el conjunto básico de liberías relacionadas con tidyverse basta con ejecutar_ install.packages(&quot;tidyverse&quot;) Las librerías básicas relacionadas con tidyverse se cargan de la forma usual, con una única llamada library(&quot;tidyverse&quot;) La diferencia con el hecho de cargar cualquier otra librería, es que en un sólo comando se instalan/cargan un par de docenas de paquetes. Como beneficio adicional, uno puede estar seguro de que todos los paquetes instalados / cargados son de versiones compatibles. 2.3 Librerías básicas Las librerías que son más conocidas y ampliamente utilizadas en tidyverse son: ggplot2: visualización avanzada de datos dplyr: manipulación de datos de forma eficiente y escalable (usa Rcpp) tidyr: funciones para ordenar datos readr: importa datos. purrr: desarrolla una especie de “dialecto” de R que facilita muchas operaciones (map, Reduce, …) tibble: forma moderna para conceptualizar los datos. magrittr: canalización para hacer el código más legible (uso del “pipe” %&gt;%) Paquetes para manipular formatos de datos específicos: hms: leer fácilmente fechas y tiempos stringr: funciones para trabajar de forma sencilla con cadenas de carácteres Lubridate: Manipulación avanzada de fechas Forcats: funciones avanzadas con factores Importar datos: DBI: define una interfaz común entre R y los sistemas de administración de bases de datos (DBMS) haven: importar archivos SPSS, SAS y Stata de forma sencilla httr: facilitar el uso del paquete curl personalizado las necesidades de las API web modernas jsonlite: análisis y generación de JSON rápido y optimizado para obtener estadísticas en la web readxl: leer archivos read.xls y .xlsx de forma sencilla y sin necesitar otras dependencias rvest: obtener información de páginas web xml2: trabajar con XML Modelización: Existen varias librerías, pero yo prefiero usar las de R y/o Bioconductor 2.4 Manejo de datos En tidyverse, los data.frames se trabajan como tibbles. Esto permite disponer de una forma consistente y eficiente de guardar nuestros datos permitiendo una forma sencilla de hacer transformaciones, visualización y modelización. Esta sección pretende ser una introducción básica a tidyverse por lo que sólo veremos cómo llevar a cabo los principales tipos de manipulación de datos. No obstante, también existen funciones específicas para: Relacionar múltiples tablas ver ejemplos. Trabajar con variables carácter Usar factores para variables categóricas de forma sencilla (sin los problemas de orden de categorías) Realizar operaciones con variables de tipo fecha ver ejemplos. 2.4.1 Tibbles Empecemos introduciendo lo que es un tibble (pronunciado “tibel”). Se puede aprender más cosas ejecutando vignette(\"tibble\") en la consola de RStudio. Tras cargar la librería tidyverse podemos crear un tibble a partir de un data.frame de la siguiente forma. Usaremos la base de datos iris a modo de ejemplo head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris.tib &lt;- tibble(iris) iris.tib ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows También podemos crear un nuevo tibble mediante (los datos se reciclan): tibble( x = 1:5, y = 1, z = x ^ 2 + y ) ## # A tibble: 5 x 3 ## x y z ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 2 ## 2 2 1 5 ## 3 3 1 10 ## 4 4 1 17 ## 5 5 1 26 NOTA 1: Nunca cambia los tipos de datos! (i.e. caracter a factor) NOTA 2: El rownames desaparece Quizás uno de los aspectos más novedosos de las tibble sea que se ha re-definido el método print() que permite, por defecto, ver las 10 primeras filas y todas las columnas que quepan en la pantalla. Esto puede cambiarse con print(iris.tib, n = 10, width = Inf) ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with 140 more rows ó print(iris.tib, n = 10, width = 25) ## # A tibble: 150 x 5 ## Sepal.Length ## &lt;dbl&gt; ## 1 5.1 ## 2 4.9 ## 3 4.7 ## 4 4.6 ## 5 5 ## 6 5.4 ## 7 4.6 ## 8 5 ## 9 4.4 ## 10 4.9 ## # ... with 140 more ## # rows, and 4 more ## # variables: ## # Sepal.Width &lt;dbl&gt;, ## # Petal.Length &lt;dbl&gt;, ## # Petal.Width &lt;dbl&gt;, ## # Species &lt;fct&gt; Podemos acceder a una columna (e.g. variable) de la misma forma que con un data.frame df &lt;- tibble( x = runif(5), y = rnorm(5) ) # Extract by name df$x ## [1] 0.6013053 0.3191631 0.3842864 0.7869046 0.8002453 df[[&quot;x&quot;]] ## [1] 0.6013053 0.3191631 0.3842864 0.7869046 0.8002453 # Extract by position df[[1]] ## [1] 0.6013053 0.3191631 0.3842864 0.7869046 0.8002453 2.4.2 Importar datos El paquete clave para leer datos es readr read_csv () lee archivos delimitados por comas, read_csv2 () lee archivos separados por punto y coma (común en países donde, se usa como el lugar decimal), read_tsv () lee archivos delimitados por tabulaciones y read_delim () lee archivos con cualquier delimitador. read_fwf () lee archivos de ancho fijo. Se puede especificar campos por sus anchos con fwf_widths () o su posición con fwf_positions (). read_table () lee archivos de ancho fijo donde las columnas están separadas por espacios en blanco. read_log () lee archivos de registro de estilo Apache (servidor web de código abierto). Pero también es muy útil webreadr, que está construido sobre read_log () y proporciona muchas más herramientas útiles. Estas funciones suelen ser mucho más rápidas (~ 10x) que sus equivalentes en R básico. Además, la importación de datos de ejecución prolongada tienen una barra de progreso para que se pueda ver lo que está sucediendo. Si se está buscando velocidad bruta, también podemos usar data.table::fread() que aunque no encaja tan bien en tidyverse puede usarse en ocasiones donde precie la velocidad (pero no es mucho más rápido). Los datos se importan como objetos que: son tibbles no convierten vectores de caracteres en factores no usan nombres de filas ni modifican los nombres de columnas. Éstas son fuentes comunes de frustración con las funciones base R [¡declaración de Hadley!]. Son más reproducibles. Las funciones de Base R heredan algún comportamiento de su sistema operativo y variables de entorno, por lo que el código para importar datos que funciona en un ordenador, podría no funcionar en el de otra persona. Hagamos una comparación con un archivo grande library(readr) system.time(dd1 &lt;- read.delim(&quot;data/genome.txt&quot;)) ## user system elapsed ## 6.38 0.11 6.50 system.time(dd2 &lt;- read_delim(&quot;data/genome.txt&quot;, delim=&quot;\\t&quot;)) ## user system elapsed ## 0.47 0.02 0.51 dim(dd2) ## [1] 733202 5 Efectivamente ambos objetos contienen la misma información head(dd1) ## Name Chr Position Log.R.Ratio B.Allele.Freq ## 1 rs1000000 12 125456933 -0.002501764 1.000000000 ## 2 rs1000002 3 185118461 -0.029741180 0.000336171 ## 3 rs10000023 4 95952928 0.004015533 0.460671800 ## 4 rs1000003 3 99825597 -0.142527700 0.541123600 ## 5 rs10000030 4 103593179 0.365104000 1.000000000 ## 6 rs10000037 4 38600725 -0.005177616 0.504625300 dd2 ## # A tibble: 733,202 x 5 ## Name Chr Position Log.R.Ratio B.Allele.Freq ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 rs1000000 12 125456933 -0.00250 1 ## 2 rs1000002 3 185118461 -0.0297 0.000336 ## 3 rs10000023 4 95952928 0.00402 0.461 ## 4 rs1000003 3 99825597 -0.143 0.541 ## 5 rs10000030 4 103593179 0.365 1 ## 6 rs10000037 4 38600725 -0.00518 0.505 ## 7 rs10000041 4 165841405 -0.179 0 ## 8 rs10000042 4 5288053 0.168 0.998 ## 9 rs10000049 4 119167668 -0.00238 0 ## 10 rs1000007 2 237416793 -0.00411 0 ## # ... with 733,192 more rows 2.4.3 Transformación de datos Antes de empezar a analizar los datos suele ser necesario realizar algunas transformaciones o crear nuevas variables ya que: Es raro que obtengamos los datos exactamente en la forma correcta que necesitamos. A menudo, se deberán crear algunas variables o resúmenes nuevos. A veces se necesita cambiar el nombre de las variables o reordenar las observaciones para que sea un poco más fácil trabajar con los datos. Ilustremos cómo realizar estas tareas utilizando los datos disponibles en una base de datos de vuelos de NYC. El objeto nycflights13::fligths contiene los 336,776 vuelos que partieron de la ciudad de Nueva York en 2013. Los datos provienen de la Oficina de Estadísticas de Transporte de EE. UU. y están documentados en ?flights. library(nycflights13) library(tidyverse) flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD ## # ... with 336,766 more rows, and 5 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Las funciones básicas que usaremos están disponibles en la librería dlpyr y son las siguientes: Elegir observaciones por sus valores: filter(). Reordenar las filas: arrange(). Elegir las variables por sus nombres: select(). Crear nuevas variables a partir de variables existentes: mutate(). Colapsar valores en un sólo resumen: summarise(). Todos los verbos (e.g funciones) se usan de la misma forma: El primer argumento es un tibble o un data.frame. Los argumentos siguientes describen qué hacer con los datos, utilizando los nombres de las variables (sin comillas). El resultado es un nuevo tibble. 2.4.4 Filtrar filas jan1 &lt;- filter(flights, month == 1, day == 1) R imprime los resultados o los guarda en una variable. Si desea hacer ambas cosas, podemos envolver la sintaxis entre paréntesis: (jan1 &lt;- filter(flights, month == 1, day == 1)) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD ## # ... with 832 more rows, and 5 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2.4.5 Filtrado lógico Estos son los operadoles lógicos que podemos aplicar boolean operations filter(flights, month == 11 | month == 12) ## # A tibble: 55,403 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 11 1 5 2359 6 352 345 7 B6 745 N568JB JFK PSE ## 2 2013 11 1 35 2250 105 123 2356 87 B6 1816 N353JB JFK SYR ## 3 2013 11 1 455 500 -5 641 651 -10 US 1895 N192UW EWR CLT ## 4 2013 11 1 539 545 -6 856 827 29 UA 1714 N38727 LGA IAH ## 5 2013 11 1 542 545 -3 831 855 -24 AA 2243 N5CLAA JFK MIA ## 6 2013 11 1 549 600 -11 912 923 -11 UA 303 N595UA JFK SFO ## 7 2013 11 1 550 600 -10 705 659 6 US 2167 N748UW LGA DCA ## 8 2013 11 1 554 600 -6 659 701 -2 US 2134 N742PS LGA BOS ## 9 2013 11 1 554 600 -6 826 827 -1 DL 563 N912DE LGA ATL ## 10 2013 11 1 554 600 -6 749 751 -2 DL 731 N315NB LGA DTW ## # ... with 55,393 more rows, and 5 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, !(arr_delay &gt; 120 | dep_delay &gt; 120)) ## # A tibble: 316,050 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD ## # ... with 316,040 more rows, and 5 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 2.4.6 Ordenar filas En orden ascendente arrange(flights, year, month, day) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH ## 2 2013 1 1 533 529 4 850 830 20 UA 1714 N24211 LGA IAH ## 3 2013 1 1 542 540 2 923 850 33 AA 1141 N619AA JFK MIA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN ## 5 2013 1 1 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL ## 6 2013 1 1 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD ## 7 2013 1 1 555 600 -5 913 854 19 B6 507 N516JB EWR FLL ## 8 2013 1 1 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD ## 9 2013 1 1 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO ## 10 2013 1 1 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD ## # ... with 336,766 more rows, and 5 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; y descendente arrange(flights, desc(dep_delay)) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2013 1 9 641 900 1301 1242 1530 1272 HA 51 N384HA JFK HNL ## 2 2013 6 15 1432 1935 1137 1607 2120 1127 MQ 3535 N504MQ JFK CMH ## 3 2013 1 10 1121 1635 1126 1239 1810 1109 MQ 3695 N517MQ EWR ORD ## 4 2013 9 20 1139 1845 1014 1457 2210 1007 AA 177 N338AA JFK SFO ## 5 2013 7 22 845 1600 1005 1044 1815 989 MQ 3075 N665MQ JFK CVG ## 6 2013 4 10 1100 1900 960 1342 2211 931 DL 2391 N959DL JFK TPA ## 7 2013 3 17 2321 810 911 135 1020 915 DL 2119 N927DA LGA MSP ## 8 2013 6 27 959 1900 899 1236 2226 850 DL 2007 N3762Y JFK PDX ## 9 2013 7 22 2257 759 898 121 1026 895 DL 2047 N6716C LGA ATL ## 10 2013 12 5 756 1700 896 1058 2020 878 AA 172 N5DMAA EWR MIA ## # ... with 336,766 more rows, and 5 more variables: air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; NOTA: los valores missing están situados al final 2.4.7 Seleccionar columnas (e.g. variables) Seleccionamos las columnas que queremos dplyr::select(flights, year, month, day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows o las que están entre dos columnas dplyr::select(flights, year:day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows también podemos seleccionar todas las columnas menos algunas dplyr::select(flights, -(year:day)) ## # A tibble: 336,776 x 16 ## dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier flight tailnum origin dest air_time distance ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 517 515 2 830 819 11 UA 1545 N14228 EWR IAH 227 1400 ## 2 533 529 4 850 830 20 UA 1714 N24211 LGA IAH 227 1416 ## 3 542 540 2 923 850 33 AA 1141 N619AA JFK MIA 160 1089 ## 4 544 545 -1 1004 1022 -18 B6 725 N804JB JFK BQN 183 1576 ## 5 554 600 -6 812 837 -25 DL 461 N668DN LGA ATL 116 762 ## 6 554 558 -4 740 728 12 UA 1696 N39463 EWR ORD 150 719 ## 7 555 600 -5 913 854 19 B6 507 N516JB EWR FLL 158 1065 ## 8 557 600 -3 709 723 -14 EV 5708 N829AS LGA IAD 53 229 ## 9 557 600 -3 838 846 -8 B6 79 N593JB JFK MCO 140 944 ## 10 558 600 -2 753 745 8 AA 301 N3ALAA LGA ORD 138 733 ## # ... with 336,766 more rows, and 3 more variables: hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Existen numerosas funciones que pueden ser de mucha utilidad para hacer selecciones más complejas y rápidas dentro de la función select() starts_with(\"abc\"): nombres que empiezan con “abc”. ends_with(\"xyz\"): nombres que acaban con “xyz”. contains(\"ijk\"): nombres que contienen “ijk”. matches(\"(.)\\\\1\"): selecciona variables que coinciden con una expresión regular. Se puede aprender más con strings. num_range(\"x\", 1:3): coincide con x1, x2 y x3. 2.4.8 Añadir nuevas variables Debemos usar la función mutate() flights_sml &lt;- dplyr::select(flights, year:day, ends_with(&quot;delay&quot;), distance, air_time ) mutate(flights_sml, gain = dep_delay - arr_delay, speed = distance / air_time * 60 ) ## # A tibble: 336,776 x 9 ## year month day dep_delay arr_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 1400 227 -9 370. ## 2 2013 1 1 4 20 1416 227 -16 374. ## 3 2013 1 1 2 33 1089 160 -31 408. ## 4 2013 1 1 -1 -18 1576 183 17 517. ## 5 2013 1 1 -6 -25 762 116 19 394. ## 6 2013 1 1 -4 12 719 150 -16 288. ## 7 2013 1 1 -5 19 1065 158 -24 404. ## 8 2013 1 1 -3 -14 229 53 11 259. ## 9 2013 1 1 -3 -8 944 140 5 405. ## 10 2013 1 1 -2 8 733 138 -10 319. ## # ... with 336,766 more rows Si sólo queremos mantener las nuevas variables en nuestra tabla de datos, debemos usar transmute(): transmute(flights, gain = dep_delay - arr_delay, hours = air_time / 60, gain_per_hour = gain / hours ) ## # A tibble: 336,776 x 3 ## gain hours gain_per_hour ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -9 3.78 -2.38 ## 2 -16 3.78 -4.23 ## 3 -31 2.67 -11.6 ## 4 17 3.05 5.57 ## 5 19 1.93 9.83 ## 6 -16 2.5 -6.4 ## 7 -24 2.63 -9.11 ## 8 11 0.883 12.5 ## 9 5 2.33 2.14 ## 10 -10 2.3 -4.35 ## # ... with 336,766 more rows 2.4.9 Grouped summaries Podemos agrupar variables de la siguiente forma: summarise(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 x 1 ## delay ## &lt;dbl&gt; ## 1 12.6 by_day &lt;- group_by(flights, year, month, day) summarise(by_day, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 365 x 4 ## # Groups: year, month [12] ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # ... with 355 more rows También podemos agrupar según varios criterios flights %&gt;% group_by(year, month, day) %&gt;% summarise( avg_delay1 = mean(arr_delay, na.rm=TRUE), avg_delay2 = mean(arr_delay[arr_delay &gt; 0], na.rm=TRUE) ) ## # A tibble: 365 x 5 ## # Groups: year, month [12] ## year month day avg_delay1 avg_delay2 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 12.7 32.5 ## 2 2013 1 2 12.7 32.0 ## 3 2013 1 3 5.73 27.7 ## 4 2013 1 4 -1.93 28.3 ## 5 2013 1 5 -1.53 22.6 ## 6 2013 1 6 4.24 24.4 ## 7 2013 1 7 -4.95 27.8 ## 8 2013 1 8 -3.23 20.8 ## 9 2013 1 9 -0.264 25.6 ## 10 2013 1 10 -5.90 27.3 ## # ... with 355 more rows Existen otras funciones útiles por las que nos podría interesar agrupar count() mean() median() min() max() quantile(x, 0.25) IQR() mad() 2.5 Uso del pipe %&gt;% Imaginemos que queremos explorar la relación entre la distancia y el retraso promedio para cada ubicación. Los pasos para obtener esta información serían: Agrupar los vuelos por destino. Resumir la información calculando la distancia, el retraso promedio y el número de vuelos. Filtrar algunos valores que introducen ruido (producen sesgo) como el aeropuerto de Honolulu, que está casi el doble de lejos que el siguiente aeropuerto más cercano. Utilizando dplyr escribiríamos algo como esto (aún más largo en R tradicional y menos legible): by_dest &lt;- group_by(flights, dest) delay &lt;- summarise(by_dest, count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) delay &lt;- filter(delay, count &gt; 20, dest != &quot;HNL&quot;) delay ## # A tibble: 96 x 4 ## dest count dist delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 254 1826 4.38 ## 2 ACK 265 199 4.85 ## 3 ALB 439 143 14.4 ## 4 ATL 17215 757. 11.3 ## 5 AUS 2439 1514. 6.02 ## 6 AVL 275 584. 8.00 ## 7 BDL 443 116 7.05 ## 8 BGR 375 378 8.03 ## 9 BHM 297 866. 16.9 ## 10 BNA 6333 758. 11.8 ## # ... with 86 more rows Y podríamos tener un gráfico de la siguiente forma (veremos cómo hacer esto en la siguiente sesión) ggplot(data = delay, mapping = aes(x = dist, y = delay)) + geom_point(aes(size = count), alpha = 1/3) + geom_smooth(se = FALSE) Utilizando pipes el código quedaría mucho más compacto y legible delays &lt;- flights %&gt;% group_by(dest) %&gt;% summarise( count = n(), dist = mean(distance, na.rm = TRUE), delay = mean(arr_delay, na.rm = TRUE) ) %&gt;% filter(count &gt; 20, dest != &quot;HNL&quot;) delays ## # A tibble: 96 x 4 ## dest count dist delay ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ABQ 254 1826 4.38 ## 2 ACK 265 199 4.85 ## 3 ALB 439 143 14.4 ## 4 ATL 17215 757. 11.3 ## 5 AUS 2439 1514. 6.02 ## 6 AVL 275 584. 8.00 ## 7 BDL 443 116 7.05 ## 8 BGR 375 378 8.03 ## 9 BHM 297 866. 16.9 ## 10 BNA 6333 758. 11.8 ## # ... with 86 more rows 2.6 Ejercicios (manejo de datos) Los siguientes ejercicios os ayudarán a trabajar con las tareas más básicas de dplyr. Debéis realizarlos usando las funciones que hemos visto en esta presentación - no vale usar código R básico. Usaremos los datos mtcars vistos en clase. Recordad que podemos obtener más información sobre las variables con ?mtcars. También podemos usar glimpse() para ver qué tipo de variables tenemos, y en caso de ser variables categóricas, qué categorías hay (Siempre es muy recomendable hacer este tipo de visualización de datos para ver que no hayan valores raros ni categorías no definidas o errores en la definición de categorías - por ejemplo tener la variable sexo como: H, M, h, m, hombre). Visualiza la variable ‘hp’ usando la función select(). Intenta usar la función pull() para hacer lo mismo y ver cuál es la diferencia. Visualiza todos los datos excepto la columna ‘hp’. Visualiza las columnas mpg, hp, vs, am y gear escribiendo el código más corto posible. Crea un objeto que se llame ‘mycars’ que contenga las columnas mpg y hp pero que el nombre de la variable sea ‘miles_per_gallon’ y ‘horse_power’ respectivamente. Pon los rownames del data.frame en una variable que se llame ‘model’ [PISTA: debes buscar qué función hay para poner los rownames en una columna]. Crea una nueva variable en ‘mycars’ que sea ‘km_per_litre’ que describa el consumo del coche (variable ‘mpg’). NOTA: 1 mpg es 0.425 km / l. Selecciona al azar (y visualiza) la mitad de las observaciones de ‘mycars’ [PISTA: busca una función de dplyr que haga esto de forma sencilla (similar a sample en R tradicional). Crea un objeto ‘mycars_s’ que contenga de la 10ª a la 35ª fila de mycars [PISTA: considera usar la función slice()]. Visualiza el objeto ‘mycars_s’ sin duplicados [PISTA: considera usar la función distinct()]. Visualiza del objeto ‘mycars_s’ todas las observaciones que tengan mpg&gt; 20 y hp&gt; 100. Visualiza la la fila correspondiente al coche Lotus Europa. 2.7 Visualización de datos R es una herramienta muy potente para realizar gráficos, mucho más que otros software de análisis estadístico como SPSS, SAS o Stata. Aún así, ggplot fue desarrollado con los siguientes objetivos: The aim of the grammar is to “bring together in a coherent way things that previously appeared unrelated and which also will provide a basis for dealing systematically with new situations” (Cox 1978). How well have we succeeded? (Wickham, 2012) ``The emphasis in ggplot2 is reducing the amount of thinking time by making it easier to go from the plot in your brain to the plot on the page.\" (Wickham, 2012)``` Base graphics are good for drawing pictures; ggplot2 graphics are good for understanding the data.\" (Wickham, 2012) En definitiva, deberíamos usar ggplot porque: Es flexible Tenemos gran control de lo que estamos haciendo Crea gráficos muy bonitos (y se usan en la mayoría de revistas científicas) De forma más importante, hay mucha documentación sobre cómo hacer gráficos muy complicados de forma sencilla (libros, páginas web, infografrías, etc. Aquí podéis encontrar una chuleta con las principales funciones Cada visualización en ggplot se compone de: plot = data + Aesthetics + geometry Datos (Data) que queremos representar (que serán un data frame). Características estéticas (aesthetic mappings) que describen cómo queremos que los datos se vean en el gráfico. Para más información podemos consultar la vignette (vignette(“ggplot2-specs”)). Como luego veremos, se introducen con la función aes() y se refieren a: posición (en los ejes) color exterior (color) y de relleno (fill) forma de puntos (shape) tipo de línea (linetype) tamaño (size) Objetos geométricos (Geom) representan lo que vemos en un gráficos (puntos, líneas, etc.). Todo gráfico tiene, como mínimo, una geometría. La geometría determina el tipo de gráfico: geom_point (para puntos) geom_lines (para lineas) geom_histogram (para histograma) geom_boxplot (para boxplot) geom_bar (para barras) geom_smooth (líneas suavizadas) geom_polygons (para polígonos en un mapa) etc. (si ejecutáis el comando help.search(“geom_”, package = “ggplot2”) podéis ver el listado de objetos geométricos) Por tanto, para construir un gráfico con ggplot2 comenzamos con la siguiente estructura de código: ggplot(datos, aes()) + geom_tipo() Por ejemplo para hacer una gráfica que represente las millas por galón (mpg) en función del peso del coche, podemos hacer los siguiente: mtcars[1:5,1:8] ## mpg cyl disp hp drat wt qsec vs ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 ggplot(mtcars) + # data aes(x = mpg, y=wt) + # Aesthetics geom_point() # geometry (layer) Podemos cambiar a una de estas estéticas theme_dark() theme_minimal() theme_classic() theme_void() theme_test() ggplot(mtcars) + # data aes(x = mpg, y=wt) + # Aesthetics geom_point() + # geometry (layer) theme_minimal() # theme o alguna geometría ggplot(mtcars) + aes(sample = mpg) + stat_qq() A continuación ilustratemos cómo hacer los principales tipos de gráficos que necesitamos en estadística 2.7.1 Distribución de datos categóricos library(tidyverse) diamonds ## # A tibble: 53,940 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # ... with 53,930 more rows count(diamonds, cut) ## # A tibble: 5 x 2 ## cut n ## &lt;ord&gt; &lt;int&gt; ## 1 Fair 1610 ## 2 Good 4906 ## 3 Very Good 12082 ## 4 Premium 13791 ## 5 Ideal 21551 ggplot(data = diamonds) ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) 2.7.2 Distribución de datos continuos ggplot(data = diamonds) + geom_histogram(mapping = aes(x = carat), binwidth = 0.5) 2.7.3 Distribución de datos continuos según una variable categórica ggplot(data = diamonds, mapping = aes(x = price)) + geom_freqpoly(mapping = aes(colour = cut), binwidth = 500) Idealmente esta descriptiva debemos hacerla con un boxplot Box-plot ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() Podemos reordenar el boxplot para facilitar la interpretación de la siguiente manera (notamos que el código es mucho más compacto y legible que en R tradicional) ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) Podemos cambiar las coordenadas añadiendo otra función ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) + coord_flip() Los gráficos se pueden reciclar y aprovechar el código ya escrito plt &lt;- ggplot(data = mpg) + geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) plt + coord_flip() Podemos mejorar un boxplot añadiendo más información tras ’ + ’ ggplot(iris, aes(x=Species, y=Sepal.Width) ) + geom_boxplot(alpha=0.3, outlier.colour = &quot;blue&quot;) + geom_point(stat= &quot;summary&quot;, fun.y=mean, shape=16, size=1.5, color=&quot;red&quot;) + geom_jitter(width = 0.1, alpha = 0.2) 2.7.4 Dos variables categóricas Podemos resumir dos variables categóricas de esta forma ggplot(data = diamonds) + geom_count(mapping = aes(x = cut, y = color)) Otra aproximación sería contar cuántos hay en una categoría con dplyr: diamonds %&gt;% count(color, cut) ## # A tibble: 35 x 3 ## color cut n ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 D Fair 163 ## 2 D Good 662 ## 3 D Very Good 1513 ## 4 D Premium 1603 ## 5 D Ideal 2834 ## 6 E Fair 224 ## 7 E Good 933 ## 8 E Very Good 2400 ## 9 E Premium 2337 ## 10 E Ideal 3903 ## # ... with 25 more rows y luego visualizarlo con geom_tile() que nos daría un gráfico tipo heatmap diamonds %&gt;% count(color, cut) %&gt;% ggplot(mapping = aes(x = color, y = cut)) + geom_tile(mapping = aes(fill = n)) 2.7.5 Dos variables continuas ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price)) Scatterplots se vuelven menos útiles cuando el tamaño del conjunto de datos aumenta porque los puntos coinciden. En ese caso podemos usar la estética alpha: ggplot(data = diamonds) + geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100) Otra opción es discretizar una de las variables continuas y usar boxplot() ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + geom_boxplot(mapping = aes(group = cut_width(carat, 0.1))) A veces nos interesa añadir una línea de regresión al gráfico. Exsiten numerosas librerías que extienden las facilidades de ggplot com esta: library(ggpmisc) set.seed(1234) iris &lt;- mutate(iris, Y = 1.5 + 3.2*Sepal.Width + rnorm(nrow(iris))) ggplot(iris, aes(x = Sepal.Width, y = Y)) + geom_smooth(method = &quot;lm&quot;, se=FALSE, color=&quot;black&quot;, formula = y ~ x) + stat_poly_eq(formula = y ~ x, aes(label = paste(..eq.label.., ..rr.label.., sep = &quot;~~~&quot;)), parse = TRUE) + geom_point() R tiene unas capacidades gráficas excelentes, pero el uso de ggplot hace que éstas sean aún más espectaculares. Imaginemos que queremos comparar la expresión génica según la tasa de crecimiento en 20 genes y seis condiciones1 load(&quot;data/genes.Rdata&quot;) genes ## # A tibble: 711 x 7 ## name BP MF systematic_name nutrient rate expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;SUL1&quot; sulfate transport sulfate transporter activity YBR294W Glucose 0.05 -0.32 ## 2 &quot;&quot; biological process unknown molecular function unknown YKL187C Glucose 0.05 4.13 ## 3 &quot;QDR2&quot; multidrug transport multidrug efflux pump activity YIL121W Glucose 0.05 1.07 ## 4 &quot;LEU1&quot; leucine biosynthesis 3-isopropylmalate dehydratase activity YGL009C Glucose 0.05 -1.12 ## 5 &quot;PHO5&quot; phosphate metabolism* acid phosphatase activity YBR093C Glucose 0.05 2.39 ## 6 &quot;PHO12&quot; biological process unknown acid phosphatase activity YHR215W Glucose 0.05 0.9 ## 7 &quot;PHO11&quot; phosphate metabolism acid phosphatase activity YAR071W Glucose 0.05 1.14 ## 8 &quot;GIT1&quot; glycerophosphodiester transport glycerophosphodiester transporter activity YCR098C Glucose 0.05 0.77 ## 9 &quot;AGP3&quot; amino acid transport amino acid transporter activity YFL055W Glucose 0.05 0.570 ## 10 &quot;&quot; biological process unknown molecular function unknown YOL164W Glucose 0.05 0.53 ## # ... with 701 more rows Este tendría que ser el (largo) código para hacer este gráfico usando funciones básicas de R (el aspecto del gráfico es mejorable y la sintaxis de R ilegible) par(mar = c(1.5, 1.5, 1.5, 1.5)) colors &lt;- 1:6 names(colors) &lt;- unique(genes$nutrient) m &lt;- matrix(c(1:20, 21, 21, 21, 21), nrow = 6, ncol = 4, byrow = TRUE) layout(mat = m, heights = c(.18, .18, .18, .18, .18, .1)) genes$combined &lt;- paste(genes$name, genes$systematic_name) for (gene in unique(genes$combined)) { sub_data &lt;- filter(genes, combined == gene) plot(expression ~ rate, sub_data, col = colors[sub_data$nutrient], main = gene) for (n in unique(sub_data$nutrient)) { m &lt;- lm(expression ~ rate, filter(sub_data, nutrient == n)) if (!is.na(m$coefficients[2])) { abline(m, col = colors[n]) } } } # create a new plot for legend plot(1, type = &quot;n&quot;, axes = FALSE, xlab = &quot;&quot;, ylab = &quot;&quot;) legend(&quot;top&quot;, names(colors), col = colors, horiz = TRUE, lwd = 4) Sin embargo con ggplot2 bastaría con ggplot(genes, aes(rate, expression, color = nutrient)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(~name + systematic_name, scales = &quot;free_y&quot;) 2.7.6 Facets Faceting es el proceso que en el dividimos la ventana del gráfico en varias partes pequeñas (una cuadrícula) y muestra un gráfico similar en cada sección. Cada sección generalmente muestra el mismo gráfico para un grupo específico del conjunto de datos. Aquí podemos ver cómo llevar a cabo este tipo de gráficos. Utilizaremos una base de datos sobre propinas data(tips, package=&quot;reshape2&quot;) head(tips) ## total_bill tip sex smoker day time size ## 1 16.99 1.01 Female No Sun Dinner 2 ## 2 10.34 1.66 Male No Sun Dinner 3 ## 3 21.01 3.50 Male No Sun Dinner 3 ## 4 23.68 3.31 Male No Sun Dinner 2 ## 5 24.59 3.61 Female No Sun Dinner 4 ## 6 25.29 4.71 Male No Sun Dinner 4 Imaginemos que queremos representar qué propinas se dan en función del total de la cuenta sp &lt;- ggplot(tips, aes(x=total_bill, y=tip/total_bill)) + geom_point() sp Ahora nos puede interesar obtener el mismo gráfico para hombres y mujeres. Para ello, podemos hacer el faceting de forma vertical # vertical direction sp + facet_grid(sex ~ .) u horizontal # horizontal direction sp + facet_grid(. ~ sex) también según dos variables # Divide with &quot;sex&quot; vertical, &quot;day&quot; horizontal sp + facet_grid(sex ~ day) En lugar de crear los paneles con una variable en la dirección horizontal o vertical, las gráficas se pueden colocar una al lado de la otra, envolviéndose con un cierto número de columnas o filas. La etiqueta de cada figura estará en la parte superior. # Divide by day, going horizontally and wrapping with 2 columns sp + facet_wrap( ~ day, ncol=2) Podemos cambiar todo lo que queramos. Este es sólo un ejemplo sp + facet_grid(sex ~ day) + theme(strip.text.x = element_text(size=8, angle=75), strip.text.y = element_text(size=12, face=&quot;bold&quot;), strip.background = element_rect(colour=&quot;brown&quot;, fill=&quot;tomato&quot;)) En este link tenéis la referencia de ggplot2. 2.8 Ejercicios (Visualización de datos) Visualiza el número de vuelos para cada aerolínia por mes. Visualiza la distribución de la variabla carat según el precio en el dataset diamonds. Carga los datos qe están en https://raw.githubusercontent.com/isglobal-brge/TeachingMaterials/master/Master_Modelling/data/genome.txt en tu sesión de RStudio utilizando la función read_delim (NOTA: los datos están delimitados por tabulaciones - no hace falta bajarlos al ordenador se cargan desde la URL). El archivo contiene información sobre ~730K variantes genéticas en todo el genoma: Name: variante genómica (single nucleotide polymorphism) Chr: cromosoma Position: posición en el cromosoma Log.R.Ratio: log-ratio de la intensidad de los dos alelos B.Allele.Freq: frecuencia del alelo alternativo ¿Cuál es el valor esperado (media) de Log.R.Ratio y B.Allel.Freq para cada cromosoma? (muestra el código de R que usas para obtener dicha información) Crea un “facet plot” que represente el Log.R.Ratio para cada cromosoma Crea un “facet plot” que represente el B.Allele.Freq para los cromosomas 1, 2, 3, …, 6 y pinta la etiqueta B.Allele.Freq en rojo. ejemplo obtenido de http://varianceexplained.org/r/why-I-use-ggplot2/)↩︎ "],
["introducción-al-aprendizaje-automático.html", "3 Introducción al Aprendizaje Automático", " 3 Introducción al Aprendizaje Automático El aprendizaje automático (AA) (Machine Learning en inglés) es una disciplina científica que suele incluirse en el ámbito de la Inteligencia Artificial (IA) que crea sistemas que aprenden automáticamente. Aprender en este contexto quiere decir identificar patrones complejos en millones de datos. La máquina/ordenador que realmente aprende es un algoritmo que usando datos existentes es capaz de predecir comportamientos futuros. Automáticamente, también en este contexto, implica que estos sistemas se mejoran de forma autónoma con el tiempo, sin intervención humana. En esta figura podemos observar la conexión que hay entre estas áreas y una más reciente conocida como aprendizaje profundo (AP) (Deep Learning en inglés) que veréis en el curso de Aprendizaje Automático 2. Relación entre AA, AI y AP La principal diferencia entre estas áreas radica en el objetivo (e.g pregunta científica) que queremos tratar. Así, la IA vendría a representar a un sistema no biológico que es inteligente basándose en reglas. El AA se basa en algoritmos que entrenan modelos usando datos existentes, y el AP se basa en algoritmos que parametriza redes neuronales de múltiples capas que representan los datos mediante diferentes niveles de abstracción. En la siguiente figura podemos ver la clasificación (de manera muy genérica) de los tipos de AA a los que podemos enfrentarnos Tipos de Aprendizaje Automático En estadística, el AA se ha considerado como una ciencia independiente en la que se dispone de un conjunto de herramientas basadas en diferentes métodos y algoritmos que permiten clasificar individuos según una serie de variables. Concer estas técnicas estadísticas es de gran ayuda para la IA y el AP. En este curso estudiaremos estas metodologías en detalle que incluirán: Regresión lineal Regresión logística Regresión lasso (ridge, elastic net) Análisis lineal discriminante Árboles de clasificación KNN Random Forest Boosting XGBoost y cómo implementar estos algoritmos con funciones eficientes (caret) y escalables (H2O). "],
["regresión-lineal.html", "4 Regresión lineal 4.1 Preliminares 4.2 Conceptos básicos 4.3 Ajuste de un modelo lineal 4.4 Estimación por mínimos cuadrados 4.5 Medidas adicionales de ajuste del modelo 4.6 Sesgo, variación, sobreajuste 4.7 Regresión como estimación de una media condicional 4.8 La función de regresión 4.9 Estimación no paramétrica de la función de regresión: regresión KNN 4.10 Estimación paramétrica de la función de regresión: regresión lineal 4.11 Predicción 4.12 Inferencia en el contexto de regresión 4.13 Asunciones de un modelo de regresión 4.14 Ejemplos adicionales de interpretación de modelos 4.15 Centrado y escalado 4.16 Transformación de variables 4.17 Colinealidad 4.18 Valores atípicos", " 4 Regresión lineal Este capítulo presenta la regresión lineal, el método de regresión paramétrica que usamos cuando la variable de resultado o respuesta es continua. Cuando el resultado es binario, utilizamos la regresión logística, tema de un capítulo posterior. El contenido de este capítulo ya se introdujo en la asignatura Inferencia Estadística 1, y se vió de forma exhaustiva en la asignatura de Modelos lineales 1 y aquí se hará un breve repaso de los conceptos más importantes y se hará énfasis en la interpretación práctica de los conceptos aprendidos. Qué pretendemos aprender en este capítulo: Entender qué pretende y cuándo se usa la regresión lineal Cómo estimar los parámetros de un modelo de regresión Familiarizarnos con algunas medidas usadas en la regresión lineal para valorar la utilidad del modelo Tener una idea de otros aspectos a tener encuenta para estos modelos (suposiciones del modelo, colinealidad, valores atípicos, …) Existen numerosos recursos en la red para complementar este curso. Aquí tenéis algunos tutoriales/cursos en Datacamp: – [DataCamp: Correlación y regresión] (https://www.datacamp.com/courses/correlation-and-regression) – [DataCamp: Intro to Statistics with R: Correlation and Linear Regression] (https://www.datacamp.com/courses/intro-to-statistics-with-r-correlation-and-linear-regression) – [Intro to Statistics with R: Multiple Regression] (https://www.datacamp.com/courses/intro-to-statistics-with-r-multiple-regression) 4.1 Preliminares ¿Qué son los modelos? Los modelos simplifican la realidad con fines de comprensión o predicción. Si bien pueden ser herramientas poderosas, debemos tener en cuenta que, después de todo, no son la realidad. En consecuencia, como se dice que dijo el estadístico George Box, “Todos los modelos son incorrectos, pero algunos son útiles”. En términos generales, el modelado estadístico tiene estos dos objetivos a veces divergentes: Descripción: usar un modelo para describir la relación entre una variable de resultado de interés y una o más variables predictoras. Predicción: uso de un modelo para predecir instancias desconocidas de la variable de resultado de manera que se minimice el error predictivo fuera de la muestra. En el modelado, es posible centrarse en la descripción e ignorar la predicción, y viceversa. Por ejemplo, muchos algoritmos de aprendizaje automático son cajas negras: crean modelos que hacen un buen trabajo de predicción, pero son difíciles, si no imposibles, de interpretar y, en consecuencia, a menudo no nos ayudan a comprender las relaciones entre variables. La regresión lineal puede no ser la técnica más sofisticada, pero si se usa correctamente, su precisión predictiva compara bien con otros algoritmos más avanzados que veremos en este curso. Además, ofrece información descriptiva, en forma de coeficientes para cada variable, que son de gran utilida. La regresión lineal hace un buen trabajo con tanto descripción como predicción. En este capítulo aprenderemos estos usos de la regresión lineal. 4.2 Conceptos básicos Comencemos por presentar brevemente el modelo lineal junto con algunos de los conceptos y terminología que usaremos a lo largo del curso. Un modelo lineal es paramétrico porque asumimos que la relación entre dos variables es lineal y puede ser definida por los parámetros de una recta (el intercept y la pendiente). Comenzaremos considerando un modelo lineal simple. En la siguiente figura podemos observar cómo existe una relación lineal entre la dosis de chocolate consumida y el nivel de felicidad reportado por una muestra de individuos seleccionados al azar en una población de Barcelona. Los puntos negros muestran los datos observados para cada individuo y los blancos representan a la felicidad que tendría cada individuo según la dosis de chocolate que reporta tomar. Regresión lineal simple 4.2.1 Modelo lineal simple Un modelo lineal simple tiene un resultado (outcome, variable predictiva - en nuestro ejemplo la felicidad), \\(y\\), y un predictor, \\(x\\) (el consumo de chocolate en nuestro ejemplo). Está definido por la siguiente ecuación. \\[ y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\] donde \\(i = 1, \\ldots, n.\\) El subíndice en esta ecuación, \\(i\\), indexa las observaciones \\(n\\) en el conjunto de datos. (Pensemos en \\(i\\) como un número de fila que corresponde a los datos de un individuo). La ecuación se puede leer de la siguiente manera: el valor de la \\(i\\)-ésima variable resultado, \\(y_i\\), está definido por una intercept, \\(\\beta_0\\), más una pendiente, \\(\\beta_1\\), multiplicada por la variable predictora \\(i\\)-ésima, \\(x_i\\). Estos elementos definen la parte sistemática o determinista del modelo. Sin embargo, debido a que el mundo es incierto y contiene aleatoriedad, sabemos que el modelo será incorrecto (estará sujeto a error). Para describir completamente los datos, necesitamos un término de error, \\(\\epsilon_i\\), que también está indexado por fila. El término de error es la parte estocástica o aleatoria del modelo. \\(\\epsilon_i\\) mide la distancia entre los valores ajustados o esperados del modelo — calculados a partir de la parte determinista del modelo — y los valores reales. Los errores en un modelo lineal, también conocidos como residuales del modelo, son la parte de los datos que permanece sin explicar por la parte determinista del modelo. Uno de los supuestos clave de un modelo lineal es que los residuos se distribuyen normalmente con media = 0 y varianza = \\(\\sigma^2\\), que denotamos, en notación matricial, como \\(N (0, \\sigma ^ 2)\\). 4.2.2 Regresión lineal multivariante Podemos agregar predictores adicionales, \\(p\\), a un modelo lineal simple, convirtiéndolo en un modelo lineal multivariante, que definimos de la siguiente manera: \\[ y_i = \\beta_0 + \\beta_1 x_ {i1} + \\cdots + \\beta_p x_ {ip} + \\varepsilon_i, \\] donde \\(i = 1, \\ldots, n\\) y \\(p = 1, \\ldots, p.\\) En esta ecuación \\(y_i\\) es nuevamente la variable resultado \\(i\\)-ésima, \\(\\beta_0\\) es la intercept, \\(\\beta_1\\) es el coeficiente de la primera variable predictora, \\(x_{1}\\), \\(\\beta_p\\) es el coeficiente de la variable predictora \\(p\\)-ésima, \\(x_{p}\\), y \\(\\epsilon_i\\) representa la parte estocástica del modelo, los residuos, indexados por fila. La parte determinista del modelo se puede resumir como \\(X \\beta\\), una matriz \\(p\\) x \\(n\\), que llamaremos el “predictor lineal”. 4.2.3 Incertidumbre La incertidumbre es intrínseca al modelado estadístico. Distinguimos entre Incertidumbre de estimación e Incertidumbre fundamental : La incertidumbre de la estimación se deriva del desconocimiento de los parámetros \\(\\beta\\). Disminuye a medida que \\(n\\) aumenta y los \\(SE\\)s se reducen. La incertidumbre fundamental se deriva del componente estocástico del modelo, \\(\\epsilon\\). Existe sin importar lo que haga el investigador, sin importar como de grande sea el tamaño muestral \\(n\\). Podemos reducir la incertidumbre fundamental con predictores elegidos inteligentemente, pero nunca podremos eliminarla. 4.3 Ajuste de un modelo lineal Para ajustar un modelo lineal usamos la función lm(). (La función glm() también se ajusta a un modelo lineal por defecto, definido por family = gaussian. También usaremosglm() para ajustar una regresión logística, confamily = binomial). Por ejemplo, usemos el conjunto de dataos mtcars para averiguar si el consumo de combustible (mpg) está correlacionado con el peso del coche (wt). En R deberíamos ejecutar: data(mtcars) (simple_model &lt;- lm(mpg ~ wt, data = mtcars)) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Coefficients: ## (Intercept) wt ## 37.285 -5.344 La ecuación del modelo es: \\(\\widehat {mpg} = 37.285 - 5.344wt\\). (La notación del sombrero, \\(\\widehat {mpg}\\), significa “estimación de”). Sin embargo, con el término de error incluido, ya no estamos estimando mpg sino describiéndolo exactamente: \\(mpg = 37.285 - 5.344wt + error\\) (Por lo tanto, no hay notación de sombrero). Los componentes del modelo se pueden extraer del objeto del modelo usando adjust(), o, de manera equivalente en este caso, predict() y residuals(): mtcars_new &lt;- mtcars %&gt;% mutate(cars = rownames(mtcars), fitted = fitted(simple_model), residuals = residuals(simple_model)) %&gt;% dplyr::select(cars, mpg, wt, fitted, residuals) head(mtcars_new) ## cars mpg wt fitted residuals ## 1 Mazda RX4 21.0 2.620 23.28261 -2.2826106 ## 2 Mazda RX4 Wag 21.0 2.875 21.91977 -0.9197704 ## 3 Datsun 710 22.8 2.320 24.88595 -2.0859521 ## 4 Hornet 4 Drive 21.4 3.215 20.10265 1.2973499 ## 5 Hornet Sportabout 18.7 3.440 18.90014 -0.2001440 ## 6 Valiant 18.1 3.460 18.79325 -0.6932545 El modelo se puede utilizar para calcular valores ajustados para coches individuales en el conjunto de datos. Por ejemplo, el valor ajustado para el Mazda RX4, \\(\\widehat {mpg_1}\\), se puede derivar de la ecuación del modelo, \\(\\beta_0 + \\beta_1 x_ {i1}\\): 37.29 - 5.34 x 2.62 = 23.28. (El valor real del Mazda RX4, calculado a partir del modelo, sería: 37.29 - 5.34 x 2.62 + 2.28 = 21). El modelo también se puede utilizar para la predicción. ¿Cuál sería el mpg para un coche que pesa 5000 libras? Según el modelo: 37,29 - 5,34 x 5 = 10.56. 4.3.1 Residuos Los residuales del modelo — representados por los segmentos de línea vertical en el gráfico siguiente — son las diferencias entre los valores ajustados y reales de mpg. ggplot(mtcars_new, aes(x = wt, y = mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;lightgrey&quot;) + geom_segment(aes(xend = wt, yend = fitted), alpha = .2) + geom_point() + geom_point(aes(y = fitted), shape = 1) + ggtitle(&quot;Residuales del modelo para mpg ~ wt&quot;) Podemos resumir los residuos con una medida llamada suma de cuadrados residual (RSS), que se calcula restando los resultados reales, \\(y_i\\), de los valores ajustados, \\(\\hat {y} _i\\), elevando al cuadrado esas diferencias, luego sumando los cuadrados. \\[ \\operatorname {RSS} = \\sum_ {i = 1} ^ n ((\\beta_0 + \\beta_1x_i) - y_i) ^ 2 = \\sum_{i = 1}^n(\\hat {y} _i - y_i) ^ 2 \\] Al resumir los errores del modelo, RSS nos permite cuantificar el rendimiento del modelo con un solo número: rss &lt;- function(fitted, actual){ sum((fitted - actual)^2) } rss(fitted(simple_model), mtcars$mpg) ## [1] 278.3219 4.3.2 Coeficientes de interpretación ¿Cómo interpretamos la salida de la función lm()? Comencemos con el modelo simple de mpg. intercept: 37.29 representa el valor predicho de mpg cuando wt es 0. Dado que wt no puede ser igual a 0. El intercept no es interpretable en este modelo. Para hacerlo interpretable, necesitamos centrar la variable wt en 0, lo que podemos hacer fácilmente restando la media de wt de cada observación (\\(x_ {centrado} = x - \\ bar {x}\\)). Esta es una transformación lineal que cambiará la escala del predictor y, por lo tanto, \\(\\beta_0\\) también, pero no el ajuste del modelo: \\(\\beta_1\\) permanecerá igual (-5,34) al igual que RSS (278,32). Después de la transformación, el peso promedio del coche es 0 y el intercept representa las millas por galón pronosticadas para coches de peso promedio. mtcars$wt_centered &lt;- mtcars$wt - mean(mtcars$wt) (simple_model &lt;- lm(mpg ~ wt_centered, data = mtcars)) ## ## Call: ## lm(formula = mpg ~ wt_centered, data = mtcars) ## ## Coefficients: ## (Intercept) wt_centered ## 20.091 -5.344 rss(fitted(simple_model), mtcars$mpg) ## [1] 278.3219 Ahora el intercept, 20.09, es significativa y representa el valor predicho de mpg cuando wt_centered es 0 — es decir, cuando wt es promedio. Hay dos formas de interpretar los coeficientes en un modelo lineal: Contrafactual: el coeficiente representa el cambio predicho en el resultado asociado con un aumento de 1 unidad en el predictor, mientras se mantienen constantes los demás predictores (en el caso multivariable). Predictivo: el coeficiente representa la diferencia pronosticada en el resultado entre dos grupos que difieren en 1 unidad en el predictor, mientras se mantienen constantes los otros predictores. Normalmente los coeficientes del modelo se suelen interpretar de acuerdo con el paradigma contrafáctico. Por lo tanto, wt_centered: -5.34 representa el cambio previsto en el resultado, mpg, asociado con un aumento de 1 unidad en wt_centered. Agreguemos un segundo predictor al modelo, una versión binaria de caballos de fuerza (hp_bin), que definiremos como 0 para valores de hp que están por debajo del promedio y 1 para valores mayores o iguales que el promedio. mtcars$hp_bin &lt;- ifelse(mtcars$hp &lt; mean(mtcars$hp), 0, 1) (multivariable_model &lt;- lm(mpg ~ wt_centered + hp_bin , data = mtcars)) ## ## Call: ## lm(formula = mpg ~ wt_centered + hp_bin, data = mtcars) ## ## Coefficients: ## (Intercept) wt_centered hp_bin ## 21.649 -4.168 -3.324 rss(fitted(multivariable_model), mtcars$mpg) ## [1] 231.3121 Este modelo multivariante es una mejora con respecto al modelo simple ya que tiene un RSS menor. intercept: 21,65 representa el mpg predicho cuando los predictores continuos o binarios son iguales a 0 o (no aplicable en este caso) cuando las variables de los factores están en su nivel de referencia. El intercept es el mpg pronosticado por el modelo para autos de peso promedio que tienen caballos de fuerza por debajo del promedio. wt_centered: -4,17 representa el cambio previsto en mpg asociado con un aumento de 1 unidad en wt_centered (digamos, de 1 a 2) mientras se mantiene constante el otro predictor, hp_bin. Los coeficientes de regresión multivariable capturan cómo el resultado varía de manera única con un predictor dado, después de tener en cuenta los efectos de todos los demás predictores. En la práctica, esto significa que el coeficiente que describe la relación entre mpg y wt_centrado se ha promediado en los niveles hp_bin, por lo que es igual en cada nivel de hp_bin. hp_bin: -3.32 representa el cambio previsto en mpg asociado con un aumento de 1 unidad en hp_bin (de 0 a 1) mientras se mantiene constante el otro predictor, wt_centered. 4.3.3 Interacciones Podemos agregar una interacción a este modelo. A menudo, la relación entre un predictor y un resultado puede depender del nivel de otra variable predictiva. Por ejemplo, la pendiente de la recta de regresión que define la relación entre wt_centrado y mpg puede variar con los niveles de hp_bin. Si es así, decimos que existe una interacción entre wt_centered y hp_bin al predecir mpg. Para incluir una interacción entre dos variables en la fórmula del modelo, simplemente reemplazamos “+” por “*” en la fórmula del modelo. Esta fórmula, mpg ~ wt_centered * hp_bin, es exactamente equivalente a mpg ~ wt_centered + wt_centered * hp_bin, o a mpg ~ wt_centered + hp_bin + wt_centered*hp_bin ya que lm () agrega automáticamente el efecto principal junto con la interacción. También se puede usar “:” para el término exacto de la interacción `mpg ~ wt_centered + hp_bin + wt_centered:hp_bin. Por “efecto principal” nos referimos a los términos que interactúan entre si. En este modelo, el efecto de interacción es wt_centered:hp_bin, mientras que wt_centered y hp_bin por sí mismos son los efectos principales. (multivariable_model &lt;- lm(mpg ~ wt_centered * hp_bin, data = mtcars)) ## ## Call: ## lm(formula = mpg ~ wt_centered * hp_bin, data = mtcars) ## ## Coefficients: ## (Intercept) wt_centered hp_bin wt_centered:hp_bin ## 20.276 -6.391 -3.163 3.953 rss(fitted(multivariable_model), mtcars$mpg) ## [1] 170.3792 RSS mejora una vez más. Las interacciones pueden ser difíciles de interpretar es por ello que la visualización ayuda a comprender qué está sucediendo. ggplot(mtcars, aes(wt_centered, mpg, col = factor(hp_bin), group = factor(hp_bin))) + geom_point() + stat_smooth(method=&quot;lm&quot;, se = F) + ggtitle(&quot;mpg ~ wt_centered * hp_bin&quot;) Podemos ver que la relación entre wt y mpg depende de los niveles de hp_bin: las pendientes de las rectas de regresión difieren. Las rectas de regresión no paralelas indican la presencia de una interacción. Observamos una relación más fuerte entre el peso y las millas por galón entre los autos con caballos de fuerza por debajo del promedio (una relación negativa más fuerte) que entre los autos con más caballos de fuerza. Las rectas de regresión para hp_bin se vuelven más planas a medida que aumenta wt_centrado. NOTA: la presencia de una interacción cambia la interpretación de los efectos principales. En un modelo sin interacción, los efectos principales son independientes de los valores particulares de los otros predictores. Por el contrario, una interacción hace que los efectos principales dependan de valores particulares de los otros predictores. wt_centered:hp_bin: 3.95 representa la diferencia en la pendiente de wt_centered para hp_bin = 1 en comparación con hp_bin = 0. En otras palabras, cuando aumentamos hp_bin de 0 a 1, se predice que la pendiente de la rectas de regresión para wt_centered aumentar en 3,95. O, cuando aumentamos wt_centrado en 1, se predice que la rectas de regresión para hp_bin aumentará en 3,95. wt_centered: -6.39 representa el cambio predicho en mpg asociado con un aumento de 1 unidad en wt entre aquellos coches donde hp_bin = 0. hp_bin: -3.16 representa el cambio previsto en mpg asociado con un aumento de 1 unidad en hp_bin de 0 a 1 entre los coches con wt_centered = 0 (promedio). Puede resultar instructivo ver qué está haciendo lm () en segundo plano para ajustarse a este modelo. El comando model.matrix () muestra cómo se ha reformateado la matriz del predictor para la regresión: head(model.matrix(multivariable_model)) ## (Intercept) wt_centered hp_bin wt_centered:hp_bin ## Mazda RX4 1 -0.59725 0 0.00000 ## Mazda RX4 Wag 1 -0.34225 0 0.00000 ## Datsun 710 1 -0.89725 0 0.00000 ## Hornet 4 Drive 1 -0.00225 0 0.00000 ## Hornet Sportabout 1 0.22275 1 0.22275 ## Valiant 1 0.24275 0 0.00000 El intercept es un vector de 1s. El vector para el término de interacción, wt_centered: hp_bin, consiste simplemente en el producto de los dos vectores con las componentes de cada variable 4.4 Estimación por mínimos cuadrados Para el modelo \\(y = X \\beta + \\epsilon\\), donde \\(\\beta\\) es el vector de coeficientes ajustados y $$ es el vector de residuos del modelo, la estimación de mínimos cuadrados es \\(\\hat {\\beta}\\) que minimiza RSS para los datos dados \\(X, y\\). Podemos expresar la estimación de mínimos cuadrados como \\(\\ hat {\\beta} = (X&#39;X) ^ {- 1} X&#39;y\\), donde \\(X&#39;\\) es la transposición de la matriz de \\(X\\). A continuación podemos ver cómo se deriva esta fórmula2 \\[ RSS = \\epsilon ^ 2 = (y - X \\beta) &#39;(y - X \\beta) \\] \\[ RSS = y&#39;y - y&#39;X \\beta - \\beta&#39;X&#39;y + \\beta&#39;X&#39;X \\beta \\] \\[ RSS = y&#39;y - (2y&#39;X) \\beta + \\beta &#39;(X&#39;X) \\beta \\] Según apunta el autor: “Aunque la multiplicación de matrices generalmente no es conmutativa, cada producto [arriba] es 1 x 1, por lo que \\(y&#39;X \\beta = \\beta&#39;X&#39;y\\)”. Para minimizar RSS encontramos la derivada parcial con respecto a \\(\\beta\\): \\[ \\frac{\\partial RSS}{\\partial\\beta}= 0 - 2X&#39;y + 2X&#39;X\\beta \\] Establecemos esta derivada igual a 0 y resolvemos $$: \\[ X&#39;X \\beta = X&#39;y \\] \\[ \\beta = (X&#39;X) ^ {- 1} X&#39;y \\] Podemos usar esta ecuación y la matriz del modelo para el modelo multivariable para estimar \\(\\hat{\\beta}\\) para mpg ~ wt_centered * hp_bin + hp_centered: X &lt;- model.matrix(multivariable_model) y &lt;- mtcars$mpg solve(t(X) %*% X) %*% t(X) %*% y ## [,1] ## (Intercept) 20.276155 ## wt_centered -6.390834 ## hp_bin -3.162983 ## wt_centered:hp_bin 3.953027 Este método devuelve las mismas estimaciones de coeficientes que lm (). 4.5 Medidas adicionales de ajuste del modelo Como hemos visto, RSS nos permite comparar qué tan bien se ajustan los modelos a los datos. Una medida relacionada es la raíz del error cuadrático medio (RMSE por sus siglas en inglés), la raíz cuadrada del promedio de los errores cuadráticos: \\[ \\operatorname{RMSE}= \\sqrt{\\frac{\\sum_{i=1}^n ((\\beta_0 + \\beta_1x_i) - y_i)^2}{n}} \\] \\[ = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat{y}_i - y_i)^2}{n}} \\] Lo bueno de RMSE es que, a diferencia de RSS, devuelve un valor que está en la escala del variable resultado. rss(fitted(multivariable_model), mtcars$mpg) ## [1] 170.3792 rmse &lt;- function(fitted, actual){ sqrt(mean((fitted - actual)^2)) } rmse(fitted(multivariable_model), mtcars$mpg) ## [1] 2.307455 En promedio, entonces, este modelo tiene una diferencia de aproximadamente 2.31 mpg por cada coche. \\(R^2\\) es otra medida de ajuste del modelo que es conveniente porque es una medida estandarizada — escalada entre 0 y 1 — y, por lo tanto, es comparable en todos los contextos. \\[ R^2 = 1 - \\frac{SS_\\text{resid}}{SS_\\text{tot}}, \\] donde \\(SS_\\text{tot} = \\sum_i (y_i- \\bar{y}) ^ 2\\) y \\(SS_\\text{res} = \\sum_i (y_i - \\hat {y} _i) ^ 2\\). En resumen: \\(R^2\\) representa la variación en la variable de resultado explicada por el modelo como una proporción de la variación total. En la gráfica de abajo, el panel de la izquierda, TSS, sirve como denominador para calcular \\(R^2\\), y el panel de la derecha, RSS, es el numerador. require(gridExtra) mtcars$mean &lt;- mean(mtcars$mpg) plot1 &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_hline(yintercept=mean(mtcars$mpg), col = 2) + geom_segment(aes(xend = wt, yend = mean), alpha = .2) + geom_point() + ggtitle(paste(&quot;Total Sum of Squares (TSS) \\n&quot;,tss)) plot2 &lt;- ggplot(mtcars, aes(wt, mpg)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, col=2) + geom_segment(aes(xend = wt, yend = fitted(lm(mpg~wt, data=mtcars))), alpha = .2) + geom_point() + ggtitle(paste(&quot;Residual Sum of Squares (RSS) \\n&quot;,rss)) grid.arrange(plot1, plot2, ncol=2) Para nuestro modelo lineal simple, mpg ~ wt, \\(R^2\\) era de .75, que coincide con nuestro cálculo aquí: 1 - 278/1126 = .75. Esto significa que wt explica el 75% de la variación total en mpg. Cuanto mejor se ajusta la regresión lineal a los datos en comparación con el promedio simple, más se acerca el valor de \\(R^2\\) a 1. Para la regresión lineal simple, \\(R^2\\) es como la correlación al cuadrado entre el resultado y el predictor. cor(mtcars$mpg, mtcars$wt)^2 ## [1] 0.7528328 Un problema con el \\(R^2\\) es que agregar variables al modelo tiende a mejorarlo aunque las nuevas variables no sean relevantes. Añadir más variables puede conducir a un sobreajuste. Se ha desarrollado una variante de \\(R^2\\) que penaliza la medida para predictores adicionales: \\(R^2\\)ajustados. \\[ \\bar R^2 = {1-(1-R^2){n-1 \\over n-p-1}} \\bar R^2 = {R^2-(1-R^2){p \\over n-p-1}}, \\] donde \\(n\\) es el número de observaciones en el conjunto de datos y \\(p\\) es el número de predictores en el modelo. Es fácil calcular este resultado utilizando la función summary() summary(multivariable_model) ## ## Call: ## lm(formula = mpg ~ wt_centered * hp_bin, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5837 -1.4371 -0.8214 1.4517 5.6228 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.2762 0.8323 24.363 &lt; 2e-16 *** ## wt_centered -6.3908 0.9367 -6.823 2.06e-07 *** ## hp_bin -3.1630 1.1971 -2.642 0.01333 * ## wt_centered:hp_bin 3.9530 1.2492 3.164 0.00373 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.467 on 28 degrees of freedom ## Multiple R-squared: 0.8487, Adjusted R-squared: 0.8325 ## F-statistic: 52.35 on 3 and 28 DF, p-value: 1.324e-11 En este caso \\(\\bar R^2=0.8325\\) lo que indica que nuestro modelo explica un 83.25% de la variabilidad observada de la variable ‘mpg’. 4.6 Sesgo, variación, sobreajuste ¿Qué entendemos por “sobreajuste”? Los siguientes son conceptos clave para pensar en el rendimiento del modelo, a los que volveremos a lo largo del curso: Rendimiento en la muestra: cómo se comporta el modelo en los datos que se utilizaron para construirlo. Rendimiento fuera de la muestra: cómo se comporta el modelo cuando encuentra nuevos datos. Si el modelo funciona mejor dentro de la muestra que fuera de la muestra, entonces decimos que el modelo sobreajusta los datos de la muestra o de entrenamiento. El sobreajuste ocurre cuando un modelo se ajusta a la muestra demasiado bien: el modelo ha sido optimizado para capturar las idiosincrasias — el ruido aleatorio — de la muestra. Sesgo se refiere a una alta precisión predictiva en la muestra. El sesgo bajo es bueno. Varianza se refiere a una mayor precisión predictiva dentro de la muestra que fuera de la muestra. La varianza baja es buena. Un modelo que sobreajusta tiene un sesgo bajo y una gran varianza. La compensación de sesgo-varianza se refiere a la idea de que no se puede tener un sesgo bajo y una varianza baja a la vez. Nos protegeremos contra — o evaluaremos la cantidad de — sobreajuste usando una técnica llamada validación cruzada que veremos más adelante. 4.7 Regresión como estimación de una media condicional Dadas las complejidades anteriores, se podría pensar que el uso de la regresión lineal sólo es útil para nada más (¡y nada menos!) estimar una media condicional. Pero, ¿qué es una media condicional? Consideremos el siguiente ejemplo. En 2011 se inició un programa de uso compartido de bicicletas en USA y se recopilaron datos durante 2011 y 2012 sobre el uso estacional de bicicletas y las condiciones climáticas. Estos datos se encuentran recogidos en el fichero day.csv. La variable de resultado que nos interesa es “count”— el número total de ciclistas que alquilan bicicletas en un día determinado. El conjunto de datos tiene una fila para cada día, con variables para (entre otras) estación, año, mes, feriado, día de la semana, temperatura, temperatura percibida, humedad y velocidad del viento (nombradas en inglés). day &lt;- read.csv(&quot;data/day.csv&quot;) day &lt;- day %&gt;% dplyr::select(count = cnt, season, year= yr, month = mnth, holiday, weekday, temperature = temp, atemp, humidity = hum, windspeed) glimpse(day) ## Rows: 731 ## Columns: 10 ## $ count &lt;int&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 1263, 1162, 1406, 1421, 1248, 1204, 1000, 683, 1... ## $ season &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ year &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,... ## $ holiday &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ weekday &lt;int&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5,... ## $ temperature &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.2043480, 0.1965220, 0.1650000, 0.1383330, 0.150... ## $ atemp &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.2332090, 0.2088390, 0.1622540, 0.1161750, 0.150... ## $ humidity &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261, 0.498696, 0.535833, 0.434167, 0.482917, 0.68... ## $ windspeed &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.0895652, 0.1687260, 0.2668040, 0.3619500, 0.223... Una pregunta exploratoria inicial es: ¿cómo varía el uso de la bicicleta por año? Podemos responder a esta pregunta simplemente calculando un promedio para cada año: day %&gt;% mutate(year = ifelse(year == 0, 2011, 2012)) %&gt;% group_by(year) %&gt;% dplyr::summarize(`average ridership` = round(mean(count))) ## # A tibble: 2 x 2 ## year `average ridership` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 2011 3406 ## 2 2012 5600 El número de pasajeros promedio en este resumen representa una media condicional: la media de la variable de recuento, condicional al año. ¿Qué nos dice la regresión lineal sobre el promedio de pasajeros (riders) por año? lm(count ~ year, data = day) ## ## Call: ## lm(formula = count ~ year, data = day) ## ## Coefficients: ## (Intercept) year ## 3406 2194 La salida del modelo incluye un intercept y un coeficiente para el año. El intercept es el promedio de la variable de resultado cuando los predictores numéricos son iguales a cero. Por lo tanto, 3406 es el número de pasajeros promedio cuando año = 0 (es decir, 2011), y el coeficiente para el año, 2194, es el aumento esperado en el número de pasajeros asociado con un aumento de 1 unidad en el año (es decir, cuando el año pasa de 0 a 1 ). Por lo tanto, el número promedio de pasajeros en el año 1 (2012) es solo la suma de los dos coeficientes — 3406 + 2194 = 5600 — que es idéntica a la media condicional para 2012 que calculamos anteriormente usandodplyr . En general, dadas dos variables aleatorias, X e Y (piense: año y número de pasajeros), podemos definir la media condicional como el valor esperado o promedio de Y dado que X está restringido a tener un valor específico, \\(x\\), a partir de su rango: \\(\\mathbf {E} [Y \\mid X = x]\\). Ejemplo: \\(\\mathbf{E} [Ridership \\mid Year = 2012]\\) es el número promedio de pasajeros dado ese año = 2012. Una media condicional tiene un valor descriptivo — sabemos que el coeficiente \\(\\beta\\) para el año de nuestro modelo, 2194, representa la relación entre el número de pasajeros y el año, con la magnitud o el valor absoluto del coeficiente que indica la fuerza de la relación, positiva o negativa. Los coeficientes también se pueden utilizar para la predicción. ¿Cuántos ciclistas más deberíamos esperar en 2013? Utilice el modelo: \\(\\mathbf {E} [riders \\mid year = 2013]\\) es igual al número en 2012, 5600, más el coeficiente del año: 5600 + 2194 = 7794. El modelo nos permite predecir, pero debemos recordar que no hay nada mágico en la predicción. Deberíamos pensar críticamente al respecto. Por un lado, asume una tendencia constante año tras año. ¿Es esta una suposición razonable? 4.8 La función de regresión Consideremos la cantidad media de pasajeros condicionada a la temperatura (que en este conjunto de datos se ha normalizado y convertido a grados Celsius). Podemos definir una función que devolverá la media condicional. Para cualquier temperatura, \\(t\\), defina \\(\\mu (t) = \\mathbf {E} [Riders \\mid Temperature = t]\\), que es el número medio de pasajeros cuando temperatura = \\(t\\). Dado que podemos variar \\(t\\), esto es de hecho una función, y se conoce como la función de regresión que relaciona a los pasajeros con la temperatura. Por ejemplo, \\(\\mu\\) (.68) es el número medio de pasajeros cuando \\(t\\)= .68 y \\(\\mu\\) (.05)es el número medio de pasajeros cuando \\(t\\)= .05, etc. Debemos tener en cuenta que el valor real de \\(\\mu\\) (.68) se desconoce porque es un valor de población. Existe, pero no en nuestra muestra. Entonces, nuestra estimación, \\(\\hat {\\ mu}\\) (. 68), debe basarse en los pares de Riders-Temperature que tenemos en nuestros datos: \\((r_ {1}, t_ {1}), ..., ( r_ {731}, t_ {731})\\). ¿Cómo podemos hacer esto exactamente? Un enfoque sería simplemente calcular las medias condicionales relevantes a partir de nuestros datos. Para encontrar \\(\\hat {\\mu} (t)\\) usando este método, simplificaremos los datos redondeando la temperatura a dos lugares decimales. day %&gt;% group_by(temperature = round(temperature, 2)) %&gt;% dplyr::summarize(mean = round(mean(count))) ## # A tibble: 77 x 2 ## temperature mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.06 981 ## 2 0.1 1201 ## 3 0.11 2368 ## 4 0.13 1567 ## 5 0.14 1180 ## 6 0.15 1778 ## 7 0.16 1441 ## 8 0.17 1509 ## 9 0.18 1597 ## 10 0.19 2049 ## # ... with 67 more rows Sin embargo, se puede observar que faltan valores en la secuencia. Aquí hay un gráfico que muestra las brechas en los datos. day %&gt;% group_by(temperature = round(temperature, 2)) %&gt;% dplyr::summarize(mean = round(mean(count))) %&gt;% dplyr::right_join(data.frame(temperature=seq(.01, .9, .01)), by = &quot;temperature&quot;) %&gt;% ggplot(aes(temperature, mean)) + geom_line() + ggtitle(&quot;Estimated mean daily riders (with missing observations)&quot;) + labs(x = &quot;temperature&quot;, y = &quot;mean riders&quot;) Este enfoque para estimar la función de regresión, \\(\\hat{\\mu} (t)\\) tendrá problemas cuando, por ejemplo, queremos predecir el número de pasajeros a una temperatura para la que no tenemos datos. Usar medias condicionales para estimar \\(\\hat {\\mu} (t)\\) es un enfoque no paramétrico. Es decir, no asumimos nada sobre la forma de la función desconocida \\(\\mu(t)\\) si se trazara en un gráfico, sino que simplemente la estimamos directamente a partir de nuestros datos. La regresión de K-vecinos más cercanos (KNN) es una generalización de este enfoque no paramétrico. Este tipo de regresión es muy útil cuando queremos describir cuál es la relación entre nuestros datos sin asumir que dicha relación es lineal (que puede que sea el caso). Debemos tener en cuenta que podríamos hacer algunas suposiciones sobre esa forma, posiblemente mejorando nuestras estimaciones, lo que haría que nuestro enfoque sea paramétrico, como en el caso de la regresión lineal. 4.9 Estimación no paramétrica de la función de regresión: regresión KNN A veces no es posible calcular buenas medias condicionales para el resultado debido a la falta de valores de predicción. Supongamos que queremos encontrar \\(\\hat{\\mu} (. 12)\\). Resulta que no hubo días en nuestro conjunto de datos en los que la temperatura fuera de .12. El algoritmo KNN resuelve este problema usando las \\(k\\) observaciones más cercanas a \\(t\\)= .12 para calcular la media condicional, \\(\\hat {\\mu} (. 12)\\). Si definimos \\(k\\)= 4, entonces tomaríamos los cuatro valores más cercanos a .12 en el conjunto de datos — .1275, .134783, .138333, .1075. (“Más cercano” en este caso se define como la distancia euclidiana, que en un espacio unidimensional, una recta numérica, es simplemente: \\(\\sqrt {(xy) ^ 2}\\).) Estos \\(k\\)= 4 valores más cercanos se usarían para calcular \\(\\hat{\\mu} (. 12)\\) calculando el promedio. Establecer el valor de \\(k\\) es obviamente una decisión crítica. Si usamos \\(k\\)= 100, por ejemplo, nuestras estimaciones podrían no ser muy buenas. Y quizás \\(k\\)= 4 sea demasiado bajo — podría llevar a un sobreajuste. En el caso de \\(k\\)= 4, el error dentro de la muestra (sesgo) puede ser bajo, pero el error al predecir nuevas observaciones (varianza) puede ser alto. Aquí, por ejemplo, hay una gráfica de los valores ajustados de un modelo KNN de la temperatura de los pasajeros cuando \\(k\\)= 4. El sesgo en este modelo es presumiblemente bajo porque el ajuste es muy flexible: los valores ajustados están muy cerca de los valores reales. El problema es que la función de regresión KNN en \\(k\\)= 4 podría estar haciendo demasiado buen trabajo al describir la muestra. Cuando este modelo encuentra nuevos datos, sin las mismas idiosincrasias, su rendimiento puede ser muy pobre, con una gran variación. Es posible que el modelo esté sobre-ajustado a la muestra. La compensación de sesgo-varianza expresa esta idea: cuando el sesgo es bajo, es probable que la varianza sea alta, y viceversa. A continuación se muestra un ejemplo de un sesgo más alto, posiblemente un caso de varianza menor cuando \\(k\\)= 40. r &lt;- day r$temp_rounded &lt;- round(r$temp,2) library(dplyr) knn_fit &lt;- knn.reg(train=r$temperature, y=r$count, k=40, algorithm=&quot;brute&quot;) r$knn &lt;- knn$pred est &lt;- r %&gt;% group_by(temp_rounded) %&gt;% dplyr::summarize(count=mean(count), knn=mean(knn)) plot(x=est$temp_rounded, y=est$count, pch=20, ylab=&quot;mean riders&quot;, xlab=&quot;temperature&quot;, main =&quot;KNN fit for riders ~ temperature, k = 40&quot;) lines(x=est$temp_rounded, y=est$knn, col=2) El sesgo es mayor aquí porque el error del modelo en la muestra es visiblemente mayor que en el caso de \\(k\\)= 4, pero por esa misma razón es probable que la varianza sea menor. No hay forma de lograr un sesgo bajo y una varianza baja simultáneamente. Todo lo que puede hacer es tratar de equilibrar los dos, aceptando un sesgo moderado para lograr un mejor rendimiento fuera de la muestra. La técnica que usamos para lograr este equilibrio es la validación cruzada, que cubriremos más adelante en el curso. Por ahora podemos notar que el mejor valor para \\(k\\) en la regresión KNN es el que minimiza la varianza, no el sesgo. Como referencia, aquí hay un código para ajustar una regresión KNN usando el paquete caret en R. Usaremoscaret frecuentemente en el curso porque proporciona una sintaxis consistente para ajustar una amplia gama de modelos (incluyendo, si quisiéramos, regresión lineal) y porque, muy convenientemente, ejecuta una validación cruzada en segundo plano para elegir parámetros de modelo óptimos como \\(k\\). Pero esto lo veremos más adelante. 4.10 Estimación paramétrica de la función de regresión: regresión lineal Con la regresión KNN no asumimos nada sobre la forma de la función de regresión, sino que la estimamos directamente a partir de los datos. Supongamos ahora que \\(\\mu(t)\\) es lineal y se puede describir con los parámetros de una recta: \\(\\mu (t) = \\beta_0 + \\beta_1t\\), donde \\(\\beta_0\\) es el intercept de la recta y \\(\\beta_1\\) es la pendiente. En este caso, entonces, \\(\\widehat{riders} = \\beta_0 + \\beta_1temperature\\). Dado que \\(\\mu(t)\\) es una función poblacional (es decir, promedio), los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) son valores de población y son desconocidos, pero podemos estimarlos (también) con versosimilitud. Hagamos un breve repaso de este concepto tan importante en estadística. La función de verosimilitud para un vector de parámetros \\(\\boldsymbol{\\Theta}\\) dada una muestra aleatoria \\(\\boldsymbol{x}\\) con una distribución asumida se define como: \\[ L(\\boldsymbol{\\Theta} | \\boldsymbol{x}) = \\prod_{i=1}^{n} f(x_i | \\boldsymbol{\\Theta}), \\] donde \\(x_i\\) representa uno de los elementos de la muestra aleatoria y \\(f\\) es la función de masa/densidad de la distribución de la cual se obtuvo \\(\\boldsymbol{x}\\). Por otro lado, la función de log-verosimilitud \\(l\\) se define como el logaritmo de la función de verosimilitud \\(L\\), es decir \\[ l(\\boldsymbol{\\Theta} | \\boldsymbol{x}) = \\log L(\\boldsymbol{\\Theta} | \\boldsymbol{x}) = \\sum_{i=1}^{n} \\log f(x_i | \\boldsymbol{\\Theta}) \\] Los parámetros de esta distribución se pueden estimar mediante el método de máxima verosimilitud. El objetivo de este método es encontrar los valores de \\(\\boldsymbol{\\Theta}\\) que maximizan \\(L\\) o \\(l\\) y valores encontrados se representan por \\(\\hat{\\boldsymbol{\\Theta}}\\). Veamos como estimar los parámetros para unos datos que pensamos que siguen una distribución de Poisson. Hemos recogido el número de veces que 20 alumnos escogidos al azar no han asistido a una clase durante un semestre del total de alumnos del Grado de Estadística. Queremos estimar cuál es el parámetro \\(\\lambda\\) que nos cuantificaría cual es el promedio de no asistencia semetral de los alumnos de nuestra clase alumnos &lt;- c(6, 6, 6, 0, 5, 2, 4, 4, 5, 5, 5, 6, 4, 5, 2, 5, 3, 2, 3, 2) La fución de log-versomilitud para nuestro problema la podemos escribir en R como lver_poisson &lt;- function(lambda, x){ ans &lt;- sum(dpois(x, lambda, log=TRUE)) return(ans) } Podemos encontrar el máximo de forma visual lambdas &lt;- seq(1,15, by=0.5) lv &lt;- sapply(lambdas, function(x) {lver_poisson(x, alumnos)}) df &lt;- tibble(lambda = lambdas, lv= lv) df %&gt;% ggplot(aes(x=lambda, y=lv))+ geom_point(size=4,color=&quot;dodgerblue&quot;)+ xlab(&quot;Lambda&quot;) + ylab(&quot;Log-Verosimilitud&quot;)+ theme_bw(base_size = 16) + geom_vline(xintercept = lambdas[which.max(lv)], color=&quot;red&quot;,size=2) En general, podemos usar optim() para buscar el máximo de cualquier función. En este caso optim(par=2, fn=lver_poisson, x=alumnos, control=list(fnscale=-1)) ## $par ## [1] 4 ## ## $value ## [1] -40.02868 ## ## $counts ## function gradient ## 30 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL EJERCICIO (Entrega en Moodle: P2-Regresión lineal función): Consideremos el siguiente modelo de regresión: \\[\\begin{align*} \\textrm{altura}_i &amp;\\sim N(\\mu_i, \\sigma^2), \\\\ \\mu_i &amp;= 105 + 0.9 \\textrm{peso}_i, \\\\ \\sigma &amp;= 8, \\\\ \\textrm{peso} &amp;\\sim Normal(45, 144). \\end{align*}\\] El siguiente código permite simular un conjunto de datos de valores con la estructura anterior. r n &lt;- 1000 peso &lt;- rnorm(n=n, mean=45, sd=12) altura &lt;- rnorm(n=n, mean=105 + 0.9 * peso, sd=8) Estima los parámetros del modelo lineal (\\(\\beta_0\\) y \\(\\beta_1\\)) utilizando R tal que minimicen el RSS \\[ \\operatorname {RSS} = \\sum_ {i = 1} ^ n ((\\beta_0 + \\beta_1\\textrm{peso}_i) - \\textrm{altura}_i) ^ 2 \\] OPCIONAL: Haz lo mismo maximizando el logaritmo de la verosimilitud Sin embargo, R tiene incorporado una función para obtener los estimadores máximos verosímiles utilizando algoritmos más eficientes basados en la estimación por mínimos cuadrados vista en secciones anteriores y que están implementados en la función lm(). summary(linear_fit &lt;- lm(count ~ temperature, data = day)) ## ## Call: ## lm(formula = count ~ temperature, data = day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4615.3 -1134.9 -104.4 1044.3 3737.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1214.6 161.2 7.537 1.43e-13 *** ## temperature 6640.7 305.2 21.759 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1509 on 729 degrees of freedom ## Multiple R-squared: 0.3937, Adjusted R-squared: 0.3929 ## F-statistic: 473.5 on 1 and 729 DF, p-value: &lt; 2.2e-16 ggplot(day, aes(temperature, count)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se=F) + ggtitle(&quot;riders ~ temperature, mediante regresión lineal&quot;) Interpretemos estos coeficientes del modelo: intercept: 1214.6 representa el número de pasajeros pronosticado cuando la temperatura es igual a 0. El intercept no es significativo porque la temperatura mínima en el conjunto de datos es . Podríamos hacerlo significativo al centrar la variable de temperatura en 0, en cuyo caso el intercept representaría el número de pasajeros promedio a la temperatura promedio. (Recuerde: las transformaciones lineales como el centrado no cambian el ajuste del modelo). La función summary() también genera un error estándar, valor t y valor p (“Pr (&gt; | t |)”) para el interceptar. Explicaremos estos valores a continuación cuando revisemos la inferencia en el contexto de la regresión. temperatura: 6640.7 representa el cambio previsto en el número de pasajeros asociado con un aumento de la temperatura de 1 unidad. Desafortunadamente, este coeficiente, como el intercept, no es muy interpretable porque el rango de la variable de temperatura es solo -, lo que significa que la temperatura realmente puede no aumenta en 1 unidad. Podríamos aplicar aquí otra transformación lineal, para desnormalizar la temperatura, pero, nuevamente, esa transformación no cambiaría el ajuste: la pendiente de la recta de regresión permanecería igual. Ahora podemos preguntar: ¿cuál de estos dos modelos de pasajeros, el modelo paramétrico que usa regresión lineal o el modelo no paramétrico que usa KNN, es mejor? ¿Qué entendemos por mejor? Una respuesta a esa pregunta está en términos del ajuste en la muestra. ¿Cuál es el RMSE del modelo KNN en comparación con el RMSE del modelo lineal? rmse(predict(linear_fit), day$count) ## [1] 1507.322 rmse(predict(knn_fit), day$count) ## [1] 1321.889 (predict() es equivalente a adjust() en este contexto.) Aquí podemos ver que el modelo KNN supera al modelo lineal en la muestra: en promedio, el modelo KNN está desfasado en aproximadamente 1322 ciclistas por día, mientras que el modelo lineal tiene una diferencia de 1507. Este tipo de comparación de modelos puede resultar muy útil. En este caso, sugiere que hay margen de mejora en el modelo lineal. Por un lado, podemos ver que la relación entre la temperatura y los pasajeros no es exactamente lineal: el número de pasajeros aumenta con la temperatura hasta aproximadamente .6, momento en el que se estabiliza y disminuye. La regresión KNN es mejor para modelar esta no linealidad. Sin embargo, podemos usar un modelo lineal para modelar un resultado no lineal agregando predictores. Es probable que el número de pasajeros varíe bastante según la temporada. Agreguemos una variable por temporada para ver si mejora el modelo. Necesitamos definir la temporada explícitamente como un factor, lo cual podemos hacer dentro de la función lm () usando factor (). Esto es apropiado porque la estación no es una variable continua, sino un número entero que representa las diferentes estaciones y que toma solo cuatro valores: 1 - 4. R malinterpretará la estación como una variable continua a menos que la definamos explícitamente como un factor. El orden numérico de los valores de temporada definirá automáticamente los niveles. La función lm () tratará el primer nivel, temporada = 1, como el nivel de referencia, con el que se compararán los otros niveles. ¿Cómo sabemos cuándo un predictor debe definirse como continuo y cuándo debe definirse como factor? Aquí hay una regla general: si restamos un nivel de otro y la diferencia tiene sentido, entonces podemos representar con seguridad esa variable como un número entero. Pensemos en la variable años de educación. La diferencia entre 10 años de escolaridad y 11 años es un año de educación, lo cual es una diferencia significativa. No estamos obligados a representar la educación como una variable continua, pero podríamos. (Codificar la educación como un factor esencialmente encajaría en una regresión separada para cada nivel, lo que podría correr el riesgo de sobreajuste.) Por el contrario, consideremos el código postal: una diferencia de 1 entre dos códigos postales de 5 dígitos no tiene sentido porque los códigos postales no tienen un orden intrínseco; representan diferencias categóricas, que nunca deben codificarse como números enteros. En cambio, estas variables siempre deben codificarse como factores. En este link podéis encontrar más información al respecto. summary(linear_fit2 &lt;- lm(count ~ temperature + factor(season), data = day)) ## ## Call: ## lm(formula = count ~ temperature + factor(season), data = day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4812.9 -996.8 -271.3 1240.9 3881.1 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 745.8 187.5 3.978 7.65e-05 *** ## temperature 6241.3 518.1 12.046 &lt; 2e-16 *** ## factor(season)2 848.7 197.1 4.306 1.89e-05 *** ## factor(season)3 490.2 259.0 1.893 0.0588 . ## factor(season)4 1342.9 164.6 8.159 1.49e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1433 on 726 degrees of freedom ## Multiple R-squared: 0.4558, Adjusted R-squared: 0.4528 ## F-statistic: 152 on 4 and 726 DF, p-value: &lt; 2.2e-16 rmse(predict(linear_fit), day$count) ## [1] 1507.322 rmse(predict(linear_fit2), day$count) ## [1] 1428.151 El ajuste ha mejorado; el modelo con temporada (season) tiene un RMSE más bajo. Pero, ¿cómo interpretamos los coeficientes de una variable factor?. Observamos que sólo hay 3 coeficientes para 4 temporadas. ¿No debería haber 4 coeficientes? ¿Ha cometido un error la función lm ()? No. Para una variable factor como la temporada, cada coeficiente representa el cambio en la respuesta asociado con un cambio en el predictor desde el primer nivel o nivel de referencia a cada nivel de factor subsiguiente. (Esta codificación, la predeterminada en lm (), se puede ajustar con el argumento contrasts). El nivel de referencia normalmente no se muestra en la salida del modelo. Si un predictor tiene \\(k\\) niveles, entonces habrá \\(k-1\\) coeficientes que representan los cambios previstos en el resultado asociados con aumentos desde el nivel de referencia en el predictor. factor(temporada)2: 848,7 es el cambio previsto en los ciclistas de primavera con respecto al invierno (temporada = 1). factor(temporada)3: 490,2 es el cambio previsto en los ciclistas de verano, nuevamente respecto al invierno, que es la categoría de referencia. Los aumentos de temperatura pueden tener diferentes impactos en el número de ciclistas en diferentes estaciones. Podríamos probar esta hipótesis al incluir una interacción entre la estación y la temperatura. La salida de la función summary () puede volverse difícil de manejar. En su lugar, usaremos la función display () del paquete arm, que ofrece un resumen del modelo más conciso. display(linear_fit3 &lt;- lm(count ~ temperature * factor(season), data = day)) ## lm(formula = count ~ temperature * factor(season), data = day) ## coef.est coef.se ## (Intercept) -111.04 321.28 ## temperature 9119.04 1020.33 ## factor(season)2 1513.43 571.76 ## factor(season)3 6232.96 1079.33 ## factor(season)4 2188.52 534.98 ## temperature:factor(season)2 -2524.78 1326.47 ## temperature:factor(season)3 -9795.26 1774.32 ## temperature:factor(season)4 -2851.25 1414.93 ## --- ## n = 731, k = 8 ## residual sd = 1406.35, R-Squared = 0.48 rmse(predict(linear_fit2), day$count) ## [1] 1428.151 rmse(predict(linear_fit3), day$count) ## [1] 1398.634 La interacción mejora el ajuste del modelo. temperatura: factor (temporada) 2: -2524.8 representa la diferencia en la pendiente de la temperatura comparando la temporada 2 con la temporada 1. El coeficiente negativo significa que un aumento de 1 unidad la temperatura se asocia con un cambio * menor * en el número de pasajeros en primavera en comparación con el invierno. temperatura: factor (temporada) 3: -9795.3 representa la diferencia en la pendiente de la temperatura comparando la temporada 3 con la temporada 1 Y así sucesivamente. En un modelo con interacciones, debemos tener cuidado de interpretar los efectos principales con precisión. temperatura: 9119 es el efecto principal de la temperatura y representa el cambio previsto en los ciclistas asociado con un aumento de 1 unidad en la temperatura cuando la temporada = 1 (la categoría de referencia) . En un modelo sin la interacción, el coeficiente de temperatura representaría el cambio promedio en los ciclistas asociado con un cambio de 1 unidad en la temperatura * manteniendo constante la temporada *. factor (temporada) 2: 1513.4 representa el cambio previsto en los ciclistas asociado con un aumento de 1 unidad en la temporada (es decir, de la temporada 1 a la temporada 2) cuando la temperatura = 0. Y así sucesivamente. Debido a que la temperatura no es igual a 0 en estos datos, los efectos principales de la temporada no son significativos. Para entender una interacción ¡es fundamental visualizarla! De hecho, en estadística deberíamos siempre empezar por esto antes de hacer inferencia ggplot(day, aes(temperature, count)) + geom_point() + stat_smooth(aes(group = factor(season), col = factor(season)), method=&quot;lm&quot;, se = F) + ggtitle(&quot;Riders ~ temperature, según estación (season)&quot;) Aquí podemos ver que la relación entre la temperatura y los ciclistas es más fuertemente positiva (más pronunciada) en la temporada 1, más plana en las temporadas 2 y 4, y negativa en la temporada 3. Claramente, la temperatura tiene diferentes efectos en diferentes estaciones. En enero y febrero, un día más cálido provoca un gran aumento de ciclistas: el clima es mejor para ir en bici. En julio, un día más cálido provoca una disminución de ciclistas: el clima es “peor” para ir en bici (peor = cuesta más). Los coeficientes del modelo lineal proporcionan una gran información sobre de los factores que influyen en el número de pasajeros. Sin embargo, nuestro modelo lineal todavía tiene un rendimiento inferior al modelo KNN. rmse(predict(knn_fit), day$count) ## [1] 1321.889 rmse(predict(linear_fit3), day$count) ## [1] 1398.634 La regresión lineal a menudo tendrá un sesgo más alto que un método flexible como la regresión KNN, pero también tenderá a tener una varianza más baja. Exploraremos estas propiedades más a fondo cuando lleguemos a la validación cruzada. Echemos un vistazo a la matriz del modelo. head(model.matrix(linear_fit3)) ## (Intercept) temperature factor(season)2 factor(season)3 factor(season)4 temperature:factor(season)2 ## 1 1 0.344167 0 0 0 0 ## 2 1 0.363478 0 0 0 0 ## 3 1 0.196364 0 0 0 0 ## 4 1 0.200000 0 0 0 0 ## 5 1 0.226957 0 0 0 0 ## 6 1 0.204348 0 0 0 0 ## temperature:factor(season)3 temperature:factor(season)4 ## 1 0 0 ## 2 0 0 ## 3 0 0 ## 4 0 0 ## 5 0 0 ## 6 0 0 Podemos ver que lm() ha convertido la variable de temporada en 3 vectores variables ficticias: factor (temporada) 2, factor (temporada) 3 y factor (temporada) 4. (Si un factor tiene niveles de $k $, entonces una variable ficticia para ese factor codifica $k - 1 $de esos niveles como variables binarias, con los valores 0 o 1 indicando la ausencia o presencia de ese nivel). Los términos de interacción consisten en los productos de los vectores componentes. 4.11 Predicción Podemos usar un modelo lineal no solo para la descripción sino también para la predicción. Si, por ejemplo, estuviéramos interesados en usar el modelo anterior para predecir el número de pasajeros para una temporada y temperatura en particular, digamos, un día caluroso en primavera, simplemente podríamos usar la ecuación de regresión. Definiremos un día caluroso como .85 (ojo con las escalas y las unidades de medida). Así: -111 + 9119*.85 + 1513*1 + 6233*0 + 2189*0 - 2525*.85*1 - 9795*.85*0 - 2851*.85*0 ## [1] 7006.9 Aquí hay una forma más precisa de hacer el cálculo que evita errores de redondeo al hacer referencia al objeto del modelo: t &lt;- .85 coefs &lt;- coef(linear_fit3) coefs[1] + coefs[2]*t + coefs[3]*1 + coefs[4]*0 + coefs[5]*0 + coefs[6]*t*1 + coefs[7]*t*0 + coefs[8]*t*0 ## (Intercept) ## 7007.501 Los resultados son diferentes debido al error de redondeo en el primer caso. El segundo método es más preciso. Podemos hacer el mismo cálculo tratando el vector de coeficientes como una matriz y usando la multiplicación de matrices. Esto requiere menos escritura pero, al igual que con el método anterior, requiere prestar mucha atención al orden de los términos. coefs %*% c(1, t, 1, 0, 0, t*1, 0, 0) ## [,1] ## [1,] 7007.501 Lo más simple de todo es definir un marco de datos con nuestros valores deseados y usar predecir (): predict(linear_fit3, newdata = data.frame(season = 2, temperature = .85)) ## 1 ## 7007.501 4.12 Inferencia en el contexto de regresión Además de las estimaciones de coeficientes para cada variable predictora (incluido el intercept), la salida de lm () (usando summary ()) contiene lo siguiente: “Error estándar”, “valor t” y “Pr (&gt; | t |)” (el valor p). Repasemos estos conceptos. Recuerde que la inferencia estadística nos permite estimar las características de la población a partir de las propiedades de una muestra. Por lo general, queremos saber si una diferencia o una relación que observamos en una muestra es verdadera en la población — es “estadísticamente significativa” — o es probable que se deba al azar. En el contexto de la regresión, queremos saber específicamente si la pendiente de la recta de regresión, \\(\\beta\\), que resume la relación de una variable con el resultado es diferente de 0. ¿Existe una relación positiva o negativa? En el paradigma frecuentista, respondemos a esta pregunta utilizando pruebas estadísticas basadas en test de hipótesis. De otros cursos sabemos que una prueba de hipótesis se basa en plantear una “hipótesis nula”, \\(H_0\\). En la regresión, \\(H_0\\) corresponde a que la pendiente de la recta de regresión, \\(\\beta\\), es 0. Una pendiente de 0 significa que un predictor no tiene efecto o no tiene relación con el resultado. R calcula automáticamente una prueba de hipótesis para \\(\\beta\\) usando el estadístico t, definido como: \\[ t = \\frac {\\beta - 0} {SE (\\beta)} \\] El estadístico \\(t\\) para una muestra sigue la distribución \\(t\\) de Student con n - 2 grados de libertad. Para la regresión lineal multivariante, el estadístico \\(t\\)sigue la distribución \\(t\\) de Student con $n - k - 1 $ grados de libertad, donde \\(k\\) representa el número de predictores en el modelo. Se utiliza la distribución \\(t\\) porque es más conservadora que una distribución normal cuando \\(n\\) es pequeño ya que en ese caso no podemos asumir el teorema central del límite que nos permitiría determinar que la distribución del estadístico sigue una distribución normal. La distribución \\(t\\) de Student tiene una cola más pesada pero converge a la normal cuando \\(n\\) aumenta (por encima de aproximadamente \\(n\\)= 30). Por otro lado, \\(SE (\\beta)\\) se define como \\[ SE (\\beta) = \\frac {RSE} {\\sqrt {\\sum_{i = 1} ^ n (x_i - \\bar {x} _i)}}, \\] donde error estándar residual (RSE) se calcula como \\[ RSE = \\sqrt {\\frac {RSS} {n - 2}}, \\] y la suma de cuadrados residual (RSS) se puede definir como (en una formulación ligeramente diferente a la que hemos usado antes): \\[ RSS = \\sum_ {i = 1} ^ n (y_i - f (x_i)) ^ 2. \\] Después de calcular el estadístico t, usamos una prueba t para compararlo con el valor crítico dado un nivel de significación que suele ser del 5% para la distribución \\(t\\) con n - 2 grados de libertad. summary(linear_fit) ## ## Call: ## lm(formula = count ~ temperature, data = day) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4615.3 -1134.9 -104.4 1044.3 3737.8 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1214.6 161.2 7.537 1.43e-13 *** ## temperature 6640.7 305.2 21.759 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1509 on 729 degrees of freedom ## Multiple R-squared: 0.3937, Adjusted R-squared: 0.3929 ## F-statistic: 473.5 on 1 and 729 DF, p-value: &lt; 2.2e-16 rse &lt;- sqrt(sum((day$count - predict(linear_fit))^2)/(nrow(day) - 2)) (seb &lt;- rse/sqrt(sum((day$temperature - mean(day$temperature))^2))) ## [1] 305.188 (t &lt;- as.numeric((coef(linear_fit)[2] - 0) / seb)) ## [1] 21.75941 Nuestro cálculo coincide exactamente con la salida del modelo lineal. Grafiquemos este estadístico t contra la distribución nula de una t de Student con 729 grados de libertad. Usamos la función dt () para generar una gráfica de densidad para una distribución t con 729 grados de libertad, y qt () para identificar los valores críticos para un IC del 95% en la distribución nula; los valores con baja probabilidad (p &lt;.05) estarán a la izquierda del IC inferior oa la derecha del IC superior. Los valores P y los IC proporcionan la misma información sobre lo inusual de un valor observado bajo el nulo. tdist &lt;- data.frame(x = seq(-5, 25, .01), y = dt(seq(-5, 25, .01), df = 729)) qt(c(.025, .975), df = 729) ## [1] -1.963223 1.963223 ggplot(tdist, aes(x, y)) + geom_line() + geom_vline(xintercept = t, col = &quot;red&quot;, lty = 2) + geom_vline(xintercept = qt(.025, df = 729), lty = 2) + geom_vline(xintercept = qt(.975, df = 729), lty = 2) + ggtitle(&quot;t(n - 2 = 729) según valores críticos (negro) y el estadístico t observado (rojo)&quot;) + xlab(&quot;t-statistic&quot;) + ylab(&quot;density&quot;) Un estadístico t de 21.76 esencialmente nunca ocurriría bajo la distribución nula, lo que nos permite “rechazar \\(H_0\\)” con un nivel de confianza del 95% (suponiendo un nivel de significación del 5%). El valor p asociado con el coeficiente \\(\\beta\\) para la temperatura en el resumen del modelo — esencialmente cero — refleja este resultado. Podemos calcular nuestro propio valor p con el siguiente código: 2 * pt(t, df = 729, lower.tail = FALSE) ## [1] 2.810622e-81 Usamos lower.tail = F porque estamos interesados en la probabilidad de t =r round (as.numeric (t), 2)en la cola superior además esta forma es más informativa que hacerlo con: 1 - pt(t, df = 729) ## [1] 0 que nos daría un p-valor de 0 por un problema de tolerancia de nuestra máquina (del orden de \\(10^{-21}\\)) El resumen del modelo también genera otro estadístico basado en la distribución F con un valor p asociado: \\[ F = \\frac{\\frac{TSS - RSS}{p - 1}}{\\frac{RSS}{n - p}} \\] La hipótesis nula para esta prueba F es: \\(H_0: \\beta_1 = ... \\beta_ {p-1} = 0\\). En otras palabras, la prueba responde a la pregunta: “¿Alguno de los predictores es útil para predecir la respuesta?” Esta no es una medida muy útil del rendimiento del modelo, ya que los modelos casi siempre tienen algún valor predictivo. NOTA (Avanzado por si lo necesitáis en el futuro): Podríamos estimar muy fácilmente \\(SE(\\beta)\\) usando bootstrap (aquí tenéis una descripción de este método). Utilizaremos este enfoque si tenemos motivos para desconfiar de cómo se calcula \\(SE(\\beta)\\) analíticamente usando lm (). Por ejemplo, en el caso de errores heterocedásticos (discutidos a continuación) lm () tenderá a subestimar \\(SE(\\beta)\\) y tendríamos muchos resultados significativos que serían falsos. Obtener resultados similares utilizando boostrap nos haría confiar en los resultados reportados por lm (). temperature_coef &lt;- NULL for(i in 1:1000){ rows &lt;- sample(nrow(day), replace = T) boot_sample &lt;- day[rows, ] model &lt;- lm(count ~ temperature, data = boot_sample) temperature_coef[i] &lt;- coef(model)[2] } sd(temperature_coef) ## [1] 275.6306 En este caso, la estimación de \\(SE (\\beta_{temp})\\) es similar pero menor que la calculada analíticamente. Por lo tanto, la estimación lm() es en realidad más conservadora en este caso. El \\(SE\\) para los coeficientes en la salida de lm () se puede usar para calcular los IC para la estimación del coeficiente. \\(\\hat {\\beta} + 1.96 (SE)\\) nos da el límite superior al 95%, y $ - 1.96 (SE) $ el límite inferior. Los IC del 95% que no incluyen 0 indican que el coeficiente es estadísticamente significativo, equivalente a un valor p del coeficiente menor de .05. El uso de IC en lugar de p-valores nos da una forma más flexible de hacer inferencia. Además, también suele ser útil porque nos indica qué posibles valores puede tomar nuestros parámetros (si quisiéramos cuantificar el efecto). Ésta es otra razón para usar la función display () del paquete arm. No solo presenta la salida de lm () de manera más compacta, sino que tampoco reporta estadísticas t ni valores p. Como hemos visto, \\(SE\\)s transmiten la misma información. display(linear_fit) ## lm(formula = count ~ temperature, data = day) ## coef.est coef.se ## (Intercept) 1214.64 161.16 ## temperature 6640.71 305.19 ## --- ## n = 731, k = 2 ## residual sd = 1509.39, R-Squared = 0.39 Los valores posibles (recordamos que el verdadero valor del parámetro es desconocido en la población) para la temperatura son \\(6641 \\pm 2(305)\\) o aproximadamente [6031,r 6641 + 2 * 305].3 Este IC no incluye 0, de lo cual podemos concluir que la temperatura se asocia con el resultado de forma estadísticamente significativa. (El valor p para la temperatura informado en el summary() concuerda). 4.13 Asunciones de un modelo de regresión Los resultados de la regresión solo son precisos si se dan un conjunto de supuestos (en orden de importancia):4 Validez de los datos para responder a la pregunta de investigación. Linealidad de la relación entre el resultado y las variables predictoras. Independencia de los errores (en particular, sin correlación entre errores consecutivos como en el caso de los datos de series de tiempo). Varianza igual de errores (homocedasticidad). Normalidad de errores. La mayoría de estos problemas no son fatales y se pueden solucionar mejorando el modelo, seleccionando variables diferentes o adicionales o utilizando una distribución de modelización diferente (los conocidos como modelos lineales generalizados o GLMs). Los gráficos de residuos son la mejor herramienta para evaluar si se han cumplido los supuestos del modelo. 1. Validez de los datos para responder a la pregunta de investigación Esto puede parecer obvio pero es necesario enfatizarlo: La medida de resultado debe reflejar con precisión el fenómeno de interés. El modelo debe incluir todas las variables relevantes. El modelo debe generalizarse a todos los casos a los que se aplica. En resumen, debemos asegurarnos de que nuestros datos proporcionan información precisa y relevante para responder a la pregunta de investigación. 2. Supuesto de linealidad La suposición matemática más importante del modelo de regresión es que el resultado es una función lineal determinista de los predictores separados: \\(y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2}...\\). Podemos comprobar este supuesto visualmente trazando las variables predictoras contra el resultado: ggplot(day, aes(temperature, count)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = F) + ggtitle(&quot;count ~ temperature&quot;) Los datos son claramente no lineales, ¿qué hacemos? Podemos agregar predictores al modelo, como la temporada, que permiten que un modelo lineal se ajuste mejor a datos no lineales. También podríamos considerar añadir un término cuadrático al modelo: conteo ~ temperatura + temperatura\\(^2\\). linear_fit4 &lt;- lm(count ~ temperature + I(temperature^2), data = day) ggplot(day, aes(temperature, count)) + geom_point() + geom_line(aes(temperature, fitted(linear_fit4)), col= &quot;red&quot;) + ggtitle(&quot;count ~ temperature + temperature^2&quot;) Una vez que se han añadido predictores adicionales, podemos verificar los gráficos de residuos, ya que si el modelo no cumple la condición de linealidad se mostrará en los residuos. plot () es una función R incorporada para verificar la distribución de errores de un modelo. plot(linear_fit, which = 1) Podemos ver, como era de esperar, una no linealidad significativa en la gráfica residual para el modelo con temperatura solamente. Esperamos que los residuos no tengan una estructura visible, ningún patrón. En términos del supuesto de linealidad, la línea de resumen roja no debe tener curvatura. Cuando agregamos temporada, la parcela residual, aunque no es perfecta, mejora mucho. plot(linear_fit2, which = 1) Sin embargo, el modelo todavía lucha con días de gran volumen, con más de 5000 ciclistas previstos. Veamos si agregar una interacción entre la estación y la temperatura ayuda: plot(linear_fit3, which = 1) Quizás esto sea mejor. La no linealidad implica menos observaciones, la mayoría de ellas en los días con valores ajustados superiores a 6000. Pero ha surgido otro problema con este modelo: los errores heterocedásticos. Discutiremos este supuesto antes de la independencia de errores. 4. Igual varianza de errores (homocedasticidad) Observamos cómo los errores en el gráfico residual anterior tienen forma de embudo. Los errores del modelo no se distribuyen por igual en el rango de los valores ajustados, una situación conocida como heterocedasticidad. Una solución es transformar la variable de resultado tomando el registro (solo funciona con valores positivos). Si esto no funciona, recuerde que la principal consecuencia de los errores heterocedásticos es que los \\(SE (\\beta)\\)s son más pequeños de lo que deberían ser, lo que lleva a valores p más significativos de los que debería haber. Un remedio para este problema inferencial es calcular los errores estándar ajustados que son robustos a la varianza desigual; el paquete MASS ofrece la función rlm () para ajustar tal modelo. Aquí está la gráfica residual para un modelo de log (recuento): plot(lm(log(count) ~ temperature * season, data = day), which = 1) Es posible que la heterocedasticidad haya mejorado, pero ahora han surgido algunos valores atípicos y todavía tenemos un problema de no linealidad. ¿Qué hacemos? Después de revisar estos gráficos de residuos, deberíamos dar un paso atrás y pensar en nuestros datos. Un problema queda claro. Es probable que el número de pasajeros en días consecutivos sea muy similar debido a la temperatura, el clima y la temporada. En consecuencia, los residuos del modelo se agruparán (veríamos clusters). Si el modelo no hace un buen trabajo al contabilizar el número de pasajeros en, digamos, días de alta temperatura, los errores grandes no se distribuirán al azar sino que ocurrirán juntos, producidos por una ola de calor en julio, por ejemplo. Los errores de los días siguientes serán similares. En estos casos, la regresión lineal no sería un buen modelo ya que el supuesto de independencia de errores no se cumpliría. Estos problemas ocurren en la mayoría de casos que nuestros datos se recogen de forma seriada (series temporales). Es por ello que en estos casos se debe de utilizar otros modelos más complejos como la regresión KNN (entre otros). Sin embargo, debemos tener en cuenta que la regresión KNN puede ajustarse mejor a los datos y posiblemente ofrecer mejores predicciones, pero no ofrece ninguna ayuda para comprender las relaciones entre las variables y muchas veces (sobre todo en medicina) esto es muy importante. La regresión lineal, incluso si el modelo no es perfecto, proporciona información sobre los factores que afectan a la cantidad de usuarios, información que puede ser extremadamente valiosa, por ejemplo, para los administradores del programa de bicicletas compartidas, mientras que la regresión KNN solo puede ofrecer una predicción. 3. No independencia de errores (residuales correlacionados) La falta de independencia de los errores ocurre en los datos de series de tiempo o en los datos con observaciones agrupadas, cuando, por ejemplo, varios puntos de datos provienen de individuos de un mismo barrio, país, , tiendas o aulas. Podemos diagnosticar los residuos correlacionados en los datos de los usuarios de bicicletas mirando un gráfico de residuos por fecha. data.frame(day = seq(1,nrow(day)), residuals = residuals(linear_fit3)) %&gt;% ggplot(aes(day, residuals)) + geom_point() + ggtitle(&quot;Residuales según el día: count ~ temperature * season&quot;) + geom_hline(yintercept = 0, lty = 2, col = &quot;red&quot;) Podemos ver muy claramente que los errores ocurren en grupos relacionados con la fecha. Quizás el patrón más importante venga dado por la variable año. Sin una variable que determine el año, el modelo tiene problemas ya que predice de forma insuficiente en el primer año y prediciendo de más en el segundo. Si agregamos un año al modelo, los residuos se ven mejor pero la agrupación sigue siendo evidente. data.frame(day = seq(1,nrow(day)), residuals = residuals(update(linear_fit3, ~ . + year))) %&gt;% ggplot(aes(day, residuals)) + geom_point() + ggtitle(&quot;Residuales según el día: count ~ temperature * season + year&quot;) + geom_hline(yintercept = 0, lty = 2, col = &quot;red&quot;) ¿Cómo abordamos los errores no independientes? Si la no independencia está relacionada con el tiempo, entonces deberíamos usar un modelo apropiado para datos de series de tiempo, como ARIMA (que se podrá ver en otras asignaturas). Si la agrupación se debe a alguna otra estructura en los datos, por ejemplo, la agrupación debido a la ubicación, entonces podríamos considerar el uso de un modelo jerárquico o multinivel (también se podrá ver en otras asignaturas). Para manejar errores no independientes con un modelo lineal, necesitamos agregar variables que controlen el agrupamiento. La agrupación en este caso se debe a la estacionalidad, por lo que agregamos predictores como año, temporada, mes o día de la semana. Si el modelo resultante aún no se ajusta bien a los datos y solo nos interesa la predicción, entonces podríamos considerar el uso de un modelo no paramétrico como KNN. 5. Normalidad de los residuales Comparado con los otros supuestos, este no es muy importante. La regresión lineal es extremadamente robusta a las violaciones de la normalidad. Podemos comprobar visualmente la normalidad de los residuales con un histograma: data.frame(residuals = residuals(linear_fit3)) %&gt;% ggplot(aes(residuals)) + geom_histogram() + ggtitle(&quot;Residuales: count ~ temperature * season&quot;) Queda bastante claro que el modelo sin año no es normal. La bimodalidad de esta distribución ofrece una pista de que el año es un término estacional clave. La librería car incluye una función, qqPlot () que “muestra cuantiles empíricos de una variable, o de residuales studentizados de un modelo lineal, contra cuantiles teóricos de una distribución teórica con la que podemos comparar”. library(car) qqPlot(linear_fit3, pch = 20) ## [1] 239 668 Aquí podemos ver desviaciones de la normalidad que también tienen una estructura anual discernible. En resumen, utilizamos gráficos de residuos para validar y mejorar el ajuste del modelo. Si bien hay funciones disponibles para probar formalmente la mayoría de los supuestos del modelo anterior, es mejor (en mi opinión) evitar tales pruebas binarias a favor de graficar los residuos y pensar en los datos y cómo mejorar un modelo. 4.14 Ejemplos adicionales de interpretación de modelos Para estos ejemplos, usaremos el conjunto de datos de vivienda de Boston, que registra los precios de la vivienda en el área de Boston en la década de 1970 junto con varios predictores. La variable de resultado es el valor mediano de las viviendas ocupadas por sus propietarios en $1000, codificado como “medv”. Los predictores incluyen lo siguiente: chas: variable ficticia de Charles River (= 1 si el tramo limita con el río; 0 en caso contrario). lstat: menor estatus de la población (porcentaje). El diccionario de datos no es explícito, pero esta variable parece ser una medida del estatus socioeconómico de un barrio, representado como el porcentaje de clase trabajadora o familias pobres. Centraremos esta variable para que los efectos principales sean interpretables. rm: número medio de habitaciones por vivienda en un área geográfica determinada. Centraremos esta variable para que los efectos principales sean interpretables. 4.14.1 Interpretación del intercept y los coeficientes \\(\\beta\\) para un modelo con predictores continuos library(MASS) data(Boston) Boston$rm_centered &lt;- Boston$rm - mean(Boston$rm) Boston$lstat_centered &lt;- Boston$lstat - mean(Boston$lstat) display(lm(medv ~ rm_centered + lstat_centered, data = Boston)) ## lm(formula = medv ~ rm_centered + lstat_centered, data = Boston) ## coef.est coef.se ## (Intercept) 22.53 0.25 ## rm_centered 5.09 0.44 ## lstat_centered -0.64 0.04 ## --- ## n = 506, k = 3 ## residual sd = 5.54, R-Squared = 0.64 La intersección con el eje Y (intercept) es el valor predicho de la variable de resultado cuando los predictores son 0. A veces, la intersección no será interpretable porque un predictor no puede = 0. La solución es centrar la variable para que 0 tenga sentido. intercept: El valor predicho de medv cuando todos los predictores son 0: 22.53 + 5.09 (0) - .64 (0). rm_centered: 5.09 representa el cambio predicho en medv cuando rm_centered aumenta en una unidad (1 habitación), mientras se mantienen constantes las otras variables. lstat_centered: -.64 representa el cambio predicho en medv cuando lstat_centered aumenta en una unidad, mientras se mantienen constantes las otras variables. 4.14.2 Interpretación del intercept y los coeficientes \\(\\beta\\) para un modelo con predictores binarios y continuos display(lm(medv ~ rm_centered + lstat_centered + chas, data = Boston)) ## lm(formula = medv ~ rm_centered + lstat_centered + chas, data = Boston) ## coef.est coef.se ## (Intercept) 22.25 0.25 ## rm_centered 4.96 0.44 ## lstat_centered -0.64 0.04 ## chas 4.12 0.96 ## --- ## n = 506, k = 4 ## residual sd = 5.45, R-Squared = 0.65 La intersección (intercept) es el valor predicho de la variable de resultado cuando el predictor binario es 0 y la variable continua es 0 (que, para las variables centradas, es el promedio). intercept: El valor predicho de medv cuando todos los predictores son 0: 22.25 + 4.96 (0) -.64 (0) + 4.12 (0). rm_centered: 4.96 representa el cambio predicho en medv cuando rm_centered aumenta en una unidad (1 habitación), mientras se mantienen constantes las otras variables. lstat_centered: -.64 representa el cambio predicho en medv cuando lstat_centered aumenta en una unidad, mientras se mantienen constantes las otras variables. chas: 4.12 representa el cambio predicho en medv cuando chas aumenta en una unidad, mientras se mantienen constantes las otras variables. 4.14.3 Interpretación del intercept y los coeficientes \\(\\beta\\) para un modelo con predictores binarios y continuos, con interacciones display(lm(medv ~ rm_centered* chas + lstat_centered, data = Boston)) ## lm(formula = medv ~ rm_centered * chas + lstat_centered, data = Boston) ## coef.est coef.se ## (Intercept) 22.25 0.25 ## rm_centered 4.98 0.46 ## chas 4.17 0.99 ## lstat_centered -0.64 0.04 ## rm_centered:chas -0.22 1.13 ## --- ## n = 506, k = 5 ## residual sd = 5.45, R-Squared = 0.65 Recordemos que la visualización de datos en estadística es muy necesaria! ggplot(Boston, aes(rm_centered, medv, col= factor(chas))) + geom_point() + stat_smooth(method=&quot;lm&quot;, se=F)+ ggtitle(&quot;medv ~ rm_centered según chas&quot;) Estas rectas de regresión son prácticamente paralelas, lo que indica que no hay interacción. Este resultado se confirma por el hecho de que el valor p para rm_centered: chas es .85 (p&gt; .05). intercept: 4.98 es el valor predicho de medv cuando todos los predictores son 0. rm_centered: 4.98 representa el cambio predicho en medv cuando rm_centered aumenta en una unidad (1 habitación), entre aquellas casas donde chas = 0. chas: 4.17 representa el cambio predicho en medv cuando chas aumenta en una unidad, entre hogares con habitaciones promedio (rm_centered = 0). lstat_centered: -.64 representa el cambio predicho en medv cuando lstat_centered aumenta en una unidad, mientras se mantienen constantes las otras variables. rm_centered: chas: -.21 se agrega a la pendiente de rm_centered, 4.98, para chas aumenta de 0 a 1. O, alternativamente, se agrega -.21 a la pendiente de chas, 4.17, por cada unidad adicional de rm__centrado. 4.14.4 Interpretación del intercept y los coeficientes \\(\\beta\\) para un modelo con predictores continuos, con interacciones display(lm(medv ~ rm_centered * lstat_centered, data = Boston)) ## lm(formula = medv ~ rm_centered * lstat_centered, data = Boston) ## coef.est coef.se ## (Intercept) 21.04 0.23 ## rm_centered 3.57 0.39 ## lstat_centered -0.85 0.04 ## rm_centered:lstat_centered -0.48 0.03 ## --- ## n = 506, k = 4 ## residual sd = 4.70, R-Squared = 0.74 Primero, visualizaremos la interacción dicotomizando lstat. Boston$lstat_bin &lt;- ifelse(Boston$lstat &gt; mean(Boston$lstat), &quot;above avg&quot;,&quot;below avg&quot;) ggplot(Boston, aes(rm_centered, medv, col= lstat_bin) ) + geom_point() + stat_smooth(method=&quot;lm&quot;) + ggtitle(&quot;medv ~ rm según lstat&quot;) El número de habitaciones en una casa claramente afecta el valor — ambas rectas de regresión son positivas. Pero esta relación positiva es más pronunciada entre los hogares con menor lstat. Aumentar la cantidad de habitaciones tiene un impacto mayor en los vecindarios más pobres que en los vecindarios más ricos. A continuación, dicotomizamos rm. Boston$rm_bin &lt;- ifelse(Boston$rm &gt; mean(Boston$rm), &quot;above avg&quot;,&quot;below avg&quot;) ggplot(Boston, aes(lstat_centered, medv, col = rm_bin) ) + geom_point() + stat_smooth(method=&quot;lm&quot;) + ggtitle(&quot;medv ~ lstat según rm&quot;) El nivel socioeconómico promedio en un vecindario afecta claramente el valor de la vivienda; ambas rectas de regresión son negativas. Pero esta relación negativa es más pronunciada entre los hogares con habitaciones por encima del promedio. El nivel socioeconómico bajo (lstat aumentado) tiene un mayor impacto en el valor de las viviendas con habitaciones por encima del promedio que en las casas con habitaciones por debajo del promedio. intercept: 21.04 es el valor predicho de medv cuando tanto rm como lstat son promedios. rm_centered: 3.57 es el cambio predicho en medv si rm_centered aumenta en 1 unidad, entre aquellos hogares donde lstat_centered es promedio (= 0). lstat_centered: .85 es el cambio predicho en medv si lstat_centered aumenta en 1 unidad, entre aquellos hogares donde rm_centered es promedio (= 0) rm_centered: lstat_centered: .48 se agrega a la pendiente de rm_centered, 3.57, por cada unidad adicional de lstat_centered. O, alternativamente, se agrega -.48 a la pendiente de lstat_centered, -.85, para cada unidad adicional de rm__centered. Podemos entender la interacción diciendo que la importancia de lstat como predictor de medv disminuye a mayor número de habitaciones y, de manera similar, que la importancia de rm como predictor de medv disminuye a niveles más altos de lstat. 4.15 Centrado y escalado Hemos visto cómo los predictores centrados pueden ayudar a la interpretación del modelo. Además de centrar, podemos escalar predictores, lo que hace que los coeficientes del modelo resultante sean directamente comparables (nos puede servir para discernir qué variable influye más en el resultado). La función rescale () en el paquete arm automáticamente centra una variable y divide por 2 desviaciones estándar (\\(\\frac {x_i - \\bar {x}}{2sd}\\)). La configuración predeterminada ignora las variables binarias. La división por 2 desviaciones estándar, en lugar de 1 (como cuando se calcula una puntuación z tradicional), hace que las variables continuas reescaladas sean comparables a las variables binarias no transformadas. Después de centrar y escalar, los coeficientes del modelo se pueden usar para evaluar los tamaños del efecto e identificar los predictores más fuertes. ¿Cuál es un predictor más sólido del número de ciclistas, la velocidad del viento o el año? display(lm(count ~ windspeed + year, data = day)) ## lm(formula = count ~ windspeed + year, data = day) ## coef.est coef.se ## (Intercept) 4496.05 161.80 ## windspeed -5696.31 733.60 ## year 2183.75 113.63 ## --- ## n = 731, k = 3 ## residual sd = 1535.95, R-Squared = 0.37 Este modelo hace que parezca que windpseed es, con mucho, el predictor más fuerte: el valor absoluto del coeficiente es más de 2 veces mayor. Pero este resultado es engañoso, un artefacto de escala variable. display(lm(count ~ arm::rescale(windspeed) + year, data = day)) ## lm(formula = count ~ arm::rescale(windspeed) + year, data = day) ## coef.est coef.se ## (Intercept) 3410.98 80.40 ## arm::rescale(windspeed) -882.90 113.70 ## year 2183.75 113.63 ## --- ## n = 731, k = 3 ## residual sd = 1535.95, R-Squared = 0.37 El ajuste del modelo no ha cambiado — \\(R^2\\) es el mismo en ambos modelos — pero el centrado y la escala nos permiten ver que el año en realidad tiene un tamaño de efecto mucho mayor que la velocidad del viento: se asocia un aumento de 1 unidad en el año con un mayor cambio en el número de pasajeros. Podríamos, de manera equivalente, usar la función estandardize (), también del paquete arm, que convenientemente cambia la escala de todas las variables en un modelo a la vez. display(standardize(lm(count ~ windspeed + year, data= day))) ## lm(formula = count ~ z.windspeed + c.year, data = day) ## coef.est coef.se ## (Intercept) 4504.35 56.81 ## z.windspeed -882.90 113.70 ## c.year 2183.75 113.63 ## --- ## n = 731, k = 3 ## residual sd = 1535.95, R-Squared = 0.37 La función standardize() nos advierte que la velocidad del viento es ahora un z-score (z.windspeed), y que ese año se ha centrado (c.year). La interpretación de c.year es la misma que para año: un aumento de 1 unidad (-.5 a .5) se asocia con un aumento previsto de 2183,75 pasajeros. Sin embargo, la interpretación de z.windspeed ahora es diferente, ya que un aumento de 1 unidad en z.windspeed es 2 desviaciones estándar. Por lo tanto, un aumento de 2 desviaciones estándar en la velocidad del viento (0.15) se asocia con un cambio previsto en el número de pasajeros de -882. ¿Por qué nos importa poder comparar coeficientes? El valor absoluto de \\(\\beta\\) es una medida de la fuerza de la relación entre un predictor y el resultado y, por lo tanto, de la importancia de ese predictor para explicar el resultado. Los valores p no ofrecen orientación sobre la fuerza de un predictor: un predictor estadísticamente significativo podría tener un tamaño de efecto minúsculo y prácticamente intrascendente. El valor absoluto de \\(\\beta\\) es, por tanto, una medida de importancia práctica, en oposición a la significación estadística. La significancia estadística expresa la improbabilidad de un resultado, mientras que \\(\\beta\\) representa el tamaño del efecto, cuánto esperamos que cambie el resultado como resultado de variar el predictor. Estandarizar \\(\\beta\\) nos permite interpretar el tamaño del efecto sin dejarnos engañar por diferencias arbitrarias en la escala variable. 4.16 Transformación de variables Hasta ahora hemos considerado transformaciones que no cambian el ajuste del modelo, sino que simplemente ayudan a la interpretación. A veces, sin embargo, queremos cambiar las variables para que un modelo lineal se ajuste mejor. La transformación logarítmica ampliamente utilizada es un ejemplo. Deberíamos considerar una transformación logarítmica si nuestros datos están sesgados, muestran un aumento no lineal o tienen un rango grande. Regla empírica: si una variable tiene un margen superior a dos órdenes de magnitud (x 100), la transformación logarítmica probablemente mejorará el modelo. Para la transformación logarítmica usaremos el logaritmo natural, designado \\(\\log_e\\), o \\(\\ln\\), o, en código R: log(). El logaritmo natural es la función inversa de la función exponencial (y viceversa): se deshacen entre sí. Por lo tanto, estas identidades: \\(e ^ {\\ln (x)} = x\\) (si \\(x&gt; 0\\)); \\(\\ln (e ^ x) = x\\). Para volver a poner una variable transformada logarítmica en la escala original, simplemente exponenciamos: \\(x = e ^ {\\ln (x)}\\). En código R: x = exp (log (x)). Una razón para usar registros naturales es que los coeficientes en la escala logarítmica natural son, aproximadamente, interpretables como diferencias proporcionales. Ejemplo: para una variable transformada logarítmicamente, un coeficiente de .06 significa que una diferencia de 1 unidad en \\(x\\) corresponde a una diferencia aproximada del 6% en \\(y\\), y así sucesivamente. ¿Por qué? exp (.06) = 1.06, un aumento del 6% desde 1 como referencia. Sin embargo, con coeficientes más grandes, esto no funciona exactamente: exp (.42) = 1.52, una diferencia del 52%. La población de EE. UU. De 1790 a 1970 es un ejemplo de una variable que nos gustaría transformar, ya que el aumento no es lineal y es grande: data(&quot;uspop&quot;) plot(uspop, main = &quot;US population, 1790 - 1970, en millones&quot;) Pero cuando tomamos el logaritmo de la población, el aumento es (más) lineal: plot(log(uspop), main = &quot;US population (scala logartímica), 1790 - 1970, en millones&quot;) La transformación logarítmica se usa con frecuencia con precios o ingresos, ya que el extremo superior de la escala para tales variables suele ser exponencialmente mayor que el inferior. Podemos registrar los valores de las viviendas en el conjunto de datos de viviendas de Boston para mejorar el ajuste del modelo. Boston %&gt;% arrange(medv) %&gt;% mutate(observations = seq(1, length(medv)), log_medv = log(medv)) %&gt;% dplyr::select(medv, log_medv, observations) %&gt;% gather(type, home_value, -observations) %&gt;% ggplot(aes(observations, home_value)) + geom_line() + facet_wrap(~type, scales = &quot;free_y&quot;)+ ggtitle(&quot;Comparación de log(medv) y medv&quot;) Tomar el logaritmo de medv no ha desplazado medv hacia la linealidad tanto como podríamos haber esperado. No obstante, ¿mejorará el ajuste? display(standardize(lm(medv ~ rm + lstat, data = Boston))) ## lm(formula = medv ~ z.rm + z.lstat, data = Boston) ## coef.est coef.se ## (Intercept) 22.53 0.25 ## z.rm 7.16 0.62 ## z.lstat -9.17 0.62 ## --- ## n = 506, k = 3 ## residual sd = 5.54, R-Squared = 0.64 display(standardize(lm(log(medv) ~ rm + lstat, data = Boston))) ## lm(formula = log(medv) ~ z.rm + z.lstat, data = Boston) ## coef.est coef.se ## (Intercept) 3.03 0.01 ## z.rm 0.18 0.03 ## z.lstat -0.55 0.03 ## --- ## n = 506, k = 3 ## residual sd = 0.23, R-Squared = 0.68 Un poco. \\(R^2\\) ha mejorado de .64 a .68. Se debe tener cuidado al interpretar un modelo transformado logarítmicamente. intercept: 3.03 representa el valor log (medv) predicho cuando tanto rm como lstat son promedios (ya que ambas variables se han centrado y escalado). Para volver a poner esto en la escala original, exponencial: $e ^ {3.03} $= 20.7. Por lo tanto, el medv predicho del modelo para hogares con lstat y rm promedio en dólares es $2.07^{4}. z.rm: .18 o 18% representa el cambio porcentual previsto en medv asociado con un aumento de dos desviaciones estándar en rm (1.41). z.lstat: .55 o 55% representa el cambio porcentual previsto en medv asociado con un aumento de dos desviaciones estándar en lstat (14.28). residual se: .23 es la desviación estándar de los residuos registrados. Para volver a poner esto en la escala original, exponencial: $e ^ .23 $= $1.2586. ¿Y si tanto el resultado como el predictor se transforman logarítmicamente? display(lm(log(medv) ~ log(lstat), data = Boston)) ## lm(formula = log(medv) ~ log(lstat), data = Boston) ## coef.est coef.se ## (Intercept) 4.36 0.04 ## log(lstat) -0.56 0.02 ## --- ## n = 506, k = 2 ## residual sd = 0.23, R-Squared = 0.68 La transformación del registro de un predictor, así como el resultado, es una táctica perfectamente razonable si cree que la no normalidad en ambos podría estar contribuyendo a gráficos residuales problemáticos. En este caso, \\(R^2\\) no ha cambiado, lo que sugiere que la transformación logarítmica de lstat no es necesaria. intercept: log (medv) es 4.36 cuando lstat = 1. (Tenga en cuenta que no podemos centrar lstat en este caso porque no podemos tomar el logaritmo de un número negativo). log (lstat): -.56 o -56% representa el cambio porcentual previsto en medv asociado con un aumento del 1% en lstat. En este artículo Models with transformed variables: interpretation and software se muesrta cómo reportar e interpretar efectos en escalas originales de las variables en el caso de los modelos de regresión lineal, logística y Poisson con transformaciones logarítmicas y de potencia. 4.17 Colinealidad La colinealidad ocurre cuando dos variables predictoras están fuertemente correlacionadas entre sí. Si bien no es un supuesto de regresión per se, la colinealidad puede afectar la precisión de los coeficientes, así como inflar los errores estándar. La colinealidad es menos preocupante cuando solo nos interesa la predicción. Un buen ejemplo es la temperatura y la temperatura percibida, atemp, en los datos de la bicicleta. Estas medidas de temperatura están muy cerca: ggplot(day, aes(temperature)) + geom_density()+ geom_density(aes(atemp), col= &quot;red&quot;) + ggtitle(&quot;Densidad de la temperatura (black) vs. la temperatura percibida (red)&quot;) Ahora compare el modelo de uso de la bicicleta con solo la temperatura como predictor con el modelo con temperatura y temperatura. Observe lo que sucede con los errores estándar y los coeficientes: display(standardize(lm(count ~ temperature, data = day))) ## lm(formula = count ~ z.temperature, data = day) ## coef.est coef.se ## (Intercept) 4504.35 55.83 ## z.temperature 2431.18 111.73 ## --- ## n = 731, k = 2 ## residual sd = 1509.39, R-Squared = 0.39 display(model &lt;- standardize(lm(count ~ temperature + atemp, data = day))) ## lm(formula = count ~ z.temperature + z.atemp, data = day) ## coef.est coef.se ## (Intercept) 4504.35 55.65 ## z.temperature 390.34 866.32 ## z.atemp 2057.91 866.32 ## --- ## n = 731, k = 3 ## residual sd = 1504.60, R-Squared = 0.40 Los \\(SE\\)s se hacen enormes y los coeficientes se vuelven poco fiables. La solución práctica en el caso anterior es usar solo una de estas variables sabiendo que contienen la misma información. Pero las variables a menudo están correlacionadas. ¿Cuánta correlación está bien? El factor de inflación de la varianza (VIF) puede ayudarnos a decidir. La varianza muestral estimada del coeficiente de regresión \\(j\\)-ésimo se puede escribir como: \\[{\\rm \\widehat {var}} (\\hat {\\beta} _j) = \\frac {\\hat {\\sigma} ^ 2} {(n-1) s_j ^ 2} \\cdot \\frac {1} {1-R_j ^ 2}\\] donde \\(\\hat {\\sigma} ^ 2\\) es la varianza del error estimada, \\(s_j ^ 2\\) es la varianza muestral de $x_j $y \\(\\frac {1} {1-R_j ^ 2}\\) es el factor de inflación de la varianza o \\(VIF_j\\). El término \\(R_j ^ 2\\) es el \\(R ^ 2\\) de un modelo de regresión lineal en el que el predictor \\(X_j\\) se utiliza como variable de respuesta y todas las demás covariables como variables explicativas. Un \\(R ^ 2\\) alto en dicho modelo significa que la mayor parte de la variación en el predictor \\(X_j\\) se explica por todas las demás covariables, lo que significa que hay colinealidad. Esto infla los errores estándar, ensanchando los IC y disminuyendo la probabilidad de detectar un efecto. Para evaluar la colinealidad entre los predictores, use vif () del paquete car: cor(day$temperature, day$atemp) ## [1] 0.9917016 Estas variables están casi perfectamente correlacionadas. car::vif(model) ## z.temperature z.atemp ## 60.50328 60.50328 Este resultado significa que los errores estándar son aproximadamente \\(\\sqrt {60.5}\\)= 7.8 más grandes de lo que serían sin la otra variable. El punto críto es ver que valor se considera grande … Hay autores que consideran un umbral de VIF de 4 (John Fox) o 5-10 (Hastie &amp; Tibshirani) para eliminar una variable. Lo que hay que hacer es decir cuál es nuestro criterio y dejar que el lector considere si es mucho o poco. 4.18 Valores atípicos Los valores atípicos pueden afectar el ajuste de un modelo de regresión. Es mejor no eliminarlos (al menos no al principio) sino comprenderlos. (Por supuesto, algunos valores extremos podrían ser errores de codificación, en cuyo caso querrá eliminarlos). Recuerde que los valores atípicos son principalmente una preocupación después de que el modelo se ha ajustado, en cuyo caso aparecen entre los residuos. Los predictores incluidos en el modelo pueden ocuparse de las observaciones que aparecen como valores atípicos en el análisis univariado o bivariado. Como ejemplo, consideremos un conjunto de datos incluido en la librería ISLR, Hitters, que contiene información de las estadísticas de rendimiento y los salarios de los jugadores de béisbol de las grandes ligas en la temporada de 1986. Creemos un modelo para predecir el salario usando el número de turnos al bate, hits, años en la liga, home-runs, carreras impulsadas, bases por bolas y asistencias. library(ISLR) data(Hitters) display(standardize(m &lt;- lm(log(Salary) ~ AtBat + Hits + Years + HmRun + RBI + Walks + Assists, data = Hitters))) ## lm(formula = log(Salary) ~ z.AtBat + z.Hits + z.Years + z.HmRun + ## z.RBI + z.Walks + z.Assists, data = Hitters) ## coef.est coef.se ## (Intercept) 5.88 0.04 ## z.AtBat -0.81 0.34 ## z.Hits 1.29 0.32 ## z.Years 0.91 0.08 ## z.HmRun 0.11 0.17 ## z.RBI 0.02 0.23 ## z.Walks 0.34 0.10 ## z.Assists 0.03 0.09 ## --- ## n = 263, k = 8 ## residual sd = 0.63, R-Squared = 0.51 plot(m, which=1) Los salarios de tres jugadores — Mike Schmidt, Terry Kennedy y Steve Sax — claramente no están muy bien explicados por este modelo. El modelo predice que a los dos primeros se les debería pagar menos y que al tercero se les debería pagar más. Podemos obtener otra perspectiva de estas observaciones utilizando una métrica llamada “distancia de Cook”, que es una medida de influencia de uso común. plot(m, which=4) Un punto influyente es aquel que, si se elimina de los datos, cambiaría significativamente el ajuste. El punto podría ser un valor atípico o tener un alto apalancamiento. La distancia de Cook&gt; 1 se usa a menudo como un umbral aproximado para identificar puntos influyentes. La diferencia de Cook para estos tres jugadores es menor que 1, pero al mismo tiempo no están bien explicados por el modelo. ¿Qué debemos hacer? Primero, pensar en los datos. Hablar con expertos, generalmente con los investigadores que han diseñado el estudio y han recogido la información para que nos ayuden a comprender si dichos valores tienen sentido (quizás podrían ser errores de medida o de entrada de datos). En el caso del conjunto de datos de los bateadores, tenemos datos de solo un año, 1986 — puede haber mucha variabilidad — Pero tenemos estadísticas de carrera en el conjunto de datos. ¿Quizás deberíamos usar estadísticas de carrera promedio como predictores en su lugar? Podríamos hacer una investigación histórica para averiguar por qué a Schmidt y Kennedy se les pagó tanto en relación con sus prestaciones (¿quizás se lesionaron durante la temporada?) Y por qué a Sax se le pagó comparativamente poco. Dicha investigación podría ayudarnos a identificar información explicativa adicional que podría codificarse en variables e incluirse en el modelo y que podría mejorar el ajuste del modelo. ¿Hay alguna característica compartida por estos valores atípicos que podamos codificar en una variable e incluir como predictor en la regresión? En pocas palabras: no existen reglas estrictas y rápidas sobre qué hacer con los valores atípicos (o incluso lo que cuenta como un valor atípico). Usamos estas herramientas de diagnóstico, las gráficas de distancia y residuales de Cook, para comprender mejor nuestros datos y diseñar enfoques que tengan sentido en el contexto de nuestro proyecto. NOTA IMPORTANTE: Debemos tener mucho cuidado al descartar datos (a parte que no es una buena conducta). Tan sólo podríamos descartarlos si estuviéramos 100% seguros que son errores. De Fox, John (2016). Regresión aplicada y modelos lineales generalizados. Sage: Los Ángeles.↩︎ La gente usa 2 en lugar de 1,96 para un cálculo rápido. De ahí lo de “más menos 2 veces el error estándar”↩︎ De Gelman y Hill (2007). Análisis de datos mediante regresión y modelos jerárquicos / multinivel. Cambridge: Cambridge UP.↩︎ "],
["ajuste-de-modelos.html", "5 Ajuste de modelos 5.1 Reglas generales para la selección de variables 5.2 Selección paso a paso (stepwise) 5.3 Comparación de modelos 5.4 Métodos de selección automática 5.5 Validación cruzada 5.6 Imputación de datos faltantes", " 5 Ajuste de modelos Este capítulo cubre varios temas adicionales relacionados con el ajuste de modelos, como la selección de variables, la comparación de modelos, la validación cruzada y la imputación de datos faltantes. Los conceptos y métodos discutidos aquí se aplican tanto a la regresión lineal como a la logística. Qué pretendemos aprender en este capítulo: Saber cuales son las reglas generales para seleccionar variables en un modelo. Aprender a llevar a cabo esta selección con métodos automáticos (stepwise). Cómo comparar dos modelos Cómo determinar si un modelo tiene sobre-ajuste (overfitting) Dar una pequeña idea a qué hacer cuando tenemos datos faltantes (missing data) 5.1 Reglas generales para la selección de variables ¿Cómo sabemos qué variables (independientes) deben incluirse en un modelo? La respuesta sencilla es: a menudo no lo sabemos. Aquí hay algunas reglas generales cuando se piensa en la selección de variables: Piensa en los datos. ¿Qué variables tiene sentido incluir dada la situación? ¿Alguna literatura publicada ofrece orientación? Si estamos en modo descriptivo, es posible que solo nos interesen determinadas variables y utilicemos las demás como controles. Si estamos en modo predictivo, incluimos todas las variables que, por razones aditivas, podrían ser importantes para predecir el resultado. Sin embargo, esta es una guía muy general, ya que diferentes contextos exigen diferentes enfoques para el ajuste del modelo. Incluir términos cuadráticos si hay evidencia de gráficos bivariados de una relación no lineal entre predictor y resultado. En general, no incluimos términos polinomiales con grados superiores a 2. Para hacerlo, se corre el riesgo de sobreajuste (término del que hablaremos más tarde). Buscar posibles interacciones entre variables con los efectos principales más grandes. En general, no incluimos interacciones de orden superior (mayores que 2) a menos que tengamos una razón lógica y podamos explicarla. También hay que tener en cuenta que las interacciones son bastante difíciles de explicar. Considerar combinar predictores separados en un solo predictor — un “puntaje total” — obtenido al sumarlos o promediarlos. Simplicidad. Los modelos sencillos son casi siempre mejores — son más interpretables y tienden a tener menor variación (principio de parsimonia). 5.2 Selección paso a paso (stepwise) La técnica tradicional en estadística para seleccionar variables es selección paso a paso (o stepwise en inglés). Con selección hacia adelante comenzamos con un modelo nulo (solo contiene el intercept) y agregamos una variable a la vez. Si la variable agregada mejora el modelo, la mantenemos y agregamos otra. Continuamos hasta que se hayan probado todas las variables como podemos ver en la siguiente figura: Selección hacia adelante Con selección hacia atrás comenzamos con un modelo completo (todos los términos disponibles) y eliminamos variables en serie (una a una). Si el modelo es mejor después de eliminar una variable, lo dejamos fuera. Continuamos hasta que se hayan probado todas las variables como podemos ver en la siguiente figura: Selección hacia atrás Selección hacia adelante seguida de selección hacia atrás (saltos). Consiste en ir realizando en cada paso una selección hacia adelante o hacia atrás en función del mejor paso que podamos hacer. Desafortunadamente, estos procedimientos de ajuste manual son defectuosos. Dependen del orden en el que se agregan o excluyen las variables y, a menudo, no seleccionarán el mejor modelo. Además, por ejemplo, supongamos que tenemos una base de datos con \\(k\\) = 13 variables predictoras, lo que significa que hay \\(2^k\\) o 8192 modelos posibles que podríamos ajustar y eso sin tener encuenta la posible introducción de interacciones o términos polinómicos. Este es un espacio extremadamente grande para buscar el mejor modelo, y la búsqueda es computacionalmente costosa y requiere mucho tiempo. Realizar tal búsqueda manualmente sería prácticamente imposible. 5.3 Comparación de modelos Ya estamos familiarizados con los términos \\(R^2\\), RMSE y RSS. Éstos nos servirán como herramientas para comparar modelos. En general, si agregamos una variable y \\(R ^2\\) sube y RMSE / RSS baja, entonces el modelo con la variable adicional siempre es mejor. La cantidad de variación explicada ha aumentado. Sin embargo, como siempre en estadística debemos preguntarnos ¿es este aumento estadísticamente significativo?. Este hecho también introduce un nuevo problema: el sobreajuste. Sabemos que el, \\(R^2\\) ajustado penaliza el ajuste teniendo en cuenta el número de predictores y podría ser una solución. También tenemos otra posibilidad para penalizar la complejidad de los modelos usando métodos de criterio de información como el AIC (Akaike Information Criterion). \\[\\mathrm {AIC} = - 2 \\ln(L) + 2k\\] donde \\(k\\) es el número de parámetros estimados en el modelo, \\(L\\) es el valor maximizado de la función de verosimilitud del modelo y \\(ln\\) es el logaritmo natural. Dado un conjunto de modelos candidatos para los datos, el modelo preferido es el que tiene el valor de AIC más bajo. Al penalizar por \\(k\\) más grandes (garantizados por el término final, \\(+ 2k\\)), AIC intenta protegerse contra el sobreajuste. Es posible, entonces, observar \\(R^2\\) aumentar con la adición de predictores, mientras que AIC baja. También podemos comparar modelos con una prueba estadística formal utilizando la prueba de razón de verosimilitud (LRT por sus siglas en inglés): \\[ 2 \\times [\\ln(L_{a}) - \\ln(L_{c})] \\] donde \\(\\ln(L_{c})\\) es el logaritmo de la probabilidad del modelo actual (o basal) y \\(\\ln (L_{a})\\) es el logaritmo de la probabilidad del modelo alternativo con predictores adicionales. La función lrtest () en el paquete lmtest implementa el test LRT. La función anova () en R base también comparará modelos usando una prueba F. Vamos a ilustrar algunos de ejemplos de comparación de modelos usando los datos de Hitters del paquete ISLR que tiene información sobre bateadores de la Major League de USA entre los años 1986 y 1987. Estamos interesados en crear un modelo para predecir el salario de los bateadores (variable Salary). Partimos de un modelo nulo: library(ISLR) data(Hitters) display(null &lt;- lm(Salary ~ 1, data = Hitters)) ## lm(formula = Salary ~ 1, data = Hitters) ## coef.est coef.se ## (Intercept) 535.93 27.82 ## --- ## n = 263, k = 1 ## residual sd = 451.12, R-Squared = 0.00 round(mean(Hitters$Salary, na.rm = TRUE),2) ## [1] 535.93 Un modelo nulo consiste solo en una intersección, cuyo coeficiente, como podemos ver, es solo la media de Salario. La pregunta clave es si a medida que hacemos un modelo más complejo esa complejidad está justificada. Es decir, si agregar predictores no solo reduce el sesgo sino que lo hace sin aumentar indebidamente la varianza. Veámos qué ocurre si agreguamos más predictores. library(lmtest) display(h1 &lt;- lm(Salary ~ Hits, data = Hitters)) ## lm(formula = Salary ~ Hits, data = Hitters) ## coef.est coef.se ## (Intercept) 63.05 64.98 ## Hits 4.39 0.56 ## --- ## n = 263, k = 2 ## residual sd = 406.17, R-Squared = 0.19 lrtest(null, h1) ## ## Model 1: Salary ~ 1 ## Model 2: Salary ~ Hits ## ## L.R. Chisq d.f. P ## NA 1 NA anova(null, h1) ## Analysis of Variance Table ## ## Model 1: Salary ~ 1 ## Model 2: Salary ~ Hits ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 262 53319113 ## 2 261 43058621 1 10260491 62.194 8.531e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 as.matrix(AIC(null, h1)) ## df AIC ## null 2 3964.130 ## h1 3 3909.918 La variable Hits es estadísticamente significativ, ya que el IC del 95% no incluye 0 (4,39 \\(\\pm\\) 2 x 0,56) (o el p-valor del test de Score es \\(&lt;0.05\\)). Estos tres métodos coinciden en que el modelo con Hits es una mejora con respecto al modelo nulo. En el caso de lrtest () y anova (), el p-valor representa los resultados de una prueba estadística (prueba chi-cuadrado y prueba F, respectivamente) para determinar si el segundo modelo más complejo es un mejor ajuste a los datos. ¿Agregar un predictor adicional, AtBat, mejora aún más el modelo? display(h2 &lt;- lm(Salary ~ Hits + AtBat, data = Hitters)) ## lm(formula = Salary ~ Hits + AtBat, data = Hitters) ## coef.est coef.se ## (Intercept) 141.27 76.55 ## Hits 8.21 2.08 ## AtBat -1.22 0.64 ## --- ## n = 263, k = 3 ## residual sd = 404.13, R-Squared = 0.20 lrtest(h1, h2) ## ## Model 1: Salary ~ Hits ## Model 2: Salary ~ Hits + AtBat ## ## L.R. Chisq d.f. P ## NA 1 NA anova(h1, h2) ## Analysis of Variance Table ## ## Model 1: Salary ~ Hits ## Model 2: Salary ~ Hits + AtBat ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 261 43058621 ## 2 260 42463750 1 594871 3.6423 0.05743 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 as.matrix(AIC(h1, h2)) ## df AIC ## h1 3 3909.918 ## h2 4 3908.260 Los resultados son ambiguos. El \\(R^2\\) aumenta, mientras que AIC, el logaritmo de la probabilidad y el RSS disminuyen, pero la disminución en los dos últimos casos no es estadísticamente significativa. (Este resultado es consistente con el hecho de que AtBat no es en sí mismo estadísticamente significativo, ya que el IC del 95% para AtBat incluye 0: -1.22 \\(\\pm\\) 2 x .64). ¿Deberíamos dejar AtBat en el modelo? No mejora mucho el ajuste, si es que lo hace, al tiempo que agrega complejidad. Entonces, deberíamos sacarlo. Desafortunadamente, estas opciones a menudo no son claras, razón por la cual el ajuste de modelos a veces parece más un arte que una ciencia. Para implementar el método de selección, seguiríamos agregando variables y comparando modelos usando lrtest () o anova () con el fin de encontrar el mejor ajuste posible. Sin embargo, un problema con este procedimiento es que el orden en el que recorremos los predictores afectará nuestras decisiones de selección porque el impacto de cada predictor en el ajuste del modelo depende de la presencia de los demás. Por ejemplo, supongamos que agregamos AtBat más adelante en el proceso de selección: h3 &lt;- lm(Salary ~ Hits + Years + HmRun + RBI + Walks + Assists, data = Hitters) h4 &lt;- lm(Salary ~ Hits + Years + HmRun + RBI + Walks + Assists + AtBat, data = Hitters) lrtest(h3, h4) ## ## Model 1: Salary ~ Hits + Years + HmRun + RBI + Walks + Assists ## Model 2: Salary ~ Hits + Years + HmRun + RBI + Walks + Assists + AtBat ## ## L.R. Chisq d.f. P ## NA 1 NA as.matrix(AIC(h3, h4)) ## df AIC ## h3 8 3849.311 ## h4 9 3840.198 Ahora AtBat mejora claramente el ajuste, pero nunca lo hubiéramos descubierto si ya lo hubiéramos descartado. Esto es preocupante. ¿Existe una forma mejor de seleccionar variables? Quizás, veámoslo. 5.4 Métodos de selección automática Se han desarrollado algoritmos para buscar en el espacio de modelos de manera eficiente el modelo óptimo. Sin embargo, desde el principio conviene tener cuidado con la selección automática de variables. La elección de variables no debe ser un proceso mecánico. Debemos, en cambio, buscar comprender el proceso de generación de datos. De hecho, el mayor beneficio de la selección manual por pasos consiste menos en producir un buen modelo que en la comprensión obtenida al ajustar muchos modelos y ver, mediante prueba y error, qué predictores son más reactivos con el resultado. Especialmente cuando se trata de descripción, los algoritmos de selección automática de variables son solo herramientas para explorar sus datos y pensar en modelos. La función step () en R base automatiza la selección de variables paso a paso usando AIC. display(step(lm(Salary ~ ., data = Hitters), trace = F, direction = &quot;forward&quot;)) ## lm(formula = Salary ~ AtBat + Hits + HmRun + Runs + RBI + Walks + ## Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + ## League + Division + PutOuts + Assists + Errors + NewLeague, ## data = Hitters) ## coef.est coef.se ## (Intercept) 163.10 90.78 ## AtBat -1.98 0.63 ## Hits 7.50 2.38 ## HmRun 4.33 6.20 ## Runs -2.38 2.98 ## RBI -1.04 2.60 ## Walks 6.23 1.83 ## Years -3.49 12.41 ## CAtBat -0.17 0.14 ## CHits 0.13 0.67 ## CHmRun -0.17 1.62 ## CRuns 1.45 0.75 ## CRBI 0.81 0.69 ## CWalks -0.81 0.33 ## LeagueN 62.60 79.26 ## DivisionW -116.85 40.37 ## PutOuts 0.28 0.08 ## Assists 0.37 0.22 ## Errors -3.36 4.39 ## NewLeagueN -24.76 79.00 ## --- ## n = 263, k = 20 ## residual sd = 315.58, R-Squared = 0.55 La selección hacia adelante se estableció en 19 predictores con un \\(R^2\\) de .55. display(step(lm(Salary ~ ., data = Hitters), trace = F, direction = &quot;backward&quot;)) ## lm(formula = Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + ## CRBI + CWalks + Division + PutOuts + Assists, data = Hitters) ## coef.est coef.se ## (Intercept) 162.54 66.91 ## AtBat -2.17 0.54 ## Hits 6.92 1.65 ## Walks 5.77 1.58 ## CAtBat -0.13 0.06 ## CRuns 1.41 0.39 ## CRBI 0.77 0.21 ## CWalks -0.83 0.26 ## DivisionW -112.38 39.21 ## PutOuts 0.30 0.07 ## Assists 0.28 0.16 ## --- ## n = 263, k = 11 ## residual sd = 311.81, R-Squared = 0.54 La selección hacia detrás se estableció en 10 predictores con un \\(R^2\\) de .54. Con muchas menos variables, somos capaces de explicar prácticamente la misma variabilidad. La función regsubsets () en el paquete leaps realiza una búsqueda exhaustiva del espacio modelo utilizando el algoritmo de saltos (adelante y atrás) para la selección de variables. library(leaps) plot(regsubsets(Salary ~ ., data = Hitters, method = &quot;exhaustive&quot;, nbest = 1)) El gráfico presenta diferentes modelos que pueden ser buenos candidatos organizados según el BIC (\\(\\mathrm{BIC} = {\\ln(n) k - 2 \\ ln ({L})}\\), donde \\(L\\) es el valor de máxima verosimilitud, \\(n\\) es el número de observaciones, \\(k\\) es el número de parámetros y \\(ln\\) es el logaritmo natural). Como el AIC, el BIC penaliza por la complejidad del modelo. Un BIC más bajo es mejor. El modelo con el BIC más bajo es el bastante simple en la parte superior de la figura: intercept, AtBat, Hits, Walks, CRBI, DivisionW y PutOuts. Si reajustamos un modelo con estos predictores usando lm () encontramos que tiene un \\(R^2\\) de .51, que tampoco está muy lejos del valor obtenido con un método hacia adelante y con muchísimas menos variables (principio de parsimonia). Notemos que, además, todos los coefecientes de este modelo (a excepción del intercept, pero que es necesario introducir) son estadísticamente significativos según el test de score: summary(lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = Hitters)) ## ## Call: ## lm(formula = Salary ~ AtBat + Hits + Walks + CRBI + Division + ## PutOuts, data = Hitters) ## ## Residuals: ## Min 1Q Median 3Q Max ## -873.11 -181.72 -25.91 141.77 2040.47 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91.51180 65.00006 1.408 0.160382 ## AtBat -1.86859 0.52742 -3.543 0.000470 *** ## Hits 7.60440 1.66254 4.574 7.46e-06 *** ## Walks 3.69765 1.21036 3.055 0.002488 ** ## CRBI 0.64302 0.06443 9.979 &lt; 2e-16 *** ## DivisionW -122.95153 39.82029 -3.088 0.002239 ** ## PutOuts 0.26431 0.07477 3.535 0.000484 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 319.9 on 256 degrees of freedom ## (59 observations deleted due to missingness) ## Multiple R-squared: 0.5087, Adjusted R-squared: 0.4972 ## F-statistic: 44.18 on 6 and 256 DF, p-value: &lt; 2.2e-16 **IMPORTANTE:** ¿Es este modelo realmente mejor? El algoritmo hizo una búsqueda exhaustiva del espacio de modelos y, sin embargo, arrojó un modelo con $R^2$ un poco más bajo. ¿Cómo podría ser mejor? (probablemente lo sea). Si bien el sesgo en este modelo será mayor que en el modelo más grande seleccionado por la función `step ()`, la varianza probablemente sea menor. Recuerde: el sesgo se refiere al rendimiento del modelo dentro de la muestra y la varianza se refiere al rendimiento del modelo fuera de la muestra: cómo se comporta el modelo cuando encuentra nuevos datos. Si el modelo tiene un rendimiento deficiente en datos nuevos, con una gran discrepancia entre el rendimiento dentro y fuera de la muestra, entonces está sobreajustado. AIC, BIC y $R^2$ ajustado penalizan por la complejidad del modelo para evitar el sobreajuste y tenderán a seleccionar modelos con mayor sesgo y menor varianza. 5.5 Validación cruzada La validación cruzada (CV por sus siglas en inglés) es la técnica que usamos para evaluar si un modelo está sobreajustado y para estimar cómo funcionará con nuevos datos. El sobreajuste es un peligro importante en el análisis predictivo, especialmente cuando se utilizan algoritmos de aprendizaje automático que, sin el ajuste adecuado, puede aprender datos de nuestra muestra casi a la perfección, esencialmente ajustando el ruido (o variabilidad). Cuando se utiliza un modelo de este tipo para predecir nuevos datos, con un ruido (o variabilidad) diferente, el rendimiento del modelo puede ser sorprendentemente malo. Usamos CV para ayudarnos a identificar y evitar tales situaciones. ¿Cómo podemos hacer esto? Muchos algoritmos de aprendizaje automático requieren que el usuario especifique ciertos parámetros (hiper-parámetros). Veremos más adelante que, por ejemplo, necesitaremos especificar un valor para \\(m\\) que corresponde al número de predictores elegidos al azar que se utilizarán en cada división de árbol cuando usemos “random forest” como algoritmo de aprendizaje. Cuanto menor sea \\(m\\), más simple será el árbol. Podemos usar CV para elegir el valor de \\(m\\) que minimiza la variación y reduce el sobreajuste. La regresión lineal no tiene parámetros que debe especificar el usuario, pero la CV aún nos ayuda a evaluar cuánto podría sobreajustarse un modelo a los datos de muestra. De manera breve, los algoritmos de cross-validation se pueden resumir como: Reserva una parte pequeña de los datos Crea (o entrena) el modelo usando el resto de datos Testa el modelo en los datos reservados. A continuación se describen algunas de las distintas técnicas de validación cruzada que existen. 5.5.1 Validación en un conjunto de datos externo La versión más simple de CV es el llamado método de conjunto de validación, que consta de los siguientes pasos: Dividir los datos de la muestra en dos partes: un conjunto de entrenamiento y otro de validacións. Los investigadores usan diferentes proporciones, pero es común seleccionar al azar el 70% de los datos como conjunto de entrenamiento y el 30% como conjunto de prueba o validación. . (Obviamente, debemos tener suficientes datos en la muestra para ajustar un modelo después de dividir los datos). Debido a que CV se basa en un muestreo aleatorio, nuestros resultados variarán a menos que usemos set.seed (). Demostraremos usando los datos de Hitters, usando solo casos completos (esto es importante, no tener missings - veremos más adelante alguna forma de solucionar este problema). set.seed(123) Hitters_complete &lt;- Hitters[complete.cases(Hitters), ] rows &lt;- sample(nrow(Hitters_complete), .7 * nrow(Hitters_complete)) train &lt;- Hitters_complete[rows, ] test &lt;- Hitters_complete[-rows, ] Ajustar un modelo en el conjunto de entrenamiento usando un procedimiento de selección de variables apropiado. Crearemos dos modelos para comparar: uno con todas las variables, luego otro con solo las variables elegidas por regsubsets (). full_model &lt;- lm(Salary ~., data = train) select_model &lt;- lm(Salary ~ AtBat + Hits + Walks + CRBI + Division + PutOuts, data = train) Utilizar ese modelo para predecir en el conjunto de prueba. El rendimiento en el conjunto de prueba es la estimación de CV para el rendimiento fuera de la muestra del modelo. results &lt;- data.frame(Model = c(&quot;Modelo completo muestra entrenamiento&quot;, &quot;Modelo seleccionado muestra entrenamiento&quot;, &quot;Modelo completo muestra validación&quot;, &quot;Modelo seleccionado muestra validación&quot;), RMSE = round(c(rmse(fitted(full_model), train$Salary), rmse(fitted(select_model), train$Salary), rmse(predict(full_model, newdata = test), test$Salary), rmse(predict(select_model, newdata = test), test$Salary)),1)) results ## Model RMSE ## 1 Modelo completo muestra entrenamiento 297.8 ## 2 Modelo seleccionado muestra entrenamiento 326.1 ## 3 Modelo completo muestra validación 368.2 ## 4 Modelo seleccionado muestra validación 306.4 Podemos ver que el modelo completo está sobreajustado — el RMSE dentro de la muestra es mejor que el RMSE fuera de muestra — mientras que el modelo seleccionado elegido por regsubsets () usando BIC no está sobreajustado. De hecho, el modelo seleccionado funciona mucho mejor fuera de la muestra que dentro de la muestra, aunque este resultado en particular es probablemente una cuestión de azar, una función de división aleatoria que estamos usando. Sin embargo, en general, estos resultados ilustran el peligro de la complejidad del modelo y por qué tiene sentido elegir predictores utilizando medidas de ajuste del modelo que penalicen la complejidad. Los modelos simples tienden a generalizar mejor. Esta figura muestra estas relaciones: Sobreajuste 5.5.2 Leave-one-out cross validation (LOOCV) Este método funciona de la siguiente manera: Extrae una observación de los datos y usa el resto para entrenar el modelo Testa el modelo con la observación que ha sido extraída en el paso anterior y guarda el error asociado a esa predicción Repite el proceso para todos los puntos Calcula el error de predicción global usando el promedio de todos los errores estimados en el paso 2. Veremos más adelante cómo hacer estos cálculos con una libería específica. De momento, para que aprendáis cómo funciona esta metodología, debéis realizar el siguiente ejercicio EJERCICIO (Entrega en Moodle: P2-LOOCV): Crea una función R que lleve a cabo el procedimiento de LOOCV y estima el valor de LOOCV para el modelo completo y el modelo seleccionado del ejemplo anterior. 5.5.3 K-fold cross validation (K-fold CV) La diferencia con LOOCV es que este método evalúa el comportamiento del modelo en un conjunto de datos de distingo tamaño (K). El algoritmo es el siguiente: Separa los datos en k-subconjuntos (k-fold) de forma aleatoria Guarda uno de los subconjuntos de datos y entrena el modelo con el resto de individuos Testa el modelo con los datos resevados y guarda el error de predicción promedio. Repite el proceso hasta que los k subconjuntos hayan servido de muestra test. Calcula el promedio de los k errores que han sido guardados. Este valor es el error de cross-validación y nos sirve para evaluar el comportamiento de nuestro modelo como si lo usáramos en una base de datos externa. La principal ventaja de este método respecto a LOOCV es el coste computacional. Otra ventaja que no es tan obvia, es que este método a menudo da mejores estimaciones del error del modelo que LOOCV5. Una pregunta típica es cómo se escoje el valor óptimo de K. Valores pequeños de K da estimaciones sesgadas. Por otro lado, valores grandes de K están menos sesgados, pero tienen mucha variabilidad. En la práctica, normalmente se usan valores de k = 5 or k = 10, ya que estos valores se han mostrado de forma empírica como los que tienen tasas de error estimadas no demasiado sesgadas ni con mucha varianza. Al igual que en el caso anterior veremos unas liberías adecuadas para hacer estos análisis de forma eficiente. De momento realiza el siguiente ejercicio: EJERCICIO (Entrega en Moodle: P2-Kfold): Crea una función R que lleve a cabo el procedimiento de K-fold CV y estima el valor de K-fold CV para el modelo completo y el modelo seleccionado del ejemplo anterior. Haz que la función tenga un parámetro que dependa de K, y da los resultados para K=5 y K=10. 5.5.4 Uso de CV para estimar el hiper-parámetro Si el algoritmo de aprendizaje automático que vamos a utilizar para realizar predicciones tiene un parámetro que controla el comportamiento (por ejemplo grado de polinomio en regresión no lineal, o el número de nodos en árboles de clasificación) éste podría elegirse de forma que minimizara el error de clasificación. Esta selección también puede dar problemas de sobre ajuste ya que podríamos seleccionar de forma que ajustara perféctamente a nuestros datos. Para evitar el problema, se puede utilizar cualquiera de las técnicas vistas con anterioridad. Aquí tenemos un ejemplo donde se ha usado un modelo de aprendizaje que se basa en introducir términos polinómicos de varaibles para realizar una mejor predicción mediante regresión lineal usando sólo términos lineales. Sobreajuste según un hiper-parámetro 5.5.5 Uso de bootstrap Si en vez de partir nuestra muestra en \\(K\\) submuestras, realizamos una selección aleatoria de muestras con reemplazamiento, nos encontraremos ante una aproximación de tipo bootstrap que es una técnica muy usada en estadística para hacer inferencia cuando la distribución del estadístico es desconocida basada en el remuestreo [aquí tenéis una descripción sencilla de esta metodología]. Boostrap Boostrap De manera que el procedimiento bootstrap aplicado a regresión sería: Sacar una muestra aleatoria con remplazamiento de tamaño \\(n\\) de nuestros datos (tenemos \\(n\\) observaciones) Guardar las muestras que no han sido seleccionadas (datos de prueba) Entrena el modelo con la muestra bootstrap Testa el modelo con los datos de prueba y guarda el error de predicción promedio. Repite el proceso \\(B\\) veces Calcula el promedio de los \\(B\\) errores que han sido guardados. Este valor es el error bootstrap y nos sirve para evaluar el comportamiento de nuestro modelo. EJERCICIO (Entrega en Moodle: P2-bootstrap): Crea una función R que lleve a cabo el procedimiento boostrap y estima el valor de este método para el modelo completo y el modelo seleccionado del ejemplo anterior. Haz que la función tenga un parámetro que dependa de \\(B\\), y da los resultados para B=25, B=50 y B=100. Comenta brevemente los resultados 5.6 Imputación de datos faltantes La mayoría de métodos para aprendizaje automático requiren casos completos. Sin embargo, los datos reales a menudo tienen observaciones faltantes. La función lm (), analiza casos completos sin indicar nada al usuario, pero … ¿Deberíamos eliminar estas filas o imputar las observaciones que faltan? Casi siempre es mejor imputar, aunque, en la práctica puede que no valga la pena imputar algunas observaciones faltantes, ya que eliminarlas no suele cambiar el ajuste en absoluto. La imputación de datos faltantes es un tema extenso y complicado; aquí haremos una breve introducción y discutiremos los principales temas a tener en cuenta. Tipos de valores perdidos: Falta completamente al azar (MCAR por sus siglas en inglés): la probabilidad de que falte una observación es la misma para todos los casos. Eliminar los casos que faltan en esta instancia no causará sesgos, aunque es posible que perdamos información. Missing at random (MAR pos sus siglas en inglés): la probabilidad de que falte una observación depende de un mecanismo conocido. Por ejemplo, es menos probable que algunos grupos respondan encuestas. Si conocemos la pertenencia a un grupo, podemos eliminar las observaciones faltantes siempre que incluyamos el grupo como factor en una regresión. Sin embargo, generalmente podemos hacer algo mejor que simplemente eliminar estos casos. Missing not at random (MNAR por sus siglas en inglés) : la probabilidad de que falte una observación depende de algún mecanismo desconocido — una variable no observada. Tratar los problemas del MNAR es difícil o incluso imposible. Nos centraremos en los problemas MAR. Una solución simple es completar o imputar los valores MAR. Hay dos estrategias principales: Imputación simple reemplaza los valores perdidos según una estadística univariante o un modelo de regresión multivariable. Existen muchas librerías que implementan diferentes métodos (en este curso veremos algunas). En la imputación con medianas imputamos los datos faltantes usando la mediana de la variable que presenta datos faltantes (La mediana es mejor que la media cuando los datos de la columna están sesgados). Podemos imputar también usando KNN o random forest creando un modelo multivariante de las observaciones faltantes usando otras variables y usar ese modelo para predecir los valores faltantes. El problema con la imputación simple, teóricamente, es que la variabilidad de la variable imputada es menor de lo que habría sido la variabilidad en la variable real, creando un sesgo hacia 0 en los coeficientes. Por tanto, mientras que la eliminación pierde información, la imputación única puede provocar sesgos. (Sin embargo, no me queda claro cuán grande es este problema en la práctica). La imputación múltiple aborda estos problemas imputando los valores faltantes con un modelo multivariante, pero agregando la variabilidad de nuevo al volver a incluir la variación del error que normalmente veríamos en los datos. El término “múltiple” en la imputación múltiple se refiere a los múltiples conjuntos de datos creados en el proceso de estimación de los coeficientes de regresión. Los pasos son los siguientes: Crear \\(m\\) conjuntos de datos completos con valores perdidos imputados. Las imputaciones se realizan extrayendo aleatoriamente distribuciones de valores plausibles para cada vector de columna (variables). Ajustar un modelo lineal en cada conjunto de datos imputados y almacene \\(\\hat \\beta\\)s y SE. Promediar los \\(\\hat \\beta\\)s y combinar los SE para producir coeficientes basados en múltiples conjuntos de datos imputados. Específicamente, \\[\\hat \\beta_ {j} = \\frac {1} {m} \\sum_ {i} \\hat \\beta_ {ij}\\] y \\[s ^ 2_j = \\frac {1} {m} \\sum_{i} s^2_{ij} + var \\hat \\beta_ {ij} (1 + 1 / m),\\] donde \\(\\hat \\beta_{ij}\\) y \\(s_{ij}\\) son las estimaciones y los errores estándar del resultado imputado \\(i^{th}\\) para \\(i=1, ..., m\\) y para el parámetro \\(j^{th}\\). La imputación múltiple funciona mejor para la descripción que para la predicción, y probablemente sea preferible a la imputación única si sólo queremos estimar coeficientes. Para la predicción (como es el caso del aprendizaje automático), normalmente bastará con utilizar imputación simple. Demostraremos métodos de imputación utilizando los datos de Carseats del paquete ISLR. Este es un conjunto de datos simulado de ventas de asientos de coche, del cual eliminaremos aleatoriamente el 25% de las observaciones usando la función prodNA () en el paquete missForest (teniendo cuidado de dejar la variable de resultado, Sales, intacta). library(missForest) data(Carseats) levels(Carseats$ShelveLoc) &lt;- c(&quot;Bad&quot;,&quot;Medium&quot;,&quot;Good&quot;) # Reordenamos los niveles de la variable set.seed(123) carseats_missx &lt;- prodNA(Carseats[,-1], noNA=.25) carseats_miss &lt;- cbind(Sales=Carseats[, 1], carseats_missx) glimpse(carseats_miss) ## Rows: 400 ## Columns: 11 ## $ Sales &lt;dbl&gt; 9.50, 11.22, 10.06, 7.40, 4.15, 10.81, 6.63, 11.85, 6.54, 4.69, 9.01, 11.96, 3.98, 10.96, 11.17, 8.71, 7... ## $ CompPrice &lt;dbl&gt; 138, 111, 113, 117, 141, 124, 115, NA, NA, NA, 121, 117, NA, 115, 107, NA, 118, NA, 110, 129, 125, 134, ... ## $ Income &lt;dbl&gt; 73, 48, 35, 100, 64, 113, NA, 81, 110, 113, 78, 94, NA, 28, 117, 95, 32, 74, 110, 76, NA, NA, 46, NA, 11... ## $ Advertising &lt;dbl&gt; 11, 16, NA, 4, 3, 13, NA, 15, 0, 0, 9, 4, 2, NA, 11, 5, NA, 13, 0, 16, 2, 12, 6, 0, 16, 0, 11, 0, NA, 15... ## $ Population &lt;dbl&gt; 276, 260, 269, NA, 340, 501, 45, 425, 108, 131, 150, 503, NA, 29, 148, 400, 284, 251, 408, 58, 367, 239,... ## $ Price &lt;dbl&gt; 120, NA, NA, 97, 128, 72, 108, 120, NA, 124, 100, NA, NA, NA, 118, 144, 110, 131, 68, 121, NA, 109, 138,... ## $ ShelveLoc &lt;fct&gt; Bad, NA, Good, NA, Bad, Bad, Good, NA, Good, Good, Bad, Medium, NA, Medium, Medium, Good, Medium, Medium... ## $ Age &lt;dbl&gt; 42, 65, NA, 55, 38, NA, 71, 67, 76, 76, 26, 50, NA, 53, 52, 76, 63, 52, 46, 69, NA, NA, NA, 79, 42, 54, ... ## $ Education &lt;dbl&gt; NA, 10, 12, NA, 13, 16, 15, 10, 10, 17, 10, 13, NA, NA, NA, 18, 13, 10, 17, 12, 18, NA, NA, NA, 12, 11, ... ## $ Urban &lt;fct&gt; NA, Yes, Yes, Yes, Yes, NA, NA, Yes, No, NA, NA, Yes, Yes, Yes, Yes, No, Yes, Yes, No, NA, Yes, No, Yes,... ## $ US &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, No, Yes, NA, Yes, Yes, Yes, No, Yes, Yes, No, No, NA, Yes, Yes, NA, Yes, No... Ahora faltan muchas observaciones. Cuando ajustamos un modelo de regresión para la variable Sales observamos que lm () analiza casos completos y se estima un modelo basado en un subconjunto muy pequeño de datos. display(lm(Sales ~ CompPrice + Income + Advertising + Population + Price, data = carseats_miss)) ## lm(formula = Sales ~ CompPrice + Income + Advertising + Population + ## Price, data = carseats_miss) ## coef.est coef.se ## (Intercept) 6.24 1.99 ## CompPrice 0.10 0.02 ## Income 0.01 0.01 ## Advertising 0.13 0.03 ## Population 0.00 0.00 ## Price -0.11 0.01 ## --- ## n = 93, k = 6 ## residual sd = 2.06, R-Squared = 0.59 Sólo tenemos 93 observaciones de las 400 originales! Demostraremos la imputación múltiple usando la función mice () de la librería mice. library(mice) names(Carseats) ## [1] &quot;Sales&quot; &quot;CompPrice&quot; &quot;Income&quot; &quot;Advertising&quot; &quot;Population&quot; &quot;Price&quot; &quot;ShelveLoc&quot; &quot;Age&quot; ## [9] &quot;Education&quot; &quot;Urban&quot; &quot;US&quot; carseats_imp &lt;- mice(carseats_miss, printFlag = F) El objeto carseats_imp incluye (entre muchas otras cosas) \\(m\\) conjuntos de datos imputados (la configuración predeterminada es \\(m\\) = 5). Los conjuntos de datos imputados difieren porque las imputaciones se extraen aleatoriamente de distribuciones de valores plausibles. Podemos visualizar la variabilidad de los predictores en estos conjuntos de datos imputados usando la función densityplot (). library(lattice) densityplot(carseats_imp) Las líneas azules continuas representan la distribución real de los predictores, mientras que las líneas rojas muestran las distribuciones imputadas. El siguiente paso es usar estos conjuntos de datos imputados para promediar los \\(\\beta\\)s y los SE utilizando la función pool () de la librería mice. carseats_model_imp &lt;- with(data = carseats_imp, exp = lm(Sales ~ CompPrice + Income + Advertising + Population + Price)) mi &lt;- summary(pool(carseats_model_imp)) Estos coeficientes son similares a los del modelo anterior ajustado utilizando los datos no imputados, pero deberían estar más cerca de los valores de la población porque, en lugar de simplemente eliminar los casos incompletos, utiliza información de distribución para hacer suposiciones fundamentadas sobre los datos faltantes. La imputación múltiple funciona mejor para fines de descripción — estimar coeficientes para informar en un artículo académico, por ejemplo — pero usarla para predecir nuevos datos es incómodo o imposible, por las siguientes razones: Si los nuevos datos están completos, podemos utilizar las estimaciones de coeficientes derivadas de la imputación múltiple en una ecuación de regresión para la predicción. Pero esto es difícil ya que hay que hacerlo manualmente. Usamos los datos originales de Carseats como ilustración. preds &lt;- mi[1, 2] + mi[2, 2]*Carseats$CompPrice + mi[3, 2]*Carseats$Income + mi[4, 2]*Carseats$Advertising + mi[5, 2]*Carseats$Population + mi[6, 2]*Carseats$Price head(preds) ## [1] 8.989509 10.150578 9.645498 8.405464 7.306136 12.735945 Si los nuevos datos no están completos, entonces estos coeficientes imputados son inútiles para predecir en filas con observaciones faltantes. Esto, por ejemplo, es el resultado de intentar predecir utilizando los datos con observaciones faltantes. preds &lt;- mi[1, 2] + mi[2, 2]*carseats_miss$CompPrice + mi[3, 2]*carseats_miss$Income + mi[4, 2]*carseats_miss$Advertising + mi[5, 2]*carseats_miss$Population + mi[6, 2]*carseats_miss$Price head(preds) ## [1] 8.989509 NA NA NA 7.306136 12.735945 La imputación múltiple, por lo tanto, no resuelve el principal problema al que nos enfrentamos a menudo con los datos faltantes, que es que, aunque hayamos ajustado con éxito un modelo en nuestros datos, el conjunto de validación también puede tener observaciones faltantes, y nuestras predicciones utilizando esos datos puede no poder realizarse. Podríamos usar uno de los conjuntos de datos imputados, pero entonces ya no estamos haciendo imputación múltiple sino imputación simple. En ese momento, los métodos disponibles en el paquete mice ya no ofrecen ninguna ventaja especial sobre los de los paquetes caret y missForest. De hecho, podrían ser peores ya que la función mice () no fue diseñado para producir la mejor imputación individual, sino más bien una gama de imputaciones plausibles. Usando caret, podemos hacer una imputación simple usando knnImpute, medianImpute o bagImpute. Estos métodos solo funcionan para variables numéricas, por lo que crearemos una función personalizada para convertir los factores — Shelveloc, Urban y US — en números enteros. (Al usar el conjunto de datos imputados para la regresión, podríamos dejar estas variables como números enteros, siempre que los valores enteros correspondan a los niveles de los factores). make_df_numeric &lt;- function(df){ data.frame(sapply(df, function(x) as.numeric(x))) } carseats_miss_num &lt;- make_df_numeric(carseats_miss) med_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;medianImpute&quot;)), carseats_miss_num) knn_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;knnImpute&quot;)), carseats_miss_num) bag_imp &lt;- predict(preProcess(carseats_miss_num, method = c(&quot;bagImpute&quot;)), carseats_miss_num) El paquete missForest ofrece otra solución de imputación única, que es más simple que las funciones de caret porque maneja datos categóricos automáticamente. Si bien missForest funciona bien para conjuntos de datos pequeños y proporciona imputaciones de buena calidad, es muy lento en conjuntos de datos grandes. De hecho, lo mismo ocurrirá con la función bagImpute () de caret. En tales casos, podría tener sentido usar la función medianImpute () de caret en su lugar que es muy rápida. mf_imp &lt;- missForest(carseats_miss, verbose = F) ## missForest iteration 1 in progress...done! ## missForest iteration 2 in progress...done! ## missForest iteration 3 in progress...done! ## missForest iteration 4 in progress...done! ## missForest iteration 5 in progress...done! Comparemos los errores asociados con estos diferentes métodos de imputación. Podemos hacer esto porque, habiendo creado las observaciones faltantes en primer lugar, podemos comparar las observaciones imputadas con las observaciones verdaderas calculando la suma de los cuadrados de la diferencia. Para las imputaciones usando mice () calculamos los errores para cada uno de los 5 conjuntos de datos imputados. Los resultados de knnImpute () no son comparables porque la función automáticamente centra y escala las variables y los hemos omitido. comparison &lt;- data.frame(Method = c(&quot;mice 1&quot;, &quot;mice 2&quot;, &quot;mice 3&quot;, &quot;mice 4&quot;, &quot;mice 5&quot;, &quot;medianImpute&quot;, &quot;bagImpute&quot;, &quot;missForest&quot;), RSS = c(rss(make_df_numeric(complete(carseats_imp, 1)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(carseats_imp, 2)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(carseats_imp, 3)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(carseats_imp, 4)), make_df_numeric(Carseats)), rss(make_df_numeric(complete(carseats_imp, 5)), make_df_numeric(Carseats)), rss(med_imp, make_df_numeric(Carseats)), rss(bag_imp, make_df_numeric(Carseats)), rss(make_df_numeric(mf_imp$ximp), make_df_numeric(Carseats)))) comparison %&gt;% mutate(RSS = round(RSS)) %&gt;% arrange(RSS) ## Method RSS ## 1 missForest 2489418 ## 2 medianImpute 2538059 ## 3 bagImpute 2714857 ## 4 mice 4 3752513 ## 5 mice 5 4389532 ## 6 mice 2 4399586 ## 7 mice 1 4564721 ## 8 mice 3 4791521 missforest obtiene los mejores resultados, aunque medianImpute compara muy bien. Los resultados de mice no son muy buenos, probablemente por las razones mencionadas anteriormente: está diseñado para una imputación múltiple, no simple. James et al. 2014↩︎ "],
["regresión-logística.html", "6 Regresión logística 6.1 La función logit inversa 6.2 Ejemplo de regresión logística 6.3 Coeficientes de regresión logística como probabilidades 6.4 Coeficientes de regresión logística como razones de odds 6.5 Bondad de ajuste 6.6 Ejemplo de regresión logística: modelización de riesgo diabetes 6.7 Creación de un modelo y validación 6.8 Nomogramas", " 6 Regresión logística Este capítulo introduce la regresión logística como el método más sencillo para crear modelos predictivos en problemas de clasificación que es el principal objetivo de la asignatura. Se cubrirán los siguientes temas: Conocer la función logística Cómo interpretar los coeficientes de los modelos Cómo evaluar la bondad de ajuste Cómo interpretar variables Ilustrar un ejemplo de análisis completo Aprender a hacer nomogramas fijos y dinámicos Hasta ahora, nuestra variable de resultado era continua. Pero si la variable de resultado es binaria (0/1, “No”/“Sí”), entonces nos enfrentamos a un problema de clasificación. El objetivo de la clasificación es crear un modelo capaz de clasificar el resultado — y, cuando se usa el modelo para la predicción, nuevas observaciones— en una de dos categorías. La regresión logística se introduce en el contexto de la epidemiología como un modelo de regresión que extiende el modelo lineal cuando nuestra variable respuesta es binaria, pero tambié es, probablemente, el método estadístico más utilizado para la clasificación y el más sencillo. Una de las grandes ventajas de estos modelos respectoa otros que veremo más adelante es que este método produce un modelo de probabilidad para nuestra variable resultado. En otras palabras, los valores ajustados en un modelo logístico o logit no son binarios sino que son probabilidades que representan la probabilidad de que el resultado pertenezca a una de nuestras dos categorías. Desafortunadamente, debemos afrontar nuevas complicaciones cuando trabajamos con regresión logística, lo que hace que estos modelos sean inherentemente más difíciles de interpretar que los modelos lineales. Las complicaciones surgen del hecho de que con la regresión logística modelamos la probabilidad de que \\(y\\) = 1, y la probabilidad siempre se escala entre 0 y 1. Pero el predictor lineal, \\(X_i \\beta\\), oscila entre \\(\\pm \\infty\\) (donde \\(X\\) representa todos los predictores del modelo). Esta diferencia de escala requiere transformar la variable de resultado, lo cual se logra con la función logit: \\[ \\text{logit}(x) = \\text{log}\\left( \\frac{x}{1-x} \\right) \\] La función logit asigna el rango del resultado (0,1) al rango del predictor lineal \\((-\\infty, +\\infty)\\). El resultado transformado, \\(\\text{logit} (x)\\), se expresa en logaritmos de probabilidades (\\(\\frac{x}{1-x}\\) se conoce como probabilidades del resultado - razón de odds en inglés - momios en castellano). Así que el modelo también se puede escribir como: \\[\\text{Pr}(y_i = 1) = p_i\\] \\[\\text{logit}(p_i) = X_i\\beta\\] Las probabilidades logarítmicas (e.g. el log-odds) no tienen interpretación (que no sea el signo y la magnitud) y deben transformarse nuevamente en cantidades interpretables, ya sea en probabilidades, usando el logit inverso, o en razones de probabilidades, mediante el uso de la función exponencial. Discutimos ambas transformaciones a continuación. 6.1 La función logit inversa El modelo logístico se puede escribir, alternativamente, usando el logit inverso: \\[ \\operatorname{Pr}(y_i = 1 | X_i) = \\operatorname{logit}^{-1}(X_i \\beta) \\] donde \\(y_i\\) es la respuesta binaria, \\(\\operatorname{logit}^{- 1}\\) es la función logit inversa y \\(X_i \\beta\\) es el predictor lineal. Podemos interpretar esta formulación diciendo que la probabilidad de que \\(y = 1\\) es igual al logit inverso del predictor lineal \\((X_i, \\ beta)\\). Por lo tanto, podemos expresar los valores ajustados del modelo logístico y los coeficientes como probabilidades utilizando la transformación logit inversa. Pero, ¿qué es exactamente el logit inverso? Pues es: \\[\\operatorname{logit}^{-1}(x) = \\frac{e^{x}}{1 + e^{x}}\\] donde \\(e\\) es la función exponencial. Podemos tener una idea de cómo la función logit inversa transforma el predictor lineal mediante una gráfica. Aquí usamos un rango arbitrario de valores de x en (-6, 6) para demostrar la transformación. x &lt;- seq(-6, 6, .01) y &lt;- exp(x)/(1 + exp(x)) ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_line() + ylab(expression(paste(logit^-1,&quot;(x)&quot;))) + ggtitle(expression(paste(&quot;y = &quot;, logit^-1,&quot;(x)&quot;))) Los valores \\(x\\), que van de -6 a 6, son comprimidos por la función logit inversa en el rango 0-1. El logit inverso es curvo, por lo que la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) no es constante. A valores bajos y valores altos de \\(x\\), un cambio de unidad corresponde a un cambio muy pequeño en \\(y\\), mientras que en la mitad de la curva un pequeño cambio en \\(x\\) corresponde a un cambio relativamente grande en \\(y\\). En la regresión lineal, la diferencia esperada en \\(y\\) correspondiente a una diferencia fija en \\(x\\) es, por el contrario, constante. Por lo tanto, cuando interpretamos los resultados logísticos debemos elegir en qué parte de la curva queremos evaluar la probabilidad del resultado, dado el modelo. 6.2 Ejemplo de regresión logística Ilustremos estos conceptos utilizando el conjunto de datos “Default” del ISLR. Este conjunto de datos simulado contiene una variable binaria que representa el incumplimiento en los pagos de la tarjeta de crédito (variable “default”), que modelaremos como una función de la variable “balance” (la cantidad de deuda que tiene la tarjeta) y la variable “income” (ingresos). Primero visualizaremos cómo es esta relación. library(ISLR) data(Default) str(Default) ## &#39;data.frame&#39;: 10000 obs. of 4 variables: ## $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ... ## $ balance: num 730 817 1074 529 786 ... ## $ income : num 44362 12106 31767 35704 38463 ... ggplot(Default, aes(x = balance, y = income, col = default)) + geom_point(alpha = .4) + ggtitle(&quot;Balance vs. Income by Default&quot;) grid.arrange( ggplot(Default, aes(default, balance)) + geom_boxplot() + ggtitle(&quot;Balance by Default&quot;) + ylab(&quot;balance&quot;), ggplot(Default, aes(default, income)) + geom_boxplot() + ggtitle(&quot;Income by Default&quot;) + ylab(&quot;income&quot;), ncol = 2) Claramente, los valores altos de saldo están asociados con el incumplimiento en todos los niveles de ingresos. Esto sugiere que los ingresos en realidad no son un fuerte predictor de incumplimiento, en comparación con el saldo, que es exactamente lo que vemos en los diagramas de cajas. Exploremos estas relaciones usando la regresión logística. En R ajustamos un modelo logístico usando la función glm () con family = binomial.6 Centraremos y escalaremos las variables para facilitar la interpretación. glm(default ~ balance + income, data = Default, family = binomial) %&gt;% standardize %&gt;% display ## glm(formula = default ~ z.balance + z.income, family = binomial, ## data = Default) ## coef.est coef.se ## (Intercept) -6.13 0.19 ## z.balance 5.46 0.22 ## z.income 0.56 0.13 ## --- ## n = 10000, k = 3 ## residual deviance = 1579.0, null deviance = 2920.6 (difference = 1341.7) NOTA: Se puede apreciar la ventaja del uso de tidyverse (pipe) - no se necesita crear las variables estandarizadas, ni guardar el resultado para luego hacer un print Interpretamos esta salida exactamente como lo haríamos para un modelo lineal con un predictor centrado y escalado: Intercept: -6,13 representa las probabilidades logarítmicas (log odds) de incumplimiento cuando el saldo es promedio (835.37) y el ingreso es promedio (3.351698^{4} ). (Promedio porque las variables se han centrado). z.balance: 5.46 representa el cambio predicho en las probabilidades logarítmicas de incumplimiento asociado con un aumento de 1 unidad en el saldo (z.balance), manteniendo constante el ingreso (z.income). Un aumento de 1 unidad en el saldo (z.balance) es equivalente a un aumento de 2 desviaciones estándar en el saldo (balance) (967.43). Este coeficiente es estadísticamente significativo ya que 5.46 - 2 x .22 &gt; 0 (el IC del 95% que no contiene 0 indica significación estadística). z.income: .56 representa el cambio predicho en las probabilidades logarítmicas (log odds) de incumplimiento asociadas con un aumento de 1 unidad en el ingreso (z.income), manteniendo constante el balance (z.balance). Un aumento de 1 unidad en el ingreso (z.income) es equivalente a un aumento de 2 desviaciones estándar en el ingreso (income) (2.667328^{4}). Este coeficiente también es estadísticamente significativo ya que .56 - 2 x .13 &gt; 0. ¿Qué significa que las probabilidades logarítmicas de incumplimiento aumenten en 5.46 o .56? En términos precisos, ¿quién sabe? Para que estas cantidades tengan una mejor interpretación, necesitamos transformarlas, ya sea en probabilidades (odds) o en razones de probabilidades (razón de odds -&gt; odds ratio). Sin embargo, debemos señalar que el signo y la magnitud de los coeficientes si son informativas: la relación con el incumplimiento del pago es positiva en ambos casos y, como ya se había visto de forma gráfica en los diagramas de cajas, el efecto del saldo (balance) es mucho mayor que el del ingreso (income). 6.3 Coeficientes de regresión logística como probabilidades Podemos dar una interpretación más específica de la regresión logística más allá del efecto y magnitud. Para ello, podemos usar la función logit inversa para convertir las probabilidades logarítmicas (log-odds) de incumplimiento de pago en las tarjetas (cuando el saldo y los ingresos son promedio) en una probabilidad: invlogit &lt;- function(x) exp(x)/(1 + exp(x)) invlogit(-6.13 + 5.46 * 0 + .56 * 0) ## [1] 0.002171854 La probabilidad de incumplimiento para aquellos con un saldo promedio de tarjeta de crédito de (835.37) y un ingreso promedio de (3.351698^{4}) es de hecho bastante bajo: solo 0.002. Asimismo, podemos calcular el cambio en la probabilidad de incumplimiento en el pago asociado con un aumento de 1 unidad en el saldo, manteniendo el ingreso constante en el promedio (z.ingreso=0). Esto equivaldría a aumentar el saldo en casi 1000$, de 835.37 a 1802.8. invlogit(-6.13 + 5.46 * 1) - invlogit(-6.13 + 5.46 * 0) ## [1] 0.336325 6.4 Coeficientes de regresión logística como razones de odds También podemos interpretar los coeficientes de regresión logística como razones de odds (OR).7 Si dos resultados tienen probabilidades \\((p, 1-p)\\), entonces \\(\\frac {p} {1-p}\\) se conoce como odds (probabilidades o momio) del resultado. Las odds son simplemente diferentes formas de representar la misma información: \\(\\text{odds} = \\frac{p}{1-p}\\) y \\(p = \\frac{\\text{odds}} {1+ \\text{odds}}\\). Por ejemplo, una odds de 1 es equivalente a una odds de .5 — es decir, resultados igualmente probables para \\(p\\) y \\(1-p\\): \\(\\text{odds(p = .5)} = \\frac{.5}{1-.5} = 1\\) y \\(p(\\text{oods} = 1) = \\frac{\\text{1}}{1 + 1} = .5.\\) La razón de dos odds es una OR: \\[ \\frac{\\frac{p_2}{1-p_2}}{\\frac{p_1}{1-p_1}} \\] Una razón de odds se puede interpretar como un cambio en la probabilidad. Por ejemplo, un cambio en la probabilidad de \\(p_1 = .33\\) a \\(p_2 = .5\\) da como resultado un OR de 2, de la siguiente manera: \\[ \\frac{\\frac{.5}{.5}}{\\frac{.33}{.66}} = \\frac{1}{.5} = 2 \\] También podemos interpretar el OR como el aumento porcentual de las probabilidades de un evento. Aquí, un OR de 2 equivale a aumentar las probabilidades en un 100%, de 0,5 a 1. Recordemos que representamos el modelo logit de esta manera: \\[ \\text{log} \\left(\\frac{p}{1-p}\\right) = \\alpha + \\beta x \\] La parte izquierda de la ecuación, expresado como logaritmos de probabilidades, está en la misma escala que la derecha derecho: \\(\\pm \\infty\\). Por lo tanto, no hay no linealidad en esta relación, y aumentar \\(x\\) en 1 unidad tiene el mismo efecto que en la regresión lineal: cambia el resultado predicho en \\(\\beta\\). Entonces, pasar de \\(x\\) a \\(x + 1\\) equivale a sumar \\(\\beta\\) a ambos lados de la ecuación anterior. Centrándonos solo en el lado izquierdo, tenemos, después de exponenciar, las probabilidades originales multiplicadas por \\(e^\\beta\\): \\[ e^{\\text{log} \\left(\\frac{p}{1-p}\\right) + \\beta} = \\frac{p}{1-p} *e^ \\beta \\] (ya que \\(e^{a+b} = e^a*e^b\\)). Podemos pensar en \\(e^\\beta\\) como el cambio de la odds del resultado cuando \\(x\\) cambia en 1 unidad, que se puede representar, utilizando la formulación anterior, como una OR: \\[ \\frac{\\frac{p_2}{1-p_2}}{\\frac{p_1}{1-p_1}} = \\frac{\\frac{p_1}{1-p_1} * e^\\beta }{\\frac{p_1}{1-p_1}} = e^\\beta. \\] Por lo tanto, \\(e^\\beta\\) se puede interpretar como el cambio en las probabilidades asociadas con un aumento de 1 unidad en \\(x\\), expresado en términos porcentuales. En el caso de OR = \\(\\frac{1}{. 5} = 2\\), el porcentaje de aumento en las probabilidades del resultado es del 100%. Cuando la OR es \\(&gt;2\\) se suele expresar como x-veces más (OR=3.5, hay 3.5 veces más probabilidad de observar el evento que no observarlo cuando se cambia \\(x\\) en 1 unidad), y cuando la OR es \\(&lt;1\\) se suele hablar de protección a no tener el evento y el porcentaje se calcula como 1-OR. Apliquemos esta información a nuestro modelo anterior de aplicando la exponencial a los coeficientes de saldo e ingresos: exp(5.46) ## [1] 235.0974 exp(.56) ## [1] 1.750673 Podemos interpretar estos ORs como el porcentaje o cambio multiplicativo en las probabilidades asociadas con un aumento de 1 unidad en el predictor (mientras se mantienen constantes los demás), de 1 a 235 (un aumento de 23,400%) en el caso de balance, y de 1 a 1,75 (un aumento del 75%) para los ingresos. Por ejemplo, podemos decir que la probabilidad de incumplimiento es un 75% mayor cuando los ingresos aumentan en 1 unidad. Cuando las variables predictoras son categóricas (como en biomedicina: sexo, estadío tumoral, fumar, beber, …) la interpretación se hace más sencilla porque el cambio de 1 unidad en estas variables, supone el cambio de una categoría respecto a la basal (ya que se usan dummy variables). Así, por ejemplo, si nuestro outcomes tener cáncer de pulmón o no, y nuestro predictor es ser fumanor o no, si nuestros análisis nos dan una OR de 6 asociado a ser fumador, la interpretación sería: \"La odds (probabilidad, abusando de lenguaje - también riesgo si el outcome es poco frecuente) de sufrir cáncer de pulmón es 6 veces mayor en los fumadores que en los no fumadores. 6.5 Bondad de ajuste Podemos evaluar el rendimiento del modelo logístico utilizando el AIC, así como mediante el uso de otras medidas como: la desviación (deviance) residual, la precisión, la sensibilidad, la especificidad y el área bajo la curva (AUC). Al igual que AIC, la deviance es una medida de error, por lo que una deviance más baja significa un mejor ajuste a los datos. Esperamos que la desviación disminuya en 1 para cada predictor, por lo que con un predictor informativo (e.g. variable imporante para el modleo), la deviance disminuirá en más de 1. Deviance = \\(-2ln(L)\\), donde \\(ln\\) es el logaritmo natural y \\(L\\) es la función de verosimilitud . Veámoslo con nuestro ejemplo: logistic_model1 &lt;- glm(default ~ balance, data = Default, family = binomial) logistic_model1$deviance ## [1] 1596.452 logistic_model2 &lt;- glm(default ~ balance + income, data = Default, family = binomial) logistic_model2$deviance ## [1] 1578.966 En este caso, la deviance se redujo en más de 1, lo que indica que los ingresos mejoran el ajuste del modelo. Podemos hacer una prueba formal de la diferencia usando, como para los modelos lineales, la prueba de razón de verosimilitud: lrtest(logistic_model1, logistic_model2) ## ## Model 1: default ~ balance ## Model 2: default ~ balance + income ## ## L.R. Chisq d.f. P ## 1.748541e+01 1.000000e+00 2.895205e-05 También podemos traducir las probabilidades de un modelo logístico para el incumplimiento del pago en predicciones de clase asignando “Sí” (predeterminado) a probabilidades mayores o iguales a .5 y “No” (sin valor predeterminado) a probabilidades menores que .5, y luego contar el número de veces que el modelo asigna la clase predeterminada correcta. Si dividimos este número por el total de observaciones, habremos calculado la “precisión”. La precisión se utiliza a menudo como medida del rendimiento del modelo. preds &lt;- ifelse(fitted(logistic_model2) &gt;= .5, &quot;Yes&quot;, &quot;No&quot;) (length(which(preds == Default$default)) / nrow(Default))*100 ## [1] 97.37 El modelo es 97.37% preciso. Valores superirores al 50% mostrarían una mejora en la predicción ya que por azar, se espera que el modelo tenga una precisión del 50%. EJERCICIO (Entrega Moodle P2-Precisión) Utiliza una simulación sencilla para demostrar que is asignamos por azar que una persona va a incumplir o no con los pagos, el valor esperado de la precisión de la variable “Default$default” es del 50%. NOTA: realiza 1000 simulaciones. Una forma sencilla de obtener un buen modelo de clasificación sería asignar a todos la categoría más frecuente. Por ejemplo, en nuestros datos, la clase mayoritaria es “No” para la variable incimpliminto por un amplio margen (9667 a 333). La mayoría de las personas no incumplen. ¿Cuál es nuestra precisión, entonces, si simplemente predecimos “No” para todos los casos? La proporción de “No” en los datos es 96.67%, por lo que si siempre predijimos “No” esa sería nuestra precisión (9667 / (9667 + 333) = .9667). El modelo logístico, sorprendentemente, ofrece solo una ligera mejora. Sin embargo, al evaluar el rendimiento del clasificador, debemos reconocer que no todos los errores son iguales y que la precisión tiene limitaciones como métrica de rendimiento. En algunos casos, el modelo puede haber predicho el incumplimiento cuando no lo había. Esto se conoce como “falso positivo”. En otros casos, el modelo puede haber predicho que no hubo incumplimiento cuando hubo incumplimiento. Esto se conoce como “falso negativo”. En la clasificación, utilizamos lo que se conoce como matriz de confusión para resumir estos diferentes tipos de errores del modelo, denominados así porque la matriz resume cómo se confunde el modelo. Usaremos la función confusionMatrix () de la librería caret para calcular rápidamente estos valores: confusionMatrix(as.factor(preds), Default$default, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 9629 225 ## Yes 38 108 ## ## Accuracy : 0.9737 ## 95% CI : (0.9704, 0.9767) ## No Information Rate : 0.9667 ## P-Value [Acc &gt; NIR] : 3.067e-05 ## ## Kappa : 0.4396 ## ## Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 ## ## Sensitivity : 0.3243 ## Specificity : 0.9961 ## Pos Pred Value : 0.7397 ## Neg Pred Value : 0.9772 ## Prevalence : 0.0333 ## Detection Rate : 0.0108 ## Detection Prevalence : 0.0146 ## Balanced Accuracy : 0.6602 ## ## &#39;Positive&#39; Class : Yes ## Esta función produce una gran cantidad de resultados. Podemos ver que informa la misma precisión que calculamos anteriormente: .97. La matriz de confusión 2 x 2 está en la parte superior. Podemos caracterizar estos 4 valores en la matriz de la siguiente manera: 9629 negativos verdaderos (TN): cuando el modelo predice correctamente “No” 108 verdaderos positivos (TP): cuando el modelo predice correctamente “Sí” 225 falsos negativos (FN): cuando el modelo predice incorrectamente “No” 38 falsos positivos (FP): cuando el modelo predice incorrectamente “Sí” La siguiente tabla resume estas posibilidades: Reference Predicted No Yes No TN FN Yes FP TP Hay dos medidas clave, además de la precisión, para caracterizar el rendimiento del modelo. Mientras que la precisión mide el error general, la sensibilidad y la especificidad miden errores específicos de clase. Precisión = 1 - (FP + FN) / Total: 1 - (38 + 225) / 10000 ## [1] 0.9737 Sensibilidad (o la tasa de verdaderos positivos): TP / (TP + FN). En este caso, la sensibilidad mide la proporción de incumplimientos que se clasificaron correctamente como tales. 108 / (108 + 225) ## [1] 0.3243243 Especificidad (o la tasa de verdaderos negativos): TN / (TN + FP). En este caso, la especificidad mide la proporción de no incumplimientos que se clasificaron correctamente como tales. 9629 / (9629 + 38) ## [1] 0.9960691 ¿Por qué deberíamos considerar estos errores específicos de clase? Todos los modelos tienen errores, pero no todos los errores del modelo son igualmente importantes. Por ejemplo, un falso negativo — prediciendo incorrectamente que un prestatario no incurrirá en incumplimiento — puede ser un error costoso para un banco, si el incumplimiento se hubiera podido prevenir mediante la intervención. Pero, por otro lado, un falso positivo, que predice incorrectamente que un prestatario incurrirá en incumplimiento, puede desencadenar una advertencia innecesaria que irrita a los clientes. Los errores que comete un modelo se pueden controlar ajustando el umbral de decisión utilizado para asignar probabilidades predichas a las clases. Usamos un umbral de probabilidad de .5 para clasificar los incumplimientos en los pagos. Si el umbral se establece en .1, por el contrario, la precisión general disminuiría, pero también lo haría el número de falsos negativos, lo que podría ser deseable. El modelo luego atraparía a más morosos, lo que ahorraría dinero al banco, pero eso tendría un costo: más falsos positivos (clientes potencialmente irritados). preds &lt;- as.factor(ifelse(fitted(logistic_model2) &gt;= .1, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, Default$default)$table ## Reference ## Prediction No Yes ## No 9105 90 ## Yes 562 243 La pregunta de cómo establecer el umbral de decisión —. 5, .1 o algo más — debe responderse con referencia al contexto empresarial. Una curva de característica operativa del receptor (ROC por sus siglas en ingles) visualiza las compensaciones entre los tipos de errores trazando la especificidad frente a la sensibilidad. El cálculo del área bajo la curva ROC (AUC) nos permite, además, resumir el rendimiento del modelo y comparar modelos. La curva en sí muestra los tipos de errores que cometería el modelo en diferentes umbrales de decisión. Para crear una curva ROC usamos la función roc () del paquete pROC, y mostramos los valores de sensibilidad / especificidad asociados con los umbrales de decisión de .1 y .5: library(pROC) library(plotROC) invisible(plot(roc(factor(ifelse(Default$default == &quot;Yes&quot;, 1, 0)), fitted(logistic_model2)), print.thres = c(.1, .5), col = &quot;red&quot;, print.auc = T)) Un modelo con una precisión del 50%, es decir, un clasificador aleatorio, tendría una curva ROC que siguiera la línea de referencia diagonal. Un modelo con una precisión del 100%, un clasificador perfecto, tendría una curva ROC siguiendo los márgenes del triángulo superior. Cada punto de la curva ROC representa un par de sensibilidad / especificidad correspondiente a un umbral de decisión particular. Cuando establecimos el umbral de decisión en .1, la sensibilidad fue .73 (243 / (243 + 90)) y la especificidad fue .94 (9105 / (9105 + 562)). Ese punto se muestra en la curva. Del mismo modo, cuando establecimos el umbral de decisión en .5, la sensibilidad fue .32 y la especificidad fue .996. Ese punto también está en la curva. ¿Qué umbral de decisión es óptimo? Nuevamente, depende del problema (pensad en cáncer o en este ejemplo de dinero). Las curvas ROC nos permiten elegir los errores específicos de clase que podemos cometer. El AUC es el resumen de cómo funciona el modelo en cada umbral de decisión. En general, los modelos con AUC más altos son mejores. Esta medida nos servirá para comparar métodos de aprendizaje automático que iremos aprendiendo durante el curso. 6.6 Ejemplo de regresión logística: modelización de riesgo diabetes Para este ejemplo usaremos el conjunto de datos Pima, incluido en la librería MASS que contienen esta información: Una población de mujeres que tenían al menos 21 años, de ascendencia indígena Pima y que vivían cerca de Phoenix, Arizona, se sometieron a pruebas de diabetes de acuerdo con los criterios de la Organización Mundial de la Salud. Los datos fueron recopilados por el Instituto Nacional de Diabetes y Enfermedades Digestivas y Renales de EE. UU. Usaremos información para 532 mujeres con datos completos después de eliminar los datos (principalmente faltantes) sobre la insulina sérica. El conjunto de datos incluye las siguientes variables: npreg: número de embarazos glu: concentración de glucosa en plasma a 2 horas en una prueba de tolerancia oral a la glucosa bp: presión arterial diastólica (mm Hg) piel: grosor del pliegue cutáneo del tríceps (mm) bmi: índice de masa corporal (peso en kg / (altura en m) ^ 2) ped: función del pedigrí de la diabetes age: Edad (años) type: Sí o No (diabetes) La variable resultado es “type”, que indica si una persona tiene diabetes. Dividiremos los datos en un conjunto de entrenamiento y otro test que combinaremos para ilustrar este ejemplo. library(MASS) data(&quot;Pima.tr&quot;) data(&quot;Pima.te&quot;) d &lt;- rbind(Pima.te, Pima.tr) str(d) ## &#39;data.frame&#39;: 532 obs. of 8 variables: ## $ npreg: int 6 1 1 3 2 5 0 1 3 9 ... ## $ glu : int 148 85 89 78 197 166 118 103 126 119 ... ## $ bp : int 72 66 66 50 70 72 84 30 88 80 ... ## $ skin : int 35 29 23 32 45 19 47 38 41 35 ... ## $ bmi : num 33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ... ## $ ped : num 0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 0.704 0.263 ... ## $ age : int 50 31 21 26 53 51 31 33 27 29 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 2 2 1 1 2 ... Todos los predictores son enteros o numéricos. Nuestro objetivo es construir un modelo logístico de diabetes para ilustrar cómo interpretar los coeficientes del modelo. 6.6.1 Modelo simple Comencemos con un modelo simple. bin_model1 &lt;- glm(type ~ bmi + age, data = d, family = binomial) display(bin_model1) ## glm(formula = type ~ bmi + age, family = binomial, data = d) ## coef.est coef.se ## (Intercept) -6.26 0.67 ## bmi 0.10 0.02 ## age 0.06 0.01 ## --- ## n = 532, k = 3 ## residual deviance = 577.2, null deviance = 676.8 (difference = 99.6) Intercept: -6.26 es el logaritmo de la probabilidad de tener diabetes cuando bmi = 0 y edad = 0. Dado que ni la edad ni el bmi pueden ser iguales a 0, el intercept no es interpretable en este modelo. Por tanto, tendría sentido centrar las variables para facilitar la interpretación. bmi: .1 es el cambio previsto en el log-odds de diabetes asociado con un aumento de 1 unidad en el bmi, manteniendo la edad constante. Este coeficiente es estadísticamente significativo ya que .1 - 2 x .02 &gt; 0. (Un IC del 95% que no contiene 0 indica significancia estadística) y también porque su p-valor asociacio mediante el test de score es \\(&lt;0.05\\) (usar la función summary () . Podemos traducir este coeficiente en un OR mediante la exponencial: \\(e^.1\\) = 1.11. Un aumento de 1 unidad en el IMC, manteniendo la edad constante, se asocia con un aumento del 11% en la odds (o, más coloquialmente, la probabilidad) de diabetes. edad: .06 es el cambio predicho en el log-oods de diabetes asociado con un aumento de 1 unidad en la edad, manteniendo constante el bmi. Este coeficiente también es estadísticamente significativo ya que .06 - 2 x .01&gt; 0. El OR para la edad es \\(e^.06\\) = 1.06 lo que indica un aumento del 6% en la probabilidad de sufrir diabetes asociada con un aumento de 1 unidad en la edad. 6.6.2 Agregar predictores y evaluar el ajuste Ahora ajustaremos un modelo completo (excluyendo “skin”, ya que parece medir casi lo mismo que bmi). ¿Mejora el ajuste? bin_model2 &lt;- glm(type ~ bmi + age + ped + glu + npreg + bp , data = d, family = binomial) display(bin_model2) ## glm(formula = type ~ bmi + age + ped + glu + npreg + bp, family = binomial, ## data = d) ## coef.est coef.se ## (Intercept) -9.59 0.99 ## bmi 0.09 0.02 ## age 0.03 0.01 ## ped 1.31 0.36 ## glu 0.04 0.00 ## npreg 0.12 0.04 ## bp -0.01 0.01 ## --- ## n = 532, k = 7 ## residual deviance = 466.5, null deviance = 676.8 (difference = 210.3) La deviance disminuye de 577 en el modelo anterior a 466.5 en este modelo, muy por encima de los 4 puntos que debería bajar simplemente al incluir 4 predictores adicionales. Además, el LRT nos indica que estas diferencias son estadísticamente significativas: lrtest(bin_model1, bin_model2) ## ## Model 1: type ~ bmi + age ## Model 2: type ~ bmi + age + ped + glu + npreg + bp ## ## L.R. Chisq d.f. P ## 110.6664 4.0000 0.0000 El segundo modelo es mucho mejor que el primero, lo que también es evidente cuando observamos las matrices de confusión (con el umbral de decisión establecido en .5) preds &lt;- as.factor(ifelse(fitted(bin_model1) &gt; .5, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, d$type)$table ## Reference ## Prediction No Yes ## No 312 112 ## Yes 43 65 preds &lt;- as.factor(ifelse(fitted(bin_model2) &gt; .5, &quot;Yes&quot;, &quot;No&quot;)) confusionMatrix(preds, d$type)$table ## Reference ## Prediction No Yes ## No 318 73 ## Yes 37 104 Como era de esperar, el modelo completo comete menos errores. Podemos formalizar esta impresión calculando y comparando la precisión del modelo: 1 - (112 + 43) / (112 + 43 + 312 + 65) = 0.71 para el primer modelo, en comparación con 1 - (73 + 37) / (73 + 37 + 318 + 104) =r round (1 - (73 + 37) / (73 + 37 + 318 + 104 ), 2) para el segundo modelo más grande. Los números de verdaderos negativos son cercanos, pero el modelo más grande aumenta sustancialmente el número de verdaderos positivos y reduce el número de falsos negativos, prediciendo incorrectamente que una persona no tiene diabetes (aunque esta sigue siendo la clase de error más grande). Deberíamos comprobar para ver que estos modelos mejoran la precisión sobre un modelo que siempre predice la clase mayoritaria. En este conjunto de datos, “No” es la categoría mayoritaria con 66,7%. Entonces, si siempre predijimos “No”, estaríamos en lo correcto el 66.7% de las veces, que es una precisión menor que cualquiera de los modelos. Las curvas ROC proporcionan una descripción más sistemática del rendimiento del modelo en términos de errores de clasificación errónea. invisible(plot(roc(d$type, fitted(bin_model1)), col = &quot;red&quot;, main = &quot;ROC curves: logistic model 1 (red) vs. logistic model 2 (blue)&quot;)) invisible(plot(roc(d$type, fitted(bin_model2)), print.auc = T, col = &quot;blue&quot;, add = T)) El modelo más grande es claramente mejor: el AUC es más alto. 6.6.3 Análisis de interacciones Agreguemos una interacción entre dos predictores continuos, ped y glu. Centraremos y escalaremos las entradas para que el coeficiente de la interacción sea interpretable. bin_model3 &lt;- standardize(glm(type ~ bmi + ped * glu + age + npreg + bp , data = d, family = binomial)) display(bin_model3) ## glm(formula = type ~ z.bmi + z.ped * z.glu + z.age + z.npreg + ## z.bp, family = binomial, data = d) ## coef.est coef.se ## (Intercept) -1.02 0.13 ## z.bmi 1.29 0.26 ## z.ped 1.02 0.24 ## z.glu 2.31 0.27 ## z.age 0.53 0.30 ## z.npreg 0.87 0.29 ## z.bp -0.19 0.26 ## z.ped:z.glu -1.15 0.41 ## --- ## n = 532, k = 8 ## residual deviance = 460.0, null deviance = 676.8 (difference = 216.7) La interacción mejora el modelo pero no cambia la imagen general del ajuste del modelo en el gráfico de residuos agrupados (no incluido). Intercept : -1.02 es el log-odds de diabetes cuando todos los predictores son promedio (ya que hemos centrado y escalado las entradas). La probabilidad de tener diabetes para la mujer promedio en este conjunto de datos es logit\\(^{- 1}\\) (- 1.02) = 0.27. z.bmi: 1.29 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.bmi, manteniendo constantes los demás predictores. \\(e^{1.29}\\) = 3.63 por lo que un aumento de 1 unidad en z.bmi, manteniendo constantes los otros predictores, se asocia con un aumento del 263% en la probabilidad de sufrir diabetes. z.ped: 1.02 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo constantes los otros predictores. \\(e^{1.02}\\) = 2.77 por lo que un aumento de 1 unidad en z.ped, cuando z.glu = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 177% en la probabilidad de sufrir diabetes. z.glu: 2.31 es el log-odds de diabetes asociado con un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo constantes los demás predictores. \\(e^{2.31}\\) = 10.07 por lo que un aumento de 1 unidad en z.glu, cuando z.ped = 0 y manteniendo los otros predictores constantes, se asocia con un aumento del 907% en la probabilidad de sufrir diabetes. …. el resto de predictores igual z.ped:z.glu : se añade -1.15 al log-odds de diabetes de z.ped cuando z.glu aumenta en 1 unidad, manteniendo constantes los otros predictores. O, alternativamente, se añade -1.15 al log-odds de diabetes de z.glu por cada unidad adicional de z.ped. Calculamos el OR, como en los otros casos, exponenciando: \\(e^ {- 1.15}\\) = 0.32. El OR para z.ped disminuye en un 68% (1 - .32 = .68) cuando z.glu aumenta en 1 unidad, manteniendo constantes los demás predictores. O, alternativamente, el OR para z.glu disminuye en un 68% con cada unidad adicional de z.ped. 6.6.4 Gráfico de la interacción Como siempre, debemos visualizar la interacción para comprenderla. Esto es especialmente necesario cuando las relaciones se expresan en términos de probabilidades logarítmicas y razones de probabilidades. Como hemos hecho anteriormente para fines de visualización, dicotomizaremos los predictores en la interacción y, en este caso, para facilitar la interpretación, presentaremos las relaciones en términos de probabilidades. El propósito de los gráficos es la comprensión y la ilustración, por lo que no nos preocupa demasiado la precisión estadística. Resumiremos las relaciones usando una curva loess (estimación no paramétrica de la regresión) para capturar la no linealidad del efecto del predictor (\\(\\pm \\infty\\)) al rango del resultado binario (0, 1). d$ped_bin &lt;- ifelse(d$ped &gt; mean(d$ped), &quot;above avg&quot;,&quot;below avg&quot;) d$glu_bin &lt;- ifelse(d$glu &gt; mean(d$glu), &quot;above avg&quot;,&quot;below avg&quot;) d$prob &lt;- fitted(bin_model3) d$type_num &lt;- ifelse(d$type == &quot;Yes&quot;, 1, 0) ggplot(d, aes(glu, type_num)) + geom_point() + stat_smooth(aes(glu, prob, col = ped_bin), se = F) + labs(y = &quot;Pr(diabetes)&quot;, title = &quot;Diabetes ~ glu varying by ped_bin&quot;) La relación entre glu y diabetes depende claramente de los niveles de ped. Como en el caso lineal, las líneas no paralelas indican una interacción. El coeficiente de log-odds negativo para la interacción del modelo indica que a medida que ped aumenta, la fuerza de la relación entre glu y type (diabetes) disminuye. Esto es exactamente lo que vemos en este gráfico: ggplot(d, aes(ped, type_num)) + geom_point() + stat_smooth(aes(ped, prob, col = glu_bin), se = F) + labs(y = &quot;Pr(diabetes)&quot;, title = &quot;Diabetes ~ ped varying by glu_bin&quot;) La interacción es más difícil de ver aquí porque la escala de ped está comprimida, con la mayoría de las observaciones cercanas a 0. Sin embargo, podemos ver que a medida que glu aumenta, la fuerza de la relación entre ped y diabetes disminuye. Nuevamente, las líneas no paralelas indican la presencia de una interacción. 6.6.5 Uso del modelo para predecir probabilidades El tamaño del efecto más grande en el modelo anterior con la interacción, con mucho, es z.glu. Por tanto, para comunicar los resultados de este modelo deberíamos concentrarnos en glu. Pero los coeficientes expresados como logaritmos de probabilidades son algo confusos y, lamentablemente, las razones de probabilidades no ayudan a aclarar mucho las cosas. Deberíamos ir al trabajo adicional de traducir los coeficientes del modelo en probabilidades, pero para hacerlo debemos identificar en qué parte de la curva de probabilidad nos gustaría evaluar glu. Escogeremos la región cercana al promedio de z.glu — 0 — y examinaremos el efecto de aumentar z.glu en 1 unidad (que es igual a 2 desviaciones estándar de glu) cuando los otros predictores son promedio. La forma más sencilla de hacer esto es crear una base de datos con los valores de predicción deseados para usar con la función predict (). basal &lt;- data.frame(z.bmi = 0, z.ped = 0, z.glu = 0, z.age = 0, z.npreg = 0, z.bp = 0) glucosa &lt;- data.frame(z.bmi = 0, z.ped = 0, z.glu = 1, z.age = 0, z.npreg = 0, z.bp = 0) (lo &lt;- as.numeric(invlogit(predict(bin_model3, newdata = basal)))) ## [1] 0.2652028 (hi &lt;- as.numeric(invlogit(predict(bin_model3, newdata = )))) ## [1] 0.716146591 0.025911688 0.017983270 0.029013744 0.893018807 0.665405710 0.395033985 0.211066662 0.431695033 0.225375587 ## [11] 0.032840692 0.443676401 0.022607120 0.418795847 0.194085484 0.411388311 0.122928759 0.771984610 0.760725696 0.088470567 ## [21] 0.953485181 0.785645203 0.028409690 0.026232726 0.055457646 0.038181372 0.757850071 0.007630220 0.110507313 0.066603557 ## [31] 0.213559028 0.017048355 0.294579371 0.213686248 0.245378175 0.147939308 0.058529536 0.047092679 0.169073996 0.023421517 ## [41] 0.434375748 0.123923484 0.796680824 0.257623355 0.236877492 0.032937565 0.075796893 0.459769771 0.014969537 0.214416331 ## [51] 0.409898562 0.024597682 0.736084480 0.071421984 0.863089942 0.109813203 0.415148971 0.192888308 0.671705524 0.707181753 ## [61] 0.222687051 0.074378276 0.047288218 0.250211485 0.102873418 0.451885105 0.016412055 0.839500999 0.593725195 0.887724350 ## [71] 0.023636511 0.972427462 0.421875342 0.240818110 0.193220347 0.275433738 0.029285367 0.858427986 0.792694816 0.428056807 ## [81] 0.241389504 0.359146427 0.159848949 0.126229936 0.089106804 0.965646716 0.021565093 0.504101659 0.255700597 0.303575110 ## [91] 0.316087819 0.695623695 0.059157940 0.023850831 0.703188586 0.768676985 0.198583192 0.736445668 0.019832290 0.911158959 ## [101] 0.794437117 0.023391174 0.082335119 0.536540158 0.491636172 0.875915114 0.830550408 0.453101254 0.050024142 0.017749687 ## [111] 0.151803493 0.175667620 0.900329614 0.125898408 0.410586667 0.586540636 0.832688763 0.035561910 0.252327286 0.110864806 ## [121] 0.054988088 0.131662542 0.393097130 0.492496368 0.513182966 0.032671230 0.036078371 0.218212971 0.686096496 0.281975363 ## [131] 0.145884637 0.351411775 0.239547073 0.725330877 0.392954359 0.354585338 0.328110094 0.183872054 0.649535926 0.189835213 ## [141] 0.750660386 0.100154335 0.253081898 0.306902095 0.138876568 0.112315917 0.602945913 0.033225614 0.021023766 0.126008933 ## [151] 0.074805807 0.593242488 0.276438109 0.030025810 0.388107478 0.814468072 0.819988723 0.320406177 0.512019175 0.189866084 ## [161] 0.019102564 0.778045038 0.063532324 0.021660839 0.087123658 0.371697775 0.017831740 0.174573799 0.072586081 0.050795966 ## [171] 0.321374939 0.390300727 0.242283766 0.101281785 0.477098938 0.177832066 0.851704388 0.421578801 0.059594070 0.801660505 ## [181] 0.263016503 0.158261796 0.629712769 0.680147379 0.627289909 0.143229332 0.371021820 0.048405335 0.155912833 0.844592024 ## [191] 0.774738072 0.074975116 0.094313006 0.034029316 0.156403796 0.765185930 0.162409946 0.941491860 0.098203334 0.096782855 ## [201] 0.013696456 0.086137707 0.665930779 0.328426313 0.230933621 0.045364772 0.067599667 0.128835144 0.552086895 0.192049128 ## [211] 0.391808734 0.634609576 0.138796509 0.034537822 0.402157076 0.473285370 0.915578006 0.109882027 0.060813059 0.078347861 ## [221] 0.511021110 0.046236210 0.637413042 0.057352160 0.060578365 0.328120324 0.117022441 0.150080125 0.033383541 0.548421342 ## [231] 0.727231653 0.273012605 0.006665104 0.023959522 0.212622505 0.263913260 0.304803089 0.282446484 0.528587177 0.053303093 ## [241] 0.042787259 0.895978588 0.951451693 0.260631793 0.057999466 0.077448786 0.039445750 0.078320973 0.521759602 0.120793953 ## [251] 0.109749728 0.115212481 0.063160685 0.315382778 0.186348273 0.242141093 0.800887574 0.866564275 0.147824356 0.483279794 ## [261] 0.458255040 0.058388003 0.052876258 0.244883311 0.761115562 0.021394588 0.531937078 0.751371443 0.307274782 0.801133302 ## [271] 0.005240345 0.656336098 0.109766806 0.101435208 0.023548802 0.056804667 0.081720211 0.513341731 0.014456472 0.127221754 ## [281] 0.744369844 0.498650317 0.182496315 0.605638441 0.021059011 0.094691510 0.633177077 0.189201907 0.229657632 0.818999683 ## [291] 0.081933947 0.793780642 0.798845854 0.137588419 0.275762056 0.057395933 0.445742740 0.584089416 0.070998635 0.209775098 ## [301] 0.174014948 0.294145785 0.743857124 0.126042513 0.893349297 0.712229694 0.217225821 0.135596074 0.343239883 0.187767754 ## [311] 0.233598292 0.822021476 0.082764167 0.102210424 0.113711375 0.122461316 0.828791325 0.107099346 0.054386091 0.931938668 ## [321] 0.326454054 0.678618259 0.863704140 0.217542759 0.056249096 0.816489720 0.491806299 0.084796292 0.943486215 0.302404878 ## [331] 0.132747361 0.037305147 0.053261653 0.902996173 0.042527808 0.787771463 0.031525411 0.220902831 0.054775507 0.642266770 ## [341] 0.520264613 0.707148463 0.902568015 0.695527581 0.848650675 0.466017086 0.291730904 0.027601848 0.133315806 0.771592655 ## [351] 0.383831136 0.090423918 0.020481869 0.115239515 0.038125985 0.017056834 0.070184592 0.755542001 0.050653582 0.481007350 ## [361] 0.195086963 0.187977925 0.169394579 0.377212811 0.151859655 0.475032680 0.186324471 0.612248541 0.023804839 0.007054577 ## [371] 0.085213924 0.171343641 0.900710787 0.150595032 0.257766676 0.013926514 0.035750449 0.185105131 0.253500436 0.369424730 ## [381] 0.640522481 0.841160836 0.060643162 0.051633327 0.731126069 0.154040714 0.063202801 0.337255163 0.087712142 0.059196122 ## [391] 0.446522479 0.839581017 0.899946965 0.051162921 0.569809764 0.085461942 0.080315826 0.343678259 0.504516436 0.092545675 ## [401] 0.175360179 0.310167781 0.720652085 0.094860697 0.905656358 0.432122047 0.742494468 0.704660222 0.065240504 0.044936864 ## [411] 0.086384200 0.460776518 0.058306508 0.538740077 0.360529087 0.413937339 0.248787673 0.063168325 0.205549345 0.143823024 ## [421] 0.121451790 0.204643255 0.028047843 0.262047997 0.797845942 0.020192814 0.067782381 0.880235920 0.067820578 0.093376849 ## [431] 0.145058897 0.885115279 0.023833728 0.172919691 0.023186441 0.343149266 0.491733925 0.334336420 0.182785009 0.107602309 ## [441] 0.159470782 0.480057756 0.718226410 0.110307283 0.282105476 0.504892387 0.023457816 0.246497165 0.288819782 0.406440777 ## [451] 0.333499471 0.789228958 0.024672867 0.158285199 0.915532293 0.083660243 0.616972917 0.156244026 0.066104438 0.117576505 ## [461] 0.582609228 0.935907656 0.494078320 0.906231057 0.044752423 0.098120327 0.817658808 0.063067848 0.048103513 0.094446457 ## [471] 0.009656172 0.457728456 0.371287692 0.732871561 0.239173153 0.048538537 0.427026826 0.838680152 0.005916331 0.639747396 ## [481] 0.192937289 0.044098532 0.307301900 0.771646053 0.893235289 0.633989608 0.325705459 0.861507987 0.924405484 0.211878609 ## [491] 0.232857992 0.553168378 0.199630964 0.167297598 0.636698113 0.102745817 0.412260537 0.037681077 0.499347951 0.561847838 ## [501] 0.108622049 0.036902563 0.275679034 0.142289294 0.929821372 0.853111736 0.052948764 0.186298288 0.034369934 0.468239290 ## [511] 0.021331580 0.617803456 0.083865824 0.751272138 0.115022996 0.496356896 0.458485256 0.303064449 0.512265744 0.935001789 ## [521] 0.011659745 0.243551235 0.234201771 0.733582682 0.439827183 0.132566837 0.200183137 0.254534757 0.620459940 0.187218540 ## [531] 0.128750723 0.801532793 round(hi - lo, 2) ## [1] 0.45 -0.24 -0.25 -0.24 0.63 0.40 0.13 -0.05 0.17 -0.04 -0.23 0.18 -0.24 0.15 -0.07 0.15 -0.14 0.51 0.50 -0.18 ## [21] 0.69 0.52 -0.24 -0.24 -0.21 -0.23 0.49 -0.26 -0.15 -0.20 -0.05 -0.25 0.03 -0.05 -0.02 -0.12 -0.21 -0.22 -0.10 -0.24 ## [41] 0.17 -0.14 0.53 -0.01 -0.03 -0.23 -0.19 0.19 -0.25 -0.05 0.14 -0.24 0.47 -0.19 0.60 -0.16 0.15 -0.07 0.41 0.44 ## [61] -0.04 -0.19 -0.22 -0.01 -0.16 0.19 -0.25 0.57 0.33 0.62 -0.24 0.71 0.16 -0.02 -0.07 0.01 -0.24 0.59 0.53 0.16 ## [81] -0.02 0.09 -0.11 -0.14 -0.18 0.70 -0.24 0.24 -0.01 0.04 0.05 0.43 -0.21 -0.24 0.44 0.50 -0.07 0.47 -0.25 0.65 ## [101] 0.53 -0.24 -0.18 0.27 0.23 0.61 0.57 0.19 -0.22 -0.25 -0.11 -0.09 0.64 -0.14 0.15 0.32 0.57 -0.23 -0.01 -0.15 ## [121] -0.21 -0.13 0.13 0.23 0.25 -0.23 -0.23 -0.05 0.42 0.02 -0.12 0.09 -0.03 0.46 0.13 0.09 0.06 -0.08 0.38 -0.08 ## [141] 0.49 -0.17 -0.01 0.04 -0.13 -0.15 0.34 -0.23 -0.24 -0.14 -0.19 0.33 0.01 -0.24 0.12 0.55 0.55 0.06 0.25 -0.08 ## [161] -0.25 0.51 -0.20 -0.24 -0.18 0.11 -0.25 -0.09 -0.19 -0.21 0.06 0.13 -0.02 -0.16 0.21 -0.09 0.59 0.16 -0.21 0.54 ## [181] 0.00 -0.11 0.36 0.41 0.36 -0.12 0.11 -0.22 -0.11 0.58 0.51 -0.19 -0.17 -0.23 -0.11 0.50 -0.10 0.68 -0.17 -0.17 ## [201] -0.25 -0.18 0.40 0.06 -0.03 -0.22 -0.20 -0.14 0.29 -0.07 0.13 0.37 -0.13 -0.23 0.14 0.21 0.65 -0.16 -0.20 -0.19 ## [221] 0.25 -0.22 0.37 -0.21 -0.20 0.06 -0.15 -0.12 -0.23 0.28 0.46 0.01 -0.26 -0.24 -0.05 0.00 0.04 0.02 0.26 -0.21 ## [241] -0.22 0.63 0.69 0.00 -0.21 -0.19 -0.23 -0.19 0.26 -0.14 -0.16 -0.15 -0.20 0.05 -0.08 -0.02 0.54 0.60 -0.12 0.22 ## [261] 0.19 -0.21 -0.21 -0.02 0.50 -0.24 0.27 0.49 0.04 0.54 -0.26 0.39 -0.16 -0.16 -0.24 -0.21 -0.18 0.25 -0.25 -0.14 ## [281] 0.48 0.23 -0.08 0.34 -0.24 -0.17 0.37 -0.08 -0.04 0.55 -0.18 0.53 0.53 -0.13 0.01 -0.21 0.18 0.32 -0.19 -0.06 ## [301] -0.09 0.03 0.48 -0.14 0.63 0.45 -0.05 -0.13 0.08 -0.08 -0.03 0.56 -0.18 -0.16 -0.15 -0.14 0.56 -0.16 -0.21 0.67 ## [321] 0.06 0.41 0.60 -0.05 -0.21 0.55 0.23 -0.18 0.68 0.04 -0.13 -0.23 -0.21 0.64 -0.22 0.52 -0.23 -0.04 -0.21 0.38 ## [341] 0.26 0.44 0.64 0.43 0.58 0.20 0.03 -0.24 -0.13 0.51 0.12 -0.17 -0.24 -0.15 -0.23 -0.25 -0.20 0.49 -0.21 0.22 ## [361] -0.07 -0.08 -0.10 0.11 -0.11 0.21 -0.08 0.35 -0.24 -0.26 -0.18 -0.09 0.64 -0.11 -0.01 -0.25 -0.23 -0.08 -0.01 0.10 ## [381] 0.38 0.58 -0.20 -0.21 0.47 -0.11 -0.20 0.07 -0.18 -0.21 0.18 0.57 0.63 -0.21 0.30 -0.18 -0.18 0.08 0.24 -0.17 ## [401] -0.09 0.04 0.46 -0.17 0.64 0.17 0.48 0.44 -0.20 -0.22 -0.18 0.20 -0.21 0.27 0.10 0.15 -0.02 -0.20 -0.06 -0.12 ## [421] -0.14 -0.06 -0.24 0.00 0.53 -0.25 -0.20 0.62 -0.20 -0.17 -0.12 0.62 -0.24 -0.09 -0.24 0.08 0.23 0.07 -0.08 -0.16 ## [441] -0.11 0.21 0.45 -0.15 0.02 0.24 -0.24 -0.02 0.02 0.14 0.07 0.52 -0.24 -0.11 0.65 -0.18 0.35 -0.11 -0.20 -0.15 ## [461] 0.32 0.67 0.23 0.64 -0.22 -0.17 0.55 -0.20 -0.22 -0.17 -0.26 0.19 0.11 0.47 -0.03 -0.22 0.16 0.57 -0.26 0.37 ## [481] -0.07 -0.22 0.04 0.51 0.63 0.37 0.06 0.60 0.66 -0.05 -0.03 0.29 -0.07 -0.10 0.37 -0.16 0.15 -0.23 0.23 0.30 ## [501] -0.16 -0.23 0.01 -0.12 0.66 0.59 -0.21 -0.08 -0.23 0.20 -0.24 0.35 -0.18 0.49 -0.15 0.23 0.19 0.04 0.25 0.67 ## [521] -0.25 -0.02 -0.03 0.47 0.17 -0.13 -0.07 -0.01 0.36 -0.08 -0.14 0.54 6.7 Creación de un modelo y validación Todo lo explicado en la sección de selección de variables para un modelo lineal, aplica para el caso de la regresión logística. Las funciones reconocen que el objeto es un glm con familia binomial y realiza los cálculos requeridos para este tipo de regresión (progamación orientada a objetos). El tema de validación cruzada para evaluar un modelo también aplica. Veámoslo con un ejemplo. Supongamos que queremos elegir el mejor modelo para predecir el riesgo de diabetes y queremos validarlo con valización cruzada. Todos los pasos y métodos que hemos aprendido en las lecciones anteriores, podemos realizarlos de la siguiente manera. Usaremos los datos train y test que hay por defecto (Pima.tr y Pima.te). Para la validación cruzada usaremos la librería caret que veremos en detalle más adelante. Empecemos seleccionando el mejor modelo en los datos de entrenamiento con un stepwise hacia atrás mod.all &lt;- glm(type ~ ., data=Pima.tr, family=&quot;binomial&quot;) mod &lt;- step(mod.all, trace=FALSE, direction=&quot;backward&quot;) summary(mod) ## ## Call: ## glm(formula = type ~ npreg + glu + bmi + ped + age, family = &quot;binomial&quot;, ## data = Pima.tr) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0009 -0.6816 -0.3664 0.6467 2.2898 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.938059 1.541571 -6.447 1.14e-10 *** ## npreg 0.103142 0.064517 1.599 0.10989 ## glu 0.031809 0.006667 4.771 1.83e-06 *** ## bmi 0.079672 0.032649 2.440 0.01468 * ## ped 1.811417 0.661048 2.740 0.00614 ** ## age 0.039286 0.020967 1.874 0.06097 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 256.41 on 199 degrees of freedom ## Residual deviance: 178.47 on 194 degrees of freedom ## AIC: 190.47 ## ## Number of Fisher Scoring iterations: 5 Podemos evaluar la capacidad predictiva en la muestra test mediante preds &lt;- predict(mod, newdata = Pima.te, type=&quot;response&quot;) preds &lt;- as.factor(ifelse(preds &gt;= .5, &quot;Yes&quot;, &quot;No&quot;) ) confusionMatrix(preds, Pima.te$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 199 42 ## Yes 24 67 ## ## Accuracy : 0.8012 ## 95% CI : (0.7542, 0.8428) ## No Information Rate : 0.6717 ## P-Value [Acc &gt; NIR] : 1.116e-07 ## ## Kappa : 0.5294 ## ## Mcnemar&#39;s Test P-Value : 0.03639 ## ## Sensitivity : 0.8924 ## Specificity : 0.6147 ## Pos Pred Value : 0.8257 ## Neg Pred Value : 0.7363 ## Prevalence : 0.6717 ## Detection Rate : 0.5994 ## Detection Prevalence : 0.7259 ## Balanced Accuracy : 0.7535 ## ## &#39;Positive&#39; Class : No ## y calcular la capacidad predictiva en la muestra train utilizando un método de 5-fold cross-validation con: library(caret) mod.test &lt;- train(type ~ ., data=Pima.tr, trControl = trainControl(method=&quot;cv&quot;, number=5), method = &quot;glm&quot;, family=&quot;binomial&quot;) mod.test ## Generalized Linear Model ## ## 200 samples ## 7 predictor ## 2 classes: &#39;No&#39;, &#39;Yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 160, 160, 159, 160, 161 ## Resampling results: ## ## Accuracy Kappa ## 0.7394246 0.3961653 6.8 Nomogramas Una vez que ya tenemos creado y validad un modelo predictivo, nos puede interesar aplicarlo en otros individuos para poder tomar decisiones. Para ello, podemos usar nomogramas. Un nomograma es una representación gráfica que permite realizar con rapidez cálculos numéricos aproximados. Dentro del campo de la medicina, es frecuente que este tipo de gráficos este asociado al calculo de probabilidades de ocurrencia de un evento o una característica asociada a una enfermedad. Es lo que se conoce como Medicina Translacional. Aunque existen otro tipo de herramientas de cálculo vía web para estas probabilidades (Shiny), el uso de nomogramas esta muy extendido en el campo de la biomedicina como por ejemplo en el calculo de probabilidades de recurrencia en distintos tipos de cáncer, la probabilidad de supervivencia a un mes tras un infarto, o el pronóstico tras un diagnóstico de cáncer a cierto tiempo (en ese caso se usan modelos de supervivencia. Así pues, la regresión logística será una de las herramientas con una aplicación más directa y sencilla en el aprendizaje automático, donde el uso de los modelos predictivos suelen tener un aplicabilidad directa en la población. Existen numerosas herramientas para crear nomogramas en R, empecemos con la creación de nomogramas sencillos. Para ello usaremos los datos del ejemplo de diabetes con el modelo que hemos obtenido y validado anteriormente. Para usar la librería rms necesitamos que el modelo esté estimado con la función lrm () library(rms) t.data &lt;- datadist(Pima.tr) options(datadist = &#39;t.data&#39;) mod.lrm &lt;- lrm(type ~ npreg + glu + bmi + ped + age, data=Pima.tr) nom &lt;- nomogram(mod.lrm, fun = plogis, funlabel=&quot;Risk of diabetes&quot;) plot(nom) Supongamos que llega a la consulta una persona con un bmi de 35. Eso sumaría 30 puntos (basta con subir hacia arriba y ver qué valor de ‘Points’ corresponde a un bmi de 35). Una edad de 50 años (~22 puntos), una función del pedigrí de la diabetes de 1.8 (~62 puntos), una glucosa de 120 (~50 puntos) y 0 embarazos que sumaría 0 puntos. En total, el paciente suma un total de 164 puntos. Si ahora vamos a la línea de ‘Total Points’ y proyectamos sobre el predictor lineal de aproximadamente ~1.9 que se asocia con un riesgo de diabetes ligeramente superior al 80% (proyectar sobre ‘Risk of diabetes’). Obviamente estos cálculos se pueden hacer de forma más directa calculando la predicción sobre este individuo con el objeto de R indiv &lt;- data.frame(bmi=35, age=50, ped=1.8, glu=120, npreg=0) predict(mod.lrm, newdata = indiv, type=&quot;lp&quot;) ## 1 ## 1.892376 predict(mod.lrm, newdata = indiv, type = &quot;fitted&quot;) ## 1 ## 0.8690262 Estos cálculos se pueden programar en R y hace una función, o también una aplicación Shiny para aquellos médicos que no sepan usar R (los nomogramas se siguen utilizando). Otra opción es que hagamos uso de una librería para hacer nomogramas dinámicos (crea Shiny app) de forma sencilla con la librería DynNom library(DynNom) DynNom(mod, Pima.tr) Con estas simples instrucciones obtendríamos esta aplicación Shiny (Figura abajo) donde cada intervalo de confianza corresponde a un cálculo obtenido variando alguna de las variables predictoras Nomograma para el modelo de predicción para diabetes EJERCICIO (Entrega en Moodle: P-Práctica regresión logística): En esta página https://vincentarelbundock.github.io/Rdatasets/datasets.html tienes acceso a cientos de bases de datos que hay en R y que también puedes descargar como ficheros .csv. Elige una base de datos donde la pregunta científica requiera el análisis de una variable respuesta binaria. Selecciona unas 8-10 variables independientes (intenta que hayan categóricas y continuas). Crea un pdf en el que muestres un análisis completo de los datos incluyendo: 1. Descripción de tu pregunta científica 2. Análisis descriptivo de las variables independientes en función de tu variable respuesta 3. Imputación de datos en caso de ser necesario 4. Creación de un modelo predictivo 5. Descripción de las ORs para el modelo final 6. Validación del modelo mediante CV 7. Descripción de la capacidad predictiva del modelo (Sensibilidad, Precisión, curva ROC, …) 8. Creación de un nomograma estático que incluya la ilustración del cálculo de riesgo para un individuo ficticio. 9. Apéndice con el código R utilizado Se trata de crear un documento de unas 2-3 páginas (figuras y tablas aparte) explicando los principales resultados del estudio donde cada párrafo podría corresponder a cada una de las tareas anteriores. El documento debe incluir las tablas y/o figuras que creas conveniente que sostengan los resultados descritos en el documento. Se trata de intentar escribir el apartado de resultados de un artículo científico. Antes de escribir este apartado de resultados, el documento deberá empezar con un apartado breve de métodos describiendo los datos (brevemente - podéis usar la información que hay en la página de donde habéis descargado los datos) y los métodos estadísticos utilizados. Aquellos que quieran, lo pueden hacer con R Markdown. GLM significa modelo lineal generalizado. Esta función se ajustará a una variedad de modelos no lineales. Por ejemplo, podríamos especificar una regresión de Poisson con family = poisson.↩︎ Para obtener más detalles, se puede consultar este texto [How to interpret odds ratio in logistic regression?] (Https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/).↩︎ "]
]
